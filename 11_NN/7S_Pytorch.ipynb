{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание\n",
        "\n",
        "Используя заготовку кода в нотбуке из практики по Pytorch допишите самостоятельно CNN нейронную сеть и обучите ее на данных mnist.\n",
        "\n",
        "\n",
        "> Обратите внимание, <br> что при использовании categorical cross entrophy функции потерь Вам не нужно использовать активацию softmax в последнем слое.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST 0-9 CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9SScNaOka48r"
      },
      "outputs": [],
      "source": [
        "class CNN(Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Conv2d(n_channels, 32, (3,3)) # 32,26,26,32\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "\n",
        "        # first pooling layer\n",
        "        self.pool1 = MaxPool2d((2,2), stride=(2,2)) #32, 13,13,32\n",
        "\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Conv2d(32, 32, (3,3)) # 32,11,11,32\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "\n",
        "        # second pooling layer\n",
        "        self.pool2 = MaxPool2d((2,2), stride=(2,2)) # 32,5,5,32\n",
        "\n",
        "        # fully connected layer\n",
        "        self.hidden3 = Linear(5*5*32, 100)\n",
        "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
        "        self.act3 = ReLU()\n",
        "\n",
        "        # output layer\n",
        "        self.hidden4 = Linear(100, 10)\n",
        "        xavier_uniform_(self.hidden4.weight)\n",
        "        self.act4 = Softmax(dim=1)\n",
        " \n",
        "    def forward(self, X):\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.pool1(X)\n",
        "\n",
        "        # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        X = self.pool2(X)\n",
        "\n",
        "        # flatten\n",
        "        X = X.view(-1, 4*4*50)\n",
        "\n",
        "        # third hidden layer\n",
        "        X = self.hidden3(X)\n",
        "        X = self.act3(X)\n",
        "        X = self.hidden4(X)\n",
        "        X = self.act4(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(path):\n",
        "    trans = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
        "    train = MNIST(path, train=True, download=True, transform=trans)\n",
        "    test = MNIST(path, train=False, download=True, transform=trans)\n",
        "    train_dl = DataLoader(train, batch_size=64, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "    return train_dl, test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(train_dl, model):\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    for epoch in range(10):\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            optimizer.zero_grad()\n",
        "            yhat = model(inputs)\n",
        "            loss = criterion(yhat, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        yhat = model(inputs)\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(row, model):\n",
        "  row = Tensor([row]).float()\n",
        "  yhat = model(row)\n",
        "  yhat = yhat.detach().numpy()\n",
        "  return yhat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /home/leysh/.torch/datasets/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting /home/leysh/.torch/datasets/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /home/leysh/.torch/datasets/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /home/leysh/.torch/datasets/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/leysh/.torch/datasets/mnist/MNIST/raw\n",
            "\n",
            "60000 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "path = '~/.torch/datasets/mnist'\n",
        "train_dl, test_dl = prepare_data(path)\n",
        "print(len(train_dl.dataset),len(test_dl.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9863\n"
          ]
        }
      ],
      "source": [
        "model = CNN(1)\n",
        "train_model(train_dl, model)\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leysh/miniconda3/envs/tf/lib/python3.9/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
            "NVIDIA GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
            "If you want to use the NVIDIA GeForce RTX 3080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print('Accuracy: {}'.format(acc))\n",
        "row = [1, 0, 0.99539\n",
        "    , -0.05889, 0.85243, 0.02306\n",
        "    , 0.83398, -0.37708, 1\n",
        "    , 0.03760, 0.85243, -0.17755\n",
        "    , 0.59755, -0.44945, 0.60536\n",
        "    , -0.38223, 0.84356, -0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = predict(row,model)\n",
        "\n",
        "print(yhat)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ea5fa1753a683f9916dd7eb1c6c8abee70fc4f872c388cb84aa7657a3a83a143"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
