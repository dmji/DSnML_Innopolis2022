{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuu5HXaWLWSM"
      },
      "source": [
        "## One Hot Encoding \n",
        "\n",
        "В реальных проектах в основном используется реализация scikit -learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvndsBIzLWSQ",
        "outputId": "6266b6f2-4904-434d-8ed5-7e3f0bd09af8"
      },
      "outputs": [],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJcOZLBLWSW",
        "outputId": "e4fca537-e91b-4531-d455-26925b067caf"
      },
      "outputs": [],
      "source": [
        "# Собираем словарь\n",
        "vocab = {}\n",
        "count = 0\n",
        "for doc in processed_docs:\n",
        "    for word in doc.split():\n",
        "        if word not in vocab:\n",
        "            count = count +1\n",
        "            vocab[word] = count\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pesdRwpLWSc"
      },
      "outputs": [],
      "source": [
        "#Получить one hot encoding представление для любой строки на основе этого словаря.\n",
        "#Если слово есть в словаре, возвращается его представление.\n",
        "#Если нет, для этого слова возвращается список нулей.\n",
        "def get_onehot_vector(somestring):\n",
        "    onehot_encoded = []\n",
        "    for word in somestring.split():\n",
        "        temp = [0]*len(vocab)\n",
        "        if word in vocab:\n",
        "            temp[vocab[word]-1] = 1 # индексация фактов в массиве начинается с 0, а не с 1\n",
        "        onehot_encoded.append(temp)\n",
        "    return onehot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JELqSh4gLWSg",
        "outputId": "95d10c0b-ed93-42b7-cb41-5eab9c68f8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "man bites dog\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(processed_docs[1])\n",
        "get_onehot_vector(processed_docs[1]) #one hot encoding представление"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVQExJUGLWSm",
        "outputId": "18c8b398-15ca-439a-8d94-a91ec650cf30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_onehot_vector(\"man and dog are good\") \n",
        "#one hot представление случайного текста с использованием приведенного выше словаря"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "_xb8azVwLWSs",
        "outputId": "6c4e6bf5-d4b9-45cd-ffcd-85c2d323e67a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_onehot_vector(\"man and man are good\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANj41SQ4L7xI"
      },
      "source": [
        "## One-hot encoding с scikit -learn\n",
        "\n",
        "\n",
        "* One-hot encoding: При One-hot encoding кодировании каждому слову w в корпусном словаре присваивается уникальный целочисленный идентификатор wid, который находится в диапазоне от 1 до |V|, где V — набор словарного запаса корпуса. Затем каждое слово представляется V-мерным двоичным вектором из нулей и единиц.\n",
        "\n",
        "* Кодирование меток: в кодировании меток каждое слово w в нашем корпусе преобразуется в числовое значение от 0 до n-1 (где n относится к количеству уникальных слов в нашем корпусе).\n",
        "\n",
        "##### Ссылка на официальную документацию обоих можно найти [здесь](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) и [здесь](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAPkk-fZLh4W"
      },
      "outputs": [],
      "source": [
        "S1 = 'dog bites man'\n",
        "S2 = 'man bites dog'\n",
        "S3 = 'dog eats meat'\n",
        "S4 = 'man eats food'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYCRHl5SLWSy",
        "outputId": "9d90ac82-cbba-4197-c3de-4d4112d286d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The data:  ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n",
            "Label Encoded: [1 0 4 4 0 1 1 2 5 4 2 3]\n",
            "Onehot Encoded Matrix:\n",
            " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
        "values = data[0]+data[1]+data[2]+data[3]\n",
        "print(\"The data: \",values)\n",
        "\n",
        "#Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(\"Label Encoded:\",integer_encoded)\n",
        "\n",
        "#One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
        "print(\"Onehot Encoded Matrix:\\n\",onehot_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb45nUCI23jj"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMoADcrhJP2H"
      },
      "source": [
        "## Bag of Words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhnX4sORJP2J",
        "outputId": "804128f7-4e7a-4f21-8100-3d5c98c634a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] \n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEm3kuokJP2N"
      },
      "source": [
        "Теперь давайте выполним основную задачу по поиску представления набора слов. Мы будем использовать CountVectorizer от sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pbCdQVWQJP2O",
        "outputId": "546ad823-9bbe-45bf-a8b1-588e07d09b5e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
            "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#смотрим список документов\n",
        "print(\"Our corpus: \", processed_docs)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "# Создайте BOW-представление для корпуса\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Посмотрите на сопоставление словарного запаса\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "# см. представителя BOW для первых 2 документов\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "# Получить представление, используя этот словарь, для нового текста\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcHPrpjy31B0"
      },
      "source": [
        "В приведенном выше коде мы представили текст с учетом частотности слов. Однако иногда нас не очень волнует частота, а нужно только знать, появилось ли слово в тексте или нет. То есть каждый документ представлен как вектор нулей и единиц. Для этой цели мы будем использовать опцию binary=True в CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bvFoqDRAJP2Q",
        "outputId": "919d12a0-597d-45ae-ef73-251a87629a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "#BoW binary=True\n",
        "count_vect = CountVectorizer(binary=True)\n",
        "count_vect.fit(processed_docs)\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-NpN20w23oC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_hosDAcZwSq"
      },
      "source": [
        "## Bag of N-Grams\n",
        "\n",
        "##### One hot encoding, BoW и TF-IDF относиться к словам как к самостоятельным единицам. Нет понятия фраз или порядка слов. Подход Bag of Ngrams (BoN) пытается исправить это. Он делает это, разбивая текст на куски из n счетных слов/токенов. Это может помочь нам зафиксировать некоторый контекст, чего не могли сделать предыдущие подходы. Давайте посмотрим, как это работает, используя тот же корпус, который мы использовали в предыдущих примерах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wE4F2qCx7NvL",
        "outputId": "1021a10b-9239-4cab-afe5-c08f8910bcef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#our corpus\n",
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khNajktSVfnS"
      },
      "source": [
        "##### CountVectorizer, который мы использовали для BoW, также можно использовать для получения представления Bag of N-grams, используя его аргумент ngram_range. Фрагмент кода ниже показывает, как это сделать:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "j7-VReVY5jZr",
        "outputId": "d4dfff0a-b6e4-4488-f6c7-525722e2fb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
            "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Пример векторизации Ngram с векторизатором count и uni, bi, trigrams\n",
        "count_vect = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "# Создайте BOW-представление для корпуса\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Посмотрим на сопоставление словаря\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "# смотрим на представителя BOW для первых 2 документов\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Получить представление, используя этот словарь, для нового текста\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNG5mUeWVohV"
      },
      "source": [
        "##### Обратите внимание, что количество признаков (и, следовательно, размер вектора признаков) значительно увеличилось для тех же данных по сравнению с представлениями, основанными на одном слове!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOOJRxOa23En"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE4VO40Gbl1U"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "Во всех других подходах, которые мы видели до сих пор, все слова в тексте считаются одинаково важными. Нет понятия, что некоторые слова в документе важнее других. TF-IDF решает эту проблему. Он направлен на количественную оценку важности данного слова по отношению к другим словам в документе и в корпусе. Это была широко используемая схема представления для информационно-поисковых систем для извлечения соответствующих документов из корпуса для заданного текстового запроса.\n",
        "\n",
        "В этм разделе показан простой пример того, как получить представление документа в формате TF-IDF с помощью TfidfVectorizer от sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5S4Ircomar27",
        "outputId": "085cac58-e2d6-4a1e-f0aa-f7f5d73ba7ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "BRjyqt4Ba2zx",
        "outputId": "9835422d-8ea9-4028-f31c-ac3493f224d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
            "----------\n",
            "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
            "----------\n",
            "TFIDF representation for all documents in our corpus\n",
            " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "----------\n",
            "Tfidf representation for 'dog and man are friends':\n",
            " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
        "\n",
        "#IDF для всех слов из словаря\n",
        "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
        "print(\"-\"*10)\n",
        "#Все слова из словаря\n",
        "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
        "print(\"-\"*10)\n",
        "\n",
        "#Представление TFIDF для всех документов в нашем корпусе \n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
        "print(\"-\"*10)\n",
        "\n",
        "temp = tfidf.transform([\"dog and man are friends\"])\n",
        "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OekabNOV6mVQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z89NDA-ToPnT"
      },
      "source": [
        "\n",
        "\n",
        "# pre-trained word2vec model\n",
        "\n",
        "Let us take an example of a pre-trained word2vec model, and how we can use it to look for most similar words. We will use the Google News vectors embeddings.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "Несколько других предварительно обученных моделей встраивания слов и подробности о средствах доступа к ним через gensim можно найти в:\n",
        "https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0LLCGJyjykp",
        "outputId": "fd4b8945-5c7f-4234-c3fd-1ebd8dc7332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=e102875c14fd8eca31a3ef253df4dfcee1d4bdac1391fc46450d8f2fe7d9794a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.6.0) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil==5.4.8 in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.2.4\n",
            "  Downloading spacy-2.2.4-cp37-cp37m-manylinux1_x86_64.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (3.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.0.8)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 61.6 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.21.6)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (2.23.0)\n",
            "Collecting thinc==7.4.0\n",
            "  Downloading thinc-7.4.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.6.15)\n",
            "Installing collected packages: srsly, plac, catalogue, blis, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.4 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 plac-1.1.3 spacy-2.2.4 srsly-1.0.5 thinc-7.4.0\n"
          ]
        }
      ],
      "source": [
        "# Библиотеки\n",
        "\n",
        "# ===========================\n",
        "\n",
        "!pip install wget==3.2\n",
        "!pip install gensim==3.6.0\n",
        "!pip install psutil==5.4.8\n",
        "!pip install spacy==2.2.4\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "QfuPQ10u9fi0",
        "outputId": "dd8ad7b6-2182-4241-c056-5c668ed75422"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "100%|██████████| 1.65G/1.65G [00:16<00:00, 97.1MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'GoogleNews-vectors-negative300.bin.gz'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "output = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTpzLd6dvB6Q",
        "outputId": "67fc2a07-bbaa-4d57-97db-902bbba48e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model at GoogleNews-vectors-negative300.bin\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "\n",
        "gn_vec_zip_path = \"/content/GoogleNews-vectors-negative300.bin.gz\"\n",
        "gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "#Extracting the required model\n",
        "with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
        "    with open(gn_vec_path, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "\n",
        "print(f\"Model at {gn_vec_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBsTuJ5FwAFm"
      },
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\") \n",
        "\n",
        "import psutil \n",
        "process = psutil.Process(os.getpid())\n",
        "from psutil import virtual_memory\n",
        "mem = virtual_memory()\n",
        "\n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aodBmqZToPnY",
        "outputId": "fb21e4b0-47b8-4908-c95a-5a7320f81628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory used in GB before Loading the Model: 0.17\n",
            "----------\n",
            "46.89 seconds taken to load\n",
            "----------\n",
            "Finished loading Word2Vec\n",
            "----------\n",
            "Memory used in GB after Loading the Model: 5.11\n",
            "----------\n",
            "Numver of words in vocablulary:  3000000\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "pretrainedpath = gn_vec_path\n",
        "\n",
        "#Загрузим модель W2V. Это займет некоторое время, но это одноразовое действие!\n",
        "pre = process.memory_info().rss\n",
        "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Проверить использование памяти перед загрузкой модели\n",
        "print('-'*10)\n",
        "\n",
        "start_time = time.time() #Start the timer\n",
        "ttl = mem.total #Toal memory available\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #загружаем модель\n",
        "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Рассчитаем общее время, прошедшее с момента запуска таймера\n",
        "print('-'*10)\n",
        "\n",
        "print('Finished loading Word2Vec')\n",
        "print('-'*10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Рассчитаем объем используемой памяти после загрузки модели\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) # размер словаря. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhJ_488PoPnr",
        "outputId": "281a2ddb-2080-45de-b6fd-8a15f4672b9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('kings', 0.7138046026229858),\n",
              " ('queen', 0.6510956883430481),\n",
              " ('monarch', 0.6413194537162781),\n",
              " ('crown_prince', 0.6204220056533813),\n",
              " ('prince', 0.6159993410110474),\n",
              " ('sultan', 0.5864822864532471),\n",
              " ('ruler', 0.5797567367553711),\n",
              " ('princes', 0.5646552443504333),\n",
              " ('Prince_Paras', 0.543294370174408),\n",
              " ('throne', 0.5422104597091675)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Давайте рассмотрим модель и узнаем, какие слова наиболее похожи на заданное слово\n",
        "w2v_model.most_similar('king')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Or5oG5oPn1",
        "outputId": "bf1b3c7f-94ef-4907-b773-1cc3f911df83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('russian', 0.6279364228248596),\n",
              " ('russia', 0.5842015743255615),\n",
              " ('norway', 0.5729843378067017),\n",
              " ('iranian', 0.555073618888855),\n",
              " ('munich', 0.5443724393844604),\n",
              " ('birmingham', 0.5430877208709717),\n",
              " ('sns_ap', 0.542165994644165),\n",
              " ('glasgow', 0.5389269590377808),\n",
              " ('belgium', 0.5388047099113464),\n",
              " ('serbia', 0.5383957028388977)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v_model.most_similar('moscow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtQiYOR9oPn_",
        "outputId": "8ff4dadf-5548-4236-d732-a12e346ebbce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
              "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
              "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
              "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
              "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
              "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
              "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
              "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
              "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
              "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
              "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
              "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
              "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
              "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
              "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
              "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
              "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
              "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
              "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
              "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
              "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
              "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
              "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
              "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
              "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
              "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
              "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
              "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
              "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
              "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
              "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
              "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
              "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
              "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
              "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
              "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
              "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
              "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
              "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
              "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
              "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
              "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
              "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
              "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
              "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
              "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
              "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
              "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
              "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
              "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
              "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
              "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
              "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
              "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
              "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
              "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
              "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
              "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
              "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
              "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
              "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
              "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
              "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
              "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
              "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
              "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
              "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
              "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
              "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
              "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
              "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
              "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
              "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
              "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
              "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Каково векторное представление слова?\n",
        "w2v_model['computer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "RoeH_gfroPoJ",
        "outputId": "18647ecc-ea7f-4bb7-8d92-baa3fc0f0773"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-354849ef77a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#What if I am looking for a word that is not in this vocabulary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'practicalnlp' not in vocabulary\""
          ]
        }
      ],
      "source": [
        "#Что произойдет, если я ищу слово, которого нет в этом словаре?\n",
        "w2v_model['practicalnlp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "douwnKS5oPoS"
      },
      "source": [
        "#### Две вещи, которые следует учитывать при использовании предварительно обученных моделей:\n",
        "\n",
        "\n",
        "1. Токены/слова всегда пишутся в нижнем регистре. Если слова нет в словаре, модель выдает исключение.\n",
        "2. Таким образом, всегда полезно инкапсулировать эти операторы в блоки try/except.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjWxh9kBoPob"
      },
      "source": [
        "# 2. Получение представления embedding для полного текста\n",
        "\n",
        "Мы увидели, как получить векторы встраивания для отдельных слов. Как мы можем их использовать, чтобы получить такое представление для полного текста? Простой способ — просто суммировать или усреднять вложения для отдельных слов.  Давайте рассмотрим небольшой пример с использованием другой библиотеки  Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_g9XOY9jykw",
        "outputId": "01034e36-4f00-4e9f-ce1b-66d14446a295",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2022.6.15)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051301 sha256=496be5f7779dd64b6d803ce4aeb5957d4b60f94d5002e08ef2db3a5bc4e0a7d1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cy3w8s1o/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "  Attempting uninstall: en-core-web-md\n",
            "    Found existing installation: en-core-web-md 3.4.0\n",
            "    Uninstalling en-core-web-md-3.4.0:\n",
            "      Successfully uninstalled en-core-web-md-3.4.0\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFLuSb9ZoPoc",
        "outputId": "41666ab7-6ea9-43d8-d045-bdb7104f6e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.25 µs\n",
            "[-1.68667185e+00  3.63152623e+00 -1.86475205e+00  2.05965996e+00\n",
            "  6.87502003e+00 -1.59911826e-01 -8.53519887e-02  4.76753950e+00\n",
            "  2.02361584e+00  4.52375889e-01  1.18858395e+01  1.47425997e+00\n",
            " -1.87467003e+00 -4.22039986e-01  6.31290078e-01  5.01420498e+00\n",
            "  1.88179994e+00  3.22938800e+00  4.47037935e-01 -5.82363963e-01\n",
            "  2.13324022e+00  5.29049993e-01 -4.60739994e+00  1.55904198e+00\n",
            " -9.90936100e-01 -9.84359920e-01 -1.28680015e+00 -5.29258060e+00\n",
            " -1.94400001e+00 -2.45310998e+00  3.86435986e-01  2.01385784e+00\n",
            " -1.36635196e+00 -2.10137796e+00 -3.41170430e+00  3.67221832e-01\n",
            "  3.84093940e-01  3.50381911e-01  2.49042010e+00 -1.41144001e+00\n",
            " -4.48249996e-01  1.35421813e+00  1.92985988e+00  2.82409966e-01\n",
            " -2.56616020e+00  8.20699990e-01  2.51210189e+00 -1.86108780e+00\n",
            " -1.20745206e+00  2.34802008e+00 -1.94518399e+00 -3.62380967e-02\n",
            "  3.37921786e+00 -4.72873974e+00 -1.83367968e-01 -7.04308033e-01\n",
            "  2.95983410e+00 -2.66211003e-01 -3.33119988e-01  1.87380031e-01\n",
            "  2.32199597e+00 -1.13518834e+00  9.13714051e-01 -1.47889805e+00\n",
            "  7.07140088e-01  3.80459964e-01 -2.03468013e+00 -3.69810748e+00\n",
            "  3.77295995e+00  2.12850404e+00 -7.30636418e-01  3.06329036e+00\n",
            " -5.57210016e+00  2.38870001e+00 -3.22135925e-01  1.89176595e+00\n",
            " -5.11249828e+00  4.55683613e+00 -3.27238202e+00 -8.00245941e-01\n",
            " -6.39266586e+00 -2.80339813e+00  2.99389982e+00  1.19373202e+00\n",
            "  2.66539001e+00  1.60532987e+00 -1.84056592e+00 -1.65469205e+00\n",
            "  6.02674770e+00 -4.15599966e+00 -1.23102808e+00 -4.55145359e+00\n",
            "  1.66986012e+00 -6.56453991e+00  7.40500033e-01 -1.93159032e+00\n",
            "  1.31657004e+00 -3.78807932e-01 -2.86300123e-01 -2.08037043e+00\n",
            "  1.75295985e+00  5.74750066e-01  5.48530006e+00  6.18292189e+00\n",
            "  3.86435926e-01  5.32993984e+00  1.89177394e+00 -3.75039995e-01\n",
            " -2.42636585e+00 -4.43650156e-01 -2.96096027e-01  5.24747181e+00\n",
            " -2.99599004e+00  2.34865999e+00  1.67798012e-01 -1.04866004e+00\n",
            " -2.20921993e+00 -6.51816010e-01  1.86275601e+00  6.81081951e-01\n",
            "  6.36240065e-01 -2.16522622e+00  3.75800617e-02  5.70195866e+00\n",
            " -3.52109981e+00 -9.49063969e+00 -2.29586029e+00 -1.53825009e+00\n",
            "  4.29331398e+00 -7.53485858e-01 -2.54183388e+00  6.70728028e-01\n",
            "  8.10830021e+00 -3.35032010e+00  9.29481983e-01  8.79678071e-01\n",
            "  2.70876837e+00  1.41599653e-02  2.20971012e+00 -3.60080004e+00\n",
            " -2.91006017e+00  5.96879959e-01  2.46352005e+00  3.06994390e+00\n",
            "  3.12636614e+00  1.51511991e+00 -5.62082005e+00 -6.07479930e-01\n",
            " -8.14783931e-01  3.82838964e+00  5.88952959e-01  3.88758421e+00\n",
            "  4.46843952e-01  2.81888604e+00  9.35156047e-01  1.49098802e+00\n",
            "  4.09620762e+00  2.47523808e+00  1.04704000e-01 -1.08481598e+00\n",
            " -2.53873587e+00 -1.73853087e+00  3.17180014e+00  3.03174973e+00\n",
            " -3.20362806e+00 -1.60944009e+00 -3.74270582e+00  5.85271358e+00\n",
            "  3.36617994e+00 -6.31366074e-01  1.58146799e+00 -3.97047937e-01\n",
            "  1.75186992e+00  3.23813200e+00 -1.42906040e-01 -2.75892878e+00\n",
            "  3.44453007e-01 -6.70707881e-01 -2.79648566e+00 -5.08149981e-01\n",
            " -1.64866221e+00  3.00323987e+00 -1.25469995e+00 -1.32326603e+00\n",
            " -2.76311994e+00  9.68442082e-01 -1.62222993e+00  1.17252004e+00\n",
            "  3.54200065e-01  3.52752376e+00 -4.10405919e-02 -1.50758195e+00\n",
            " -9.21526134e-01 -1.07006800e+00  3.39000010e+00  2.45498228e+00\n",
            " -3.71328020e+00 -1.60541987e+00  1.69847417e+00 -1.51808000e+00\n",
            "  6.06639460e-02 -2.99338007e+00 -2.70405984e+00 -2.65087414e+00\n",
            "  2.09087992e+00 -4.10079956e-02 -6.28958035e+00  1.59218001e+00\n",
            " -2.76947999e+00 -2.42465186e+00  3.56619978e+00  4.14844036e+00\n",
            " -2.18176007e+00  1.35117984e+00  4.83732843e+00 -3.78400147e-01\n",
            "  3.19898009e+00 -5.50105953e+00 -2.21033621e+00  3.14498007e-01\n",
            " -2.42452669e+00 -4.68890101e-01  1.80766177e+00 -2.22224617e+00\n",
            "  8.11996497e-03  1.60055950e-01  8.97642016e-01  1.70044005e+00\n",
            "  2.99607992e+00 -1.66711998e+00  3.94819975e+00 -4.67569447e+00\n",
            "  1.90508008e+00  6.02406025e+00 -4.01336765e+00  4.70955938e-01\n",
            "  1.21580005e-01 -2.17726398e+00  1.01895797e+00  4.33791935e-01\n",
            "  2.35771990e+00  1.36332202e+00  2.09951806e+00  3.41598010e+00\n",
            " -3.97149205e+00 -1.27723396e+00 -3.93534613e+00  6.87535942e-01\n",
            "  9.97166038e-01  4.24554205e+00  7.43211865e-01 -3.19643998e+00\n",
            " -6.87979984e+00 -3.80437016e+00 -2.62940019e-01 -3.89886022e+00\n",
            "  3.15305209e+00  1.44702387e+00  1.29817796e+00  8.08620229e-02\n",
            " -4.41340119e-01  6.21439981e+00  3.61454010e+00  4.79072332e+00\n",
            " -8.65180016e-01  2.83331834e-02 -9.61987972e-01  2.60946393e+00\n",
            " -5.43237972e+00  5.88221967e-01  4.21081972e+00 -4.26979929e-01\n",
            "  1.61476046e-01 -3.02674007e+00  1.81719589e+00  4.33688074e-01\n",
            "  1.83541608e+00  4.49680030e-01 -8.20295930e-01  7.35859990e-01\n",
            "  1.61718404e+00 -3.08763814e+00 -6.59479916e-01 -1.13058293e+00\n",
            "  6.32529593e+00 -1.10160804e+00  3.99896002e+00  8.34416032e-01\n",
            " -2.43635607e+00  3.50229001e+00  1.39701998e+00 -6.07439935e-01\n",
            " -5.50632000e-01  2.36399603e+00 -1.85294187e+00  3.08667231e+00\n",
            "  1.61839485e+00 -7.71940202e-02 -3.80113411e+00  4.52078009e+00]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "%time \n",
        "nlp = spacy.load('en_core_web_md')\n",
        "# обработать предложение используя модель\n",
        "mydoc = nlp(\"Russia is a large country\")\n",
        "#Получить вектор для отдельных слов\n",
        "#print(doc[0].vector) #vector для «Russia», первое слово в тексте\n",
        "print(mydoc.vector) #Усредненный вектор для всего предложения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-8wrQlLoPol",
        "outputId": "9cbcb1ea-6c56-4eac-b0e9-a0db8a3dcba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Что происходит, когда я даю предложение со странными словами (и стоп-словами) и пытаюсь получить его вектор слов в Spacy?\n",
        "temp = nlp('practicalnlp is a newword')\n",
        "temp[0].vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYdcl2Qe6mb5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7gQFbUxOQtb"
      },
      "source": [
        "# Tokenization, Lemmatization, Stemming,  Sentence Segmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7qsKcOFaH2"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1h0ZNzohff1nUWMerrW50eDxY99ArRJTK)\n",
        "\n",
        "В любой типичной задаче НЛП одним из первых шагов является разбиение фрагментов текста на отдельные слова/лексемы (процесс показан на рисунке выше), результат которого используется для создания так называемых словарей, которые будут использоваться в модель языка, которую вы планируете построить. На самом деле это одна из техник, которую мы будем использовать чаще всего в этой серии, но здесь мы придерживаемся основ.\n",
        "\n",
        "Ниже пример простого токенизатора без каких-либо стандартов. Все, что он делает, это извлекает токены на основе разделителя пробелов.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn7xM8HKqAtf"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip install -U spacy\n",
        "!pip install -U spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A3g4Z_68LKsO",
        "outputId": "c5e939d8-0aaf-442f-b194-474aec68e1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.2.4\n",
            "  Using cached spacy-2.2.4-cp37-cp37m-manylinux1_x86_64.whl (10.6 MB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (0.10.1)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Using cached srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Collecting thinc==7.4.0\n",
            "  Using cached thinc-7.4.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Using cached blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (3.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.10)\n",
            "Installing collected packages: srsly, catalogue, blis, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.4 which is incompatible.\n",
            "en-core-web-md 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.4 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 spacy-2.2.4 srsly-1.0.5 thinc-7.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "catalogue",
                  "spacy",
                  "srsly",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install spacy==2.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUhMRrhFGfqJ",
        "outputId": "b12bb420-5721-455b-d883-0e70772ec729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 0: I\n",
            "Token 1: love\n",
            "Token 2: coding\n",
            "Token 3: and\n",
            "Token 4: writing\n"
          ]
        }
      ],
      "source": [
        "## токенизация фрагмента текста\n",
        "doc = \"I love coding and writing\"\n",
        "for i, w in enumerate(doc.split(\" \")):\n",
        "    print(\"Token \" + str(i) + \": \" + w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Med-k0CeG8Ke"
      },
      "source": [
        "Все, что делает код, — это разделяет предложение на отдельные токены. Приведенный выше простой блок кода хорошо работает с текстом, который я предоставил. Но обычно текст намного шумнее и сложнее, чем в примере, который я использовал. Например, если бы я использовал слово «так называемый», это было бы одно слово или два слова? Для таких сценариев могут потребоваться более продвинутые подходы к токенизации. Вы можете убрать «-» и разделить на два токена или просто объединить в один токен, но все зависит от проблемы и домена, над которым вы работаете.\n",
        "\n",
        "Еще одна проблема с нашим простым алгоритмом заключается в том, что он не может обрабатывать лишние пробелы в тексте. Кроме того, как нам быть с такими городами, как «Нью-Йорк» и «Сан-Франциско»?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0qxNrl191NS"
      },
      "source": [
        "---\n",
        "**Упражнение 1**: Скопируйте приведенный выше код и добавьте дополнительные пробелы в строковое значение, присвоенное переменной `doc`, и определите проблему с кодом. Затем попытайтесь решить проблему. Подсказка: используйте `text.strip()`, чтобы решить проблему."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx22yqPJQCQc"
      },
      "outputs": [],
      "source": [
        "###  ENTER CODE HERE\n",
        "\n",
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpYLDmLu9379"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSQwXKwrQAp0"
      },
      "source": [
        "Токенизация также может принимать различные формы. Например, в последнее время многие современные модели NLP, такие как [BERT] (https://arxiv.org/pdf/1810.04805.pdf), используют токены «подслов», в которых часто встречаются комбинации символов. также входят в состав словарного запаса. Это помогает справиться с так называемой проблемой словарного запаса (OOV).  [статьей] (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive). /37842.pdf).\n",
        "\n",
        "Чтобы продемонстрировать, как можно добиться более надежной токенизации, мы разберем бибилиотеку [spaCy](https://spacy.io/), для обработки естественного языка, встроенный токенизатор, можно найти [здесь] (https://spacy.io/usage/linguistic-features#sbd-custom).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpinv_FjoyVx",
        "outputId": "3123ac34-079e-49f5-e462-7956017281d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This\n",
            "is\n",
            "the\n",
            "so\n",
            "-\n",
            "called\n",
            "lemmatization\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "## загрузим модель\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "## токенизация фрагмента текста\n",
        "doc = nlp(\"This is the so-called lemmatization\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl6JG5yirhn0"
      },
      "source": [
        "Все, что делает код, — это токенизирует текст на основе предварительно созданной языковой модели.\n",
        "\n",
        "Попробуйте поместить другой текущий текст в часть `nlp()` кода выше. Токенизатор довольно надежен и включает в себя ряд встроенных правил, которые касаются исключений и особых случаев, таких как те токены, которые содержат знаки пунктуации, такие как «`» и «.», «-» и т. д. Вы даже можете добавить свои собственные правила, узнайте как [здесь](https://spacy.io/usage/linguistic-features#special-cases).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-F16mbBkVXF"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1_-wxBOU_JebjdG1sxoobKYRCtX3dVF0L)\n",
        "\n",
        "[Лемматизация] (https://en.wikipedia.org/wiki/Лемматизация) — это процесс, в котором мы берем отдельные токены из предложения и пытаемся привести их к их *основной* форме. Процесс, который делает это возможным, заключается в наличии словарного запаса и выполнении морфологического анализа для удаления флективных окончаний. Результатом процесса лемматизации (как показано на рисунке выше) является *лемма* или базовая форма слова. Например, процесс лемматизации сводит флексии «am», «are» и «is» к базовой форме «be». Взгляните на рисунок выше для полного примера и попытайтесь понять, что он делает.\n",
        "\n",
        "Лемматизация полезна для нормализации текста для задач классификации текста или поисковых систем, а также для множества других задач НЛП, таких как [классификация настроений] (https://en.wikipedia.org/wiki/Sentiment_analysis). Это особенно важно при работе со сложными языками, такими как арабский и испанский.\n",
        "\n",
        "Чтобы показать, как можно достичь лемматизации и как она работает, мы снова воспользуемся [spaCy](https://spacy.io/). Используя класс spaCy [Lemmatizer](https://spacy.io/api/lemmatizer#_title), мы собираемся преобразовать несколько слов в их леммы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5QgWANL3JbD",
        "outputId": "7ed7aa49-eaa9-4ede-d50c-5e0fd8470f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I => -PRON-\n",
            "love => love\n",
            "coding => code\n",
            "and => and\n",
            "writing => writing\n"
          ]
        }
      ],
      "source": [
        "from spacy.lemmatizer import Lemmatizer\n",
        "from spacy.lookups import Lookups\n",
        "\n",
        "## lemmatization\n",
        "doc = nlp(u'I love coding and writing')\n",
        "for word in doc:\n",
        "    print(word.text, \"=>\", word.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgDcWgMvOFqA"
      },
      "source": [
        "Приведенные выше результаты выглядят так, как и ожидалось. Единственная лемма, которая выглядит неправильно, - это `-PRON-`, возвращаемый для токена \"I\". Согласно документации spaCy, «*Это на самом деле ожидаемое поведение, а не ошибка. В отличие от глаголов и нарицательных существительных, нет четкой базовой формы личного местоимения. Должна ли лемма «я» быть «я», или мы должны также нормализовать лицо, давая «это» — или, может быть, «он»? Решение spaCy состоит в том, чтобы ввести новый символ -ПРОН-, который используется в качестве леммы для всех личных местоимений.*\"\n",
        "\n",
        "Узнайте больше об этом в [документации spaCy](https://spacy.io/api/annotation#lemmatization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc6wkiW-ANT6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUB3wRFhkczV"
      },
      "source": [
        "**Упражнение 2.** Попробуйте выполнить приведенный выше код с другими предложениями и посмотрите, не получите ли вы неожиданных результатов. Кроме того, попробуйте добавить знаки препинания и дополнительные пробелы, которые чаще встречаются в естественном языке. Что случается?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnfPOGgYkr3h"
      },
      "outputs": [],
      "source": [
        "### ENTER CODE HERE\n",
        "\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALKZxh54APho"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOdA8GxMta7N"
      },
      "source": [
        "Мы также можем создать собственный лемматизатор, как показано ниже (*код взят непосредственно с веб-сайта spaCy*):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwtdub8er-sU",
        "outputId": "f044615a-a84e-4d29-8237-56b72622e872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat']\n",
            "['bring']\n",
            "['sing']\n"
          ]
        }
      ],
      "source": [
        "## lookup tables\n",
        "lookups = Lookups()\n",
        "lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
        "lemmatizer = Lemmatizer(lookups)\n",
        "\n",
        "words_to_lemmatize = [\"cats\", \"brings\", \"sings\"]\n",
        "\n",
        "for w in words_to_lemmatize:\n",
        "    lemma = lemmatizer(w, \"NOUN\")\n",
        "    print(lemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzL2K-sU-e3M"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcaLqxPX5CJa"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1XcK3OzdPd2ywO8Y4G6vfjuIFthPce3FH)\n",
        "\n",
        "Стемминг — это просто более простая версия лемматизации, в которой мы заинтересованы в удалении *суффикса* в конце слова. При выделении корня нам интересно свести *измененное* или *производное* слово к его базовой форме. Взгляните на рисунок выше, чтобы получить некоторое представление о процессе.\n",
        "\n",
        "Процессы образования основы и лемматизации включают [*морфологический анализ*] (https://en.wikipedia.org/wiki/Morphology_(linguistics)), где основы и аффиксы (называемые *морфемами*) извлекаются и используются для сокращения к их основной форме. Например, слово *cats* имеет две морфемы, *cat* и *s*, причем *cat* является основой, а *s* является аффиксом, обозначающим множественность.\n",
        "\n",
        "spaCy не поддерживает выделение корней, поэтому мы будем использовать [NLTK] (https://www.nltk.org/), еще одну замечательную библиотеку Python NLP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lVd74BE5BXK",
        "outputId": "c3fb6cd6-e9b8-42f5-dee9-5187c0fb2ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I => i\n",
            "prefer => prefer\n",
            "not => not\n",
            "to => to\n",
            "argue => argu\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "doc = 'I prefer not to argue'\n",
        "for token in doc.split(\" \"):\n",
        "    print(token, '=>' , stemmer.stem(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdbxwdMw8AAD"
      },
      "source": [
        "Обратите внимание, что корень слова «argue» — «argu». Это потому, что мы можем получить такие слова, как \"argument\", \"arguing\", \"argued\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBG0l7CsBhAz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa5xGWDVBild"
      },
      "source": [
        "**Упражнение 3.** Попробуйте использовать разные предложения в приведенном выше коде и понаблюдайте за эффектом парадигматического модуля. Кстати, в библиотеке NLTK есть и другие стеммеры, такие как стеммер Porter. Каждый стеммер ведет себя по-разному, поэтому вывод может отличаться. Не стесняйтесь попробовать [стеммер Porter] (https://www.nltk.org/howto/stem.html) из библиотеки NLTK и проверить вывод различных парадигматических модулей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vow0MVZxmQQq"
      },
      "outputs": [],
      "source": [
        "###  ENTER CODE HERE\n",
        "\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKWRXPulY2y2"
      },
      "source": [
        "# BPE (Byte-Pair Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nKjVY2NY2Cn",
        "outputId": "fcd60308-57e8-4123-9b71-a96a4cc691c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bpe\n",
            "  Downloading bpe-1.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from bpe) (3.6.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from bpe) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpe) (4.64.0)\n",
            "Collecting hypothesis\n",
            "  Downloading hypothesis-6.54.5-py3-none-any.whl (390 kB)\n",
            "\u001b[K     |████████████████████████████████| 390 kB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from bpe) (3.7)\n",
            "Collecting mypy\n",
            "  Downloading mypy-0.971-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.0 MB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis->bpe) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis->bpe) (22.1.0)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting typed-ast<2,>=1.4.0\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mypy->bpe) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10 in /usr/local/lib/python3.7/dist-packages (from mypy->bpe) (4.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->bpe) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->bpe) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->bpe) (2022.6.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (8.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (57.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (1.15.0)\n",
            "Installing collected packages: typed-ast, mypy-extensions, exceptiongroup, mypy, hypothesis, bpe\n",
            "\u001b[33m  WARNING: The scripts dmypy, mypy, mypyc, stubgen and stubtest are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script hypothesis is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed bpe-1.0 exceptiongroup-1.0.0rc9 hypothesis-6.54.5 mypy-0.971 mypy-extensions-0.4.3 typed-ast-1.5.4\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --user bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqMNddsQY2Fa",
        "outputId": "535fc1bf-1c0b-4cf5-f803-cd0124441a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__sow', 'p', 'ra', 'c', 'ti', 'c', 'al', 'n', 'l', 'p', '__eow', '__sow', 'is', '__eow', '__sow', 'a', '__eow', '__sow', 'ne', 'w', 'w', 'or', 'd', '__eow']\n",
            "[25, 40, 50, 34, 112, 34, 63, 39, 31, 40, 24, 25, 45, 24, 25, 33, 24, 25, 107, 56, 56, 60, 36, 24]\n",
            "practicalnlp is a newword\n"
          ]
        }
      ],
      "source": [
        "from bpe import Encoder\n",
        "\n",
        "# Generated with http://pythonpsum.com\n",
        "test_corpus = '''\n",
        "    Object raspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
        "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future. Python raspberrypi community pypy. Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
        "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
        "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future integration mercurial self script web. Return raspberrypi community test she stable.\n",
        "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
        "'''\n",
        "\n",
        "encoder = Encoder(200, pct_bpe=0.88)  \n",
        "encoder.fit(test_corpus.split('\\n'))\n",
        "\n",
        "example = \"practicalnlp is a newword\"\n",
        "print(encoder.tokenize(example))\n",
        "print(next(encoder.transform([example])))\n",
        "print(next(encoder.inverse_transform(encoder.transform([example]))))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "846dd53c5a100503afcb3f5301bb10f61481596a80ae839ecd432be859b5d4d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
