{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "__COLAB_ACTIVE = False\n",
        "__POOL_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Проект 3. Решить задачу DaNetQA / BoolQ\n",
        "\n",
        "Можно решить как задачу для русского, так и для английского.\n",
        "\n",
        "Либо провести эксперименты с многоязычной моделью\n",
        "\n",
        "https://russiansuperglue.com/ru/tasks/task_info/DaNetQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Описание\n",
        "Причинно-следственная связь, логический вывод, Natural Language Inference\n",
        "\n",
        "DaNetQA - это набор да/нет вопросов с ответами и фрагментом текста, содержащим ответ. Все вопросы были написаны авторами без каких-либо искусственных ограничений.\n",
        "\n",
        "Каждый пример представляет собой триплет (вопрос, фрагмент текста, ответ) с заголовком страницы в качестве необязательного дополнительного контекста.\n",
        "\n",
        "Настройка классификации текстовых пар аналогична существующим задачам логического вывода (NLI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тип задачи\n",
        "Логика, Commonsense, Знания о мире. Бинарная классификация: true/false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root path: 'd:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation'\n",
            "Dataset path: d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\DaNetQA\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "base_path = os.path.abspath('')\n",
        "if __COLAB_ACTIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = os.path.join(base_path, 'drive/MyDrive/DSnML_Innopolis2022')\n",
        "\n",
        "print(f\"Root path: '{base_path}'\")\n",
        "\n",
        "trainPartNameRaw = 'raw_train'\n",
        "testPartNameRaw = 'raw_val'\n",
        "validatePartNameRaw = 'raw_test'\n",
        "\n",
        "trainPartName = 'train_v1'\n",
        "testPartName = 'val_v1'\n",
        "validatePartName = 'test_v1'\n",
        "parts = [trainPartName, testPartName]\n",
        "data_path = os.path.join(base_path, 'DaNetQA')\n",
        "print(f\"Dataset path: {data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fileNameData(s):\n",
        "    return f\"{os.path.join(data_path, s)}.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\leysh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadJSONL(path, name):\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    print(name)\n",
        "    display(df.head())\n",
        "    if (df.columns.values == 'label').any():\n",
        "        s = np.unique(df['label'].to_numpy(), return_counts=True)[1]\n",
        "        print(f\"True answer: {s[1]}\")\n",
        "        print(f\"False answer: {s[0]}\")\n",
        "        print(\"\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>«Вы́ставочный центр» — станция Московского мон...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>Вы́ставка достиже́ний наро́дного хозя́йства  ,...</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Был ли джиган в black star?</td>\n",
              "      <td>Вместе с этим треком они выступили на церемони...</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Xiaomi конкурент apple?</td>\n",
              "      <td>Xiaomi — китайская компания, основанная в 2010...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли автомат калашникова в вов?</td>\n",
              "      <td>Отметив некоторые недостатки и в целом удачную...</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            question  \\\n",
              "0      Вднх - это выставочный центр?   \n",
              "1      Вднх - это выставочный центр?   \n",
              "2        Был ли джиган в black star?   \n",
              "3            Xiaomi конкурент apple?   \n",
              "4  Был ли автомат калашникова в вов?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  «Вы́ставочный центр» — станция Московского мон...   True    0  \n",
              "1  Вы́ставка достиже́ний наро́дного хозя́йства  ,...   True    1  \n",
              "2  Вместе с этим треком они выступили на церемони...   True    2  \n",
              "3  Xiaomi — китайская компания, основанная в 2010...   True    3  \n",
              "4  Отметив некоторые недостатки и в целом удачную...  False    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 1061\n",
            "False answer: 688\n",
            "\n",
            "Test set:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>вода марсе ?</td>\n",
              "      <td>гидросфера марса — это совокупность водных зап...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>состоит англия евросоюзе ?</td>\n",
              "      <td>полночь 31 января 1 февраля 2020 года централь...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>деиствительно ссср адвокатов ?</td>\n",
              "      <td>семен львович ария — советскии россиискии юрис...</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>чума оране ?</td>\n",
              "      <td>чума — это абсурд , осмысливается форма сущест...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>кетчуп читосе ?</td>\n",
              "      <td>текущии каталог продукции размещен саите произ...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         question  \\\n",
              "0                    вода марсе ?   \n",
              "1      состоит англия евросоюзе ?   \n",
              "2  деиствительно ссср адвокатов ?   \n",
              "3                    чума оране ?   \n",
              "4                 кетчуп читосе ?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  гидросфера марса — это совокупность водных зап...   True    0  \n",
              "1  полночь 31 января 1 февраля 2020 года централь...  False    1  \n",
              "2  семен львович ария — советскии россиискии юрис...  False    2  \n",
              "3  чума — это абсурд , осмысливается форма сущест...   True    3  \n",
              "4  текущии каталог продукции размещен саите произ...   True    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 412\n",
            "False answer: 409\n",
            "\n",
            "Validation set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полезна ли ртуть с градусника?</td>\n",
              "      <td>Отравления ртутью  — расстройства здоровья, св...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Являются ли сапрофаги хищниками?</td>\n",
              "      <td>Фауна лесных почв — совокупность видов животны...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Водятся ли в индии крокодилы?</td>\n",
              "      <td>Болотный крокодил, или магер  — пресмыкающееся...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Есть ли в батате крахмал?</td>\n",
              "      <td>Клубневидно вздутые корни  весят до 15 кг, сод...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли человек в железной маске?</td>\n",
              "      <td>Остров Сент-Маргерит  — крупнейший из Лерински...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           question  \\\n",
              "0    Полезна ли ртуть с градусника?   \n",
              "1  Являются ли сапрофаги хищниками?   \n",
              "2     Водятся ли в индии крокодилы?   \n",
              "3         Есть ли в батате крахмал?   \n",
              "4  Был ли человек в железной маске?   \n",
              "\n",
              "                                             passage  idx  \n",
              "0  Отравления ртутью  — расстройства здоровья, св...    0  \n",
              "1  Фауна лесных почв — совокупность видов животны...    1  \n",
              "2  Болотный крокодил, или магер  — пресмыкающееся...    2  \n",
              "3  Клубневидно вздутые корни  весят до 15 кг, сод...    3  \n",
              "4  Остров Сент-Маргерит  — крупнейший из Лерински...    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_train = loadJSONL(fileNameData(trainPartNameRaw), \"Train set\")\n",
        "df_test = loadJSONL(fileNameData(testPartNameRaw), \"Test set:\")\n",
        "df_validation = loadJSONL(fileNameData(validatePartNameRaw), \"Validation set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Очистка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataCleaner:\n",
        "    def __init__(self) -> None:\n",
        "        self.flag_verbose = True\n",
        "\n",
        "        self.stop_words = stopwords.words('russian')\n",
        "        self.stemmer = SnowballStemmer('russian')\n",
        "\n",
        "        self.count_removed_symbols = dict()\n",
        "        self.count_removed_words = dict()\n",
        "\n",
        "        self.count_replaced_symbols = dict()\n",
        "        self.dict_replaced_symbols = dict()\n",
        "\n",
        "        self.count_replaced_words = dict()\n",
        "        self.dict_replaced_words = dict()\n",
        "\n",
        "        self.char_to_remove = ['«', '»', '—', ',', '.', '-', '/', ':', '!', \"?\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"=\", \"|\", \"\\\\\", \">\", \"<\"]\n",
        "        self.char_to_replace = [['ё', 'е']]\n",
        "\n",
        "    # функция подсчета количества измененных слов\n",
        "    def addReplacedWord(self, s_from, s_to = ' '):\n",
        "        if not self.count_replaced_words.keys().__contains__(s_from):\n",
        "            self.count_replaced_words[s_from] = 0\n",
        "        self.count_replaced_words[s_from] += 1\n",
        "        self.dict_replaced_words[s_from] = s_to\n",
        "\n",
        "    # функция подсчета количества удаленных слов\n",
        "    def addRemovedWord(self, w):\n",
        "        if w == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(w):\n",
        "                self.count_removed_symbols[w] = 0\n",
        "            self.count_removed_symbols[w] += 1\n",
        "\n",
        "    # функция подсчета количества удаленных символов\n",
        "    def addReplacedSymbol(self, s_from, s_to = ' '):\n",
        "        if s_to == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(s_from):\n",
        "                self.count_removed_symbols[s_from] = 0\n",
        "            self.count_removed_symbols[s_from] += 1\n",
        "        else:\n",
        "            if not self.count_replaced_symbols.keys().__contains__(s_from):\n",
        "                self.count_replaced_symbols[s_from] = 0\n",
        "            self.count_replaced_symbols[s_from] += 1\n",
        "            self.dict_replaced_symbols[s_from] = s_to\n",
        "\n",
        "    # удаление знаков ударения и прочих символов unicode\n",
        "    def unicodeToAscii(self, s):\n",
        "        tmp = []\n",
        "        for c in unicodedata.normalize('NFD', s):\n",
        "            if unicodedata.category(c) != 'Mn':\n",
        "                tmp.append(c)\n",
        "            else:\n",
        "                self.addReplacedSymbol(c)\n",
        "        return ''.join(tmp)\n",
        "\n",
        "    # если нужно удалить, то заменяем на пробел чтоб не потерят разделения слов\n",
        "    def replaceChar(self, s):\n",
        "        tmp = []\n",
        "        for i, c in enumerate(s):\n",
        "            if self.char_to_remove.__contains__(c):\n",
        "                self.addReplacedSymbol(c, s[i])\n",
        "                tmp.append(' ')\n",
        "            else:\n",
        "                tmp.append(c)\n",
        "        s = \"\".join(tmp)\n",
        "\n",
        "        for s_from, s_to in self.char_to_replace:\n",
        "            if c == s_from:\n",
        "                s[i] = s_to\n",
        "                self.addReplacedSymbol(s_from, s_to)\n",
        "        return s\n",
        "\n",
        "    # удаляем лишние пробелы\n",
        "    def trimSpaces(self, s):\n",
        "        while s.__contains__('  '):\n",
        "            s = s.replace('  ', ' ')\n",
        "        s = s.strip()\n",
        "        return s\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def removeStopWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            if word not in self.stop_words:\n",
        "                tmp.append(word)\n",
        "            else:\n",
        "                self.addRemovedWord(word)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def StemmWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            wordStemmed = self.stemmer.stem(word)\n",
        "            tmp.append(wordStemmed)\n",
        "            if word != wordStemmed:\n",
        "                self.addReplacedWord(word, wordStemmed)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    def clean(self, df, column):\n",
        "        for i in range(len(df)):\n",
        "            df[column][i] = self.unicodeToAscii(df[column][i])\n",
        "            df[column][i] = df[column][i].lower()\n",
        "            df[column][i] = self.replaceChar(df[column][i])\n",
        "            df[column][i] = self.removeStopWords(df[column][i])\n",
        "            df[column][i] = self.StemmWords(df[column][i])\n",
        "            df[column][i] = self.trimSpaces(df[column][i])\n",
        "        return df\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def print(self, vals):\n",
        "        if self.flag_verbose == True:\n",
        "            print(vals)\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def display(self, vals):\n",
        "            if self.flag_verbose == True:\n",
        "                display(vals)\n",
        "\n",
        "    # сбор лога в dataframe, опциональный вывод на экран \n",
        "    def summary(self, verbose = True):\n",
        "        self.flag_verbose = verbose\n",
        "        dfs = []\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Chars        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_symbols:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_words:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Words', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Replaced Chars       ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol_from\", \"symbol_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_symbols:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_symbols[c], self.count_replaced_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Replaced Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Stemmed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word_from\", \"word_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_words:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_words[c], self.count_replaced_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Stemmed Words', dfRemoved])\n",
        "\n",
        "        return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:102: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.unicodeToAscii(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:103: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = df[column][i].lower()\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.replaceChar(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:105: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.removeStopWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.StemmWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.trimSpaces(df[column][i])\n"
          ]
        }
      ],
      "source": [
        "t = DataCleaner()\n",
        "df_train = t.clean(df_train, 'passage')\n",
        "df_test = t.clean(df_test, 'passage')\n",
        "df_validation = t.clean(df_validation, 'passage')\n",
        "df_train = t.clean(df_train, 'question')\n",
        "df_test = t.clean(df_test, 'question')\n",
        "df_validation = t.clean(df_validation, 'question')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Chars        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>́</td>\n",
              "      <td>1131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>̆</td>\n",
              "      <td>20853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>̈</td>\n",
              "      <td>4020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>̔</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>̀</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>̣</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>̌</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>̥</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>̂</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>̊</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>̓</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>͂</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ُ</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>̄</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>̃</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol count_removed\n",
              "0       ́          1131\n",
              "1       ̆         20853\n",
              "2       ̈          4020\n",
              "3       ̔             5\n",
              "4       ̀             2\n",
              "5       ̣             6\n",
              "6       ̌             1\n",
              "7       ̥             1\n",
              "8       ̂             2\n",
              "9       ̊             2\n",
              "10      ̓             3\n",
              "11      ͂             1\n",
              "12      ُ             2\n",
              "13      ̄             1\n",
              "14      ̃             1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [word, count_removed]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Replaced Chars       ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol_from</th>\n",
              "      <th>symbol_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«</td>\n",
              "      <td>«</td>\n",
              "      <td>2246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>»</td>\n",
              "      <td>»</td>\n",
              "      <td>2233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>—</td>\n",
              "      <td>—</td>\n",
              "      <td>4524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>19367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>4010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>27875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>)</td>\n",
              "      <td>)</td>\n",
              "      <td>1430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>:</td>\n",
              "      <td>:</td>\n",
              "      <td>1395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>%</td>\n",
              "      <td>%</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/</td>\n",
              "      <td>/</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>|</td>\n",
              "      <td>|</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>]</td>\n",
              "      <td>]</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[</td>\n",
              "      <td>[</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>!</td>\n",
              "      <td>!</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>&lt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>3439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>&amp;</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>=</td>\n",
              "      <td>=</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>$</td>\n",
              "      <td>$</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>#</td>\n",
              "      <td>#</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(</td>\n",
              "      <td>(</td>\n",
              "      <td>1337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>{</td>\n",
              "      <td>{</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>}</td>\n",
              "      <td>}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol_from symbol_to count_replaced\n",
              "0            «         «           2246\n",
              "1            »         »           2233\n",
              "2            —         —           4524\n",
              "3            .         .          19367\n",
              "4            -         -           4010\n",
              "5            ,         ,          27875\n",
              "6            )         )           1430\n",
              "7            :         :           1395\n",
              "8            %         %            348\n",
              "9            /         /            198\n",
              "10           |         |              7\n",
              "11           ]         ]            154\n",
              "12           [         [            149\n",
              "13           !         !             54\n",
              "14           *         *             13\n",
              "15           <         <              6\n",
              "16           >         >             19\n",
              "17           ?         ?           3439\n",
              "18           &         &              5\n",
              "19           =         =              9\n",
              "20           $         $             10\n",
              "21           #         #              6\n",
              "22           (         (           1337\n",
              "23           {         {              3\n",
              "24           }         }              1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Stemmed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_from</th>\n",
              "      <th>word_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>выставочныи</td>\n",
              "      <td>выставочны</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>станция</td>\n",
              "      <td>станц</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>московского</td>\n",
              "      <td>московск</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>монорельса</td>\n",
              "      <td>монорельс</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>расположена</td>\n",
              "      <td>располож</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50960</th>\n",
              "      <td>себестоимость</td>\n",
              "      <td>себестоим</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50961</th>\n",
              "      <td>ювелирная</td>\n",
              "      <td>ювелирн</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50962</th>\n",
              "      <td>завоеван</td>\n",
              "      <td>завоева</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50963</th>\n",
              "      <td>новорожденные</td>\n",
              "      <td>новорожден</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50964</th>\n",
              "      <td>бодрит</td>\n",
              "      <td>бодр</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50965 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word_from     word_to count_replaced\n",
              "0        выставочныи  выставочны              7\n",
              "1            станция       станц              7\n",
              "2        московского    московск             26\n",
              "3         монорельса   монорельс              2\n",
              "4        расположена    располож             21\n",
              "...              ...         ...            ...\n",
              "50960  себестоимость   себестоим              1\n",
              "50961      ювелирная     ювелирн              1\n",
              "50962       завоеван     завоева              1\n",
              "50963  новорожденные  новорожден              1\n",
              "50964         бодрит        бодр              1\n",
              "\n",
              "[50965 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dfs = t.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'выставк достижен народн хозяиств 1959 1991 год выставк достижен народн хозяиств ссср 1992 2014 год всероссииск выставочны центр выставочны комплекс останкинск раион север восточн административн округ город москв второ величин выставочны комплекс город вход 50 крупнеиш выставочн центр мир ежегодн вднх посеща 30 млн гост 1 август 2019 год выставк отпразднова 80 летн юбил территориальн вднх объедин парк останкин главн ботаническ сад общ площад составля 700 га 240 2 га площад вднх 75 6 га площад парк останкин 361 га площад гбс 9 5 га музеин выставочны центр рабоч колхозниц площад арко главн вход территор выставк располож множеств шедевр архитектур 49 объект вднх призна памятник культурн наслед'"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.passage[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.to_json(fileNameData(trainPartName), force_ascii=False, lines=True, orient='records')\n",
        "df_test.to_json(fileNameData(testPartNameRaw), force_ascii=False, lines=True, orient='records')\n",
        "df_validation.to_json(fileNameData(validatePartName), force_ascii=False, lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF + LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import codecs\n",
        "import json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_feature_DaNetQA(row):\n",
        "    res = str(row[\"question\"]).strip()\n",
        "    label = row.get(\"label\")\n",
        "    return res, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_features_DaNetQA(path, vect):\n",
        "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
        "        lines = reader.read().split(\"\\n\")\n",
        "        lines = list(map(json.loads, filter(None, lines)))\n",
        "    res = list(map(build_feature_DaNetQA, lines))\n",
        "    texts = list(map(lambda x: x[0], res))\n",
        "    labels = list(map(lambda x: x[1], res))\n",
        "    ids = [x[\"idx\"] for x in lines]\n",
        "    return (vect.transform(texts), labels), ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_DaNetQA(train, labels):\n",
        "    clf = LogisticRegression()\n",
        "    return clf.fit(train, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_DaNetQA(train_path, val_path, test_path, vect):\n",
        "    train, _ = build_features_DaNetQA(train_path, vect)\n",
        "    val, _ = build_features_DaNetQA(val_path, vect)\n",
        "    test, ids = build_features_DaNetQA(test_path, vect)\n",
        "    clf = fit_DaNetQA(*train)\n",
        "    try:\n",
        "        test_score = clf.score(*test)\n",
        "    except ValueError:\n",
        "        test_score = None\n",
        "    test_pred = clf.predict(test[0])\n",
        "    return clf, {\n",
        "        \"train\": clf.score(*train),\n",
        "        \"val\": clf.score(*val),\n",
        "        \"test\": test_score,\n",
        "        \"test_pred\": [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(ids, test_pred)]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pre-Trained TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://russiansuperglue.com/tasks/tf_idf\n",
        "!unzip tf_idf_baseline.zip\n",
        "!rm tf_idf_baseline.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vect = joblib.load(\"tfidf.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Score Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = fileNameData(trainPartNameRaw)\n",
        "test_path = fileNameData(testPartNameRaw)\n",
        "val_path = fileNameData(validatePartNameRaw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.8010291595197255\n",
            "Accuracy on validation data = 0.5091352009744214\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_scores = eval_DaNetQA(train_path, test_path, val_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_scores[\"val\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Pre-Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = fileNameData(trainPartName)\n",
        "test_path = fileNameData(testPartName)\n",
        "val_path = fileNameData(validatePartName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.7004002287021155\n",
            "Accuracy on validation data = 0.5249695493300852\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_Cleared_scores = eval_DaNetQA(train_path, test_path, val_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_Cleared_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_Cleared_scores[\"val\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 0:\n",
        "    !pip install tensorflow\n",
        "    !pip install pandas\n",
        "    !pip install scipy\n",
        "    !pip install transformers\n",
        "    !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available: True\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import torch\n",
        "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm \n",
        "\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers.optimization import AdamW\n",
        "\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import seed_everything\n",
        "from utils import seed_worker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encode text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectAttentionMask(seq):\n",
        "    return [float(i > 0) for i in seq]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectTokenType(row, sepTokenIdx):\n",
        "    row = np.array(row)\n",
        "    mask = row == sepTokenIdx\n",
        "\n",
        "    whereMask = np.where(mask)[0]\n",
        "    idx = whereMask[0]\n",
        "    idx1 = whereMask[1]\n",
        "\n",
        "    token_type_row = np.zeros(row.shape[0], dtype=np.int32)\n",
        "    token_type_row[idx + 1:idx1 + 1] = 1\n",
        "    return token_type_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_text_pairs(tokenizer, sentences):\n",
        "    ENCODE_BATCH_SIZE = 20000\n",
        "    input_ids, attention_masks, token_type_ids = [], [], []\n",
        "    \n",
        "    clsTokenText = '[CLS]'\n",
        "    sepTokenText = '[SEP]'\n",
        "    sepTokenIdx = tokenizer.convert_tokens_to_ids(sepTokenText)\n",
        "\n",
        "    TEXT1_MAX = int(MAX_LEN*.75) # выделяет 75% размера слов для контекста\n",
        "    TEXT2_MAX = MAX_LEN - TEXT1_MAX # остальные слова это вопрос\n",
        "    for _, i in enumerate(range(0, len(sentences), ENCODE_BATCH_SIZE)):\n",
        "        # обрезаем предложение слов больше чем MAX_LEN\n",
        "        tokenized_texts = []\n",
        "        for sentence_context, sentence_question  in sentences[i:i + ENCODE_BATCH_SIZE]:\n",
        "            p1 = [clsTokenText] + tokenizer.tokenize(sentence_context)\n",
        "            p2 = [sepTokenText] + tokenizer.tokenize(sentence_question) + [sepTokenText]\n",
        "            final_tokens = p1[:TEXT1_MAX] + p2[:TEXT2_MAX]\n",
        "            tokenized_texts.append(final_tokens)\n",
        "\n",
        "        # токенизируем\n",
        "        b_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "        b_input_ids = pad_sequences(\n",
        "            b_input_ids, \n",
        "            maxlen=MAX_LEN, \n",
        "            dtype='long', \n",
        "            truncating='post', \n",
        "            padding='post')\n",
        "        input_ids.append(b_input_ids)\n",
        "\n",
        "        # маска внимания\n",
        "        b_attention_masks = [collectAttentionMask(seq) for seq in b_input_ids]\n",
        "        attention_masks.append(b_attention_masks)\n",
        "\n",
        "        # тип токена\n",
        "        b_token_type_ids = [collectTokenType(row, sepTokenIdx) for row in b_input_ids]\n",
        "        token_type_ids.append(b_token_type_ids)\n",
        "        \n",
        "    return np.vstack(input_ids), np.vstack(attention_masks), np.vstack(token_type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createDataLoader(set_ids, all_ids, input_ids, attention_masks, token_type_ids, all_labels, BATCH_SIZE_LOADER):\n",
        "    mask = np.array([sid in set_ids for sid in all_ids])\n",
        "\n",
        "    inputs = input_ids[mask]\n",
        "    masks = attention_masks[mask]\n",
        "    type_ids_dev = token_type_ids[mask]\n",
        "    labels = all_labels[mask]\n",
        "\n",
        "    t_inputs = torch.tensor(inputs)\n",
        "    t_masks = torch.tensor(masks)\n",
        "    t_type_ids_dev = torch.tensor(type_ids_dev)\n",
        "    t_labels = torch.tensor(labels)\n",
        "\n",
        "    t_dataset = TensorDataset(\n",
        "        t_inputs, \n",
        "        t_masks, \n",
        "        t_type_ids_dev, \n",
        "        t_labels)\n",
        "    t_sampler = SequentialSampler(t_dataset)\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset=t_dataset, \n",
        "        sampler=t_sampler, \n",
        "        batch_size=BATCH_SIZE_LOADER, \n",
        "        worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getYFromDataLoader(dl):\n",
        "    return np.argmax(dl.dataset.__dict__['tensors'][3], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluateModel(model, in_dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    accumulate_loss = 0\n",
        "    accumulate_step = 0\n",
        "    for _, batch in enumerate(in_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids = b_token_type_ids, \n",
        "                attention_mask = b_input_mask, \n",
        "                labels = b_labels)\n",
        "            loss, logits = outputs[:2]\n",
        "            \n",
        "            accumulate_loss += loss.item()\n",
        "            accumulate_step += 1\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            predictions.append(logits)\n",
        "\n",
        "    avg_loss = accumulate_loss / accumulate_step\n",
        "\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    y_true = getYFromDataLoader(in_dataloader)\n",
        "    score = accuracy_score(y_true, predictions)\n",
        "\n",
        "    return score, avg_loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train One Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainModelIteration(model, optimizer, scheduler, in_dataloader, MAX_GRAD_NORM, EPOCH_INDEX):\n",
        "    model.train() \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    predictions = []\n",
        "    accumulate_loss = 0\n",
        "    accumulate_step = 0\n",
        "\n",
        "    nStep = len(in_dataloader)\n",
        "    for step, batch in enumerate(in_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            token_type_ids = b_token_type_ids, \n",
        "            attention_mask = b_input_mask, \n",
        "            labels = b_labels\n",
        "            )\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epochLoss = loss.item()\n",
        "        accumulate_loss += epochLoss\n",
        "        accumulate_step += 1\n",
        "        \n",
        "        print(f\"Epoch {EPOCH_INDEX} Step {step} of {nStep}, loss = {epochLoss}\")\n",
        "\n",
        "    avg_loss = accumulate_loss / accumulate_step\n",
        "\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    y_true = getYFromDataLoader(in_dataloader)\n",
        "    score = accuracy_score(y_true, predictions)\n",
        "\n",
        "    return score, avg_loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1_id, text2_id, label_id, index_id = 'passage', 'question', 'label', 'idx'\n",
        "l2i = {False: 0, True:1}\n",
        "part2indices = {p:set() for p in parts}\n",
        "\n",
        "all_ids, all_sentences, all_labels = [], [], []\n",
        "idxMax = 0\n",
        "for p in parts:\n",
        "    df = pd.read_json(fileNameData(p), lines=True)\n",
        "    ids = idxMax + df[index_id].to_numpy()\n",
        "    all_ids.extend(ids)\n",
        "    idxMax = np.max(all_ids)\n",
        "    \n",
        "    part2indices[p] = ids\n",
        "    all_labels.extend(df[label_id].to_numpy())\n",
        "    all_sentences.extend(\n",
        "        np.array(\n",
        "            np.column_stack([df[text1_id].to_numpy(), \n",
        "            df[text2_id].to_numpy()])\n",
        "        ).tolist())\n",
        "\n",
        "all_ids = np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(total) 2570\n",
            "len(l2i) 2\n"
          ]
        }
      ],
      "source": [
        "print ('len(total)', len(all_sentences))\n",
        "i2l = {l2i[l]:l for l in l2i}\n",
        "print ( 'len(l2i)', len(l2i) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_indices = np.array([l2i[l] for l in all_labels])\n",
        "labels = np.zeros((len(all_ids), len(l2i)))\n",
        "for _, i in enumerate(label_indices):\n",
        "    labels[_, i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test =  label_indices[np.array([sid in part2indices['val_v1'] for sid in all_ids])]\n",
        "y_train =  label_indices[np.array([sid in part2indices['train_v1'] for sid in all_ids])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model RuBert-Cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __POOL_MODEL:\n",
        "    from utils import PoolBertForSequenceClassification as BertModel\n",
        "else:\n",
        "    from transformers import BertForSequenceClassification as BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 128\n",
        "MAX_LEN = 256\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_LOADER = 8\n",
        "EPOCHS_LIMIT = 25\n",
        "LEARNING_RATE = 3e-5\n",
        "MAX_GRAD_NORM = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open 'rubert_cased_L-12_H-768_A-12_pt.tar.gz'\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget \"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
        "!tar -xvzf rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!rm rubert_cased_L-12_H-768_A-12_pt.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\rubert_cased_L-12_H-768_A-12_pt/\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join(base_path, 'rubert_cased_L-12_H-768_A-12_pt/')\n",
        "print(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### One-Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path = os.path.join(base_path, model_path),\n",
        "    do_lower_case=True,\n",
        "    max_length=MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, attention_masks, token_type_ids = encode_text_pairs(tokenizer, all_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepeare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "train_dataloader = createDataLoader(part2indices['train_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "validate_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: torch.Size([1750, 256])\n",
            "Validation set shape: torch.Size([822, 256])\n",
            "Validation set shape: torch.Size([822, 256])\n"
          ]
        }
      ],
      "source": [
        "print (f'Training set shape: {train_dataloader.dataset.__dict__[\"tensors\"][0].shape}')\n",
        "print (f'Validation set shape: {test_dataloader.dataset.__dict__[\"tensors\"][0].shape}')\n",
        "print (f'Validation set shape: {validate_dataloader.dataset.__dict__[\"tensors\"][0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-Trained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'seed_everything' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3588\\3552668604.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mseed_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bert_config.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'seed_everything' is not defined"
          ]
        }
      ],
      "source": [
        "seed_everything(SEED)\n",
        "config_path = os.path.join(base_path, model_path, 'bert_config.json')\n",
        "conf = BertConfig.from_json_file(config_path)\n",
        "conf.num_labels = len(l2i)\n",
        "\n",
        "output_model_file = os.path.join(base_path, model_path, 'pytorch_model.bin')\n",
        "\n",
        "model = BertModel(conf)\n",
        "\n",
        "model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### Limit learning for BERT layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizer & Scheduler\n",
        "Задаем гиперпараметры для цикла обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=LEARNING_RATE, \n",
        "    correct_bias=False)\n",
        "    \n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=LEARNING_RATE, \n",
        "    steps_per_epoch=len(train_dataloader), \n",
        "    epochs=EPOCHS_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_train_predict = []\n",
        "bert_test_predict = []\n",
        "\n",
        "bert_avg_train_loss = []\n",
        "bert_avg_test_loss = []\n",
        "\n",
        "bert_train_acc = []\n",
        "bert_test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Step 0 of 219, loss = 1.1648840170819312\n",
            "Epoch 0 Step 1 of 219, loss = 0.9742748062126338\n",
            "Epoch 0 Step 2 of 219, loss = 1.069280429277569\n",
            "Epoch 0 Step 3 of 219, loss = 1.0660201665014029\n",
            "Epoch 0 Step 4 of 219, loss = 1.0121560343541205\n",
            "Epoch 0 Step 5 of 219, loss = 0.878244386985898\n",
            "Epoch 0 Step 6 of 219, loss = 0.8139510974287987\n",
            "Epoch 0 Step 7 of 219, loss = 0.7129489202052355\n",
            "Epoch 0 Step 8 of 219, loss = 0.7512796418741345\n",
            "Epoch 0 Step 9 of 219, loss = 0.587627271655947\n",
            "Epoch 0 Step 10 of 219, loss = 0.5304310782812536\n",
            "Epoch 0 Step 11 of 219, loss = 0.4581133765168488\n",
            "Epoch 0 Step 12 of 219, loss = 0.48028083937242627\n",
            "Epoch 0 Step 13 of 219, loss = 0.9146116706542671\n",
            "Epoch 0 Step 14 of 219, loss = 0.4167641280218959\n",
            "Epoch 0 Step 15 of 219, loss = 0.4478187318891287\n",
            "Epoch 0 Step 16 of 219, loss = 0.4130536369048059\n",
            "Epoch 0 Step 17 of 219, loss = 1.9040669035166502\n",
            "Epoch 0 Step 18 of 219, loss = 0.8152113617397845\n",
            "Epoch 0 Step 19 of 219, loss = 1.2541905138641596\n",
            "Epoch 0 Step 20 of 219, loss = 1.3174855469260365\n",
            "Epoch 0 Step 21 of 219, loss = 1.0602633678354323\n",
            "Epoch 0 Step 22 of 219, loss = 1.3045746069401503\n",
            "Epoch 0 Step 23 of 219, loss = 1.710839090636\n",
            "Epoch 0 Step 24 of 219, loss = 1.4279132755473256\n",
            "Epoch 0 Step 25 of 219, loss = 0.616090040653944\n",
            "Epoch 0 Step 26 of 219, loss = 0.4623240097425878\n",
            "Epoch 0 Step 27 of 219, loss = 1.1460612451191992\n",
            "Epoch 0 Step 28 of 219, loss = 0.232850308297202\n",
            "Epoch 0 Step 29 of 219, loss = 0.7980932465288788\n",
            "Epoch 0 Step 30 of 219, loss = 0.6707243067212403\n",
            "Epoch 0 Step 31 of 219, loss = 1.0265836962498724\n",
            "Epoch 0 Step 32 of 219, loss = 0.44021719112060964\n",
            "Epoch 0 Step 33 of 219, loss = 1.233529387973249\n",
            "Epoch 0 Step 34 of 219, loss = 0.8962225685827434\n",
            "Epoch 0 Step 35 of 219, loss = 0.3089415952563286\n",
            "Epoch 0 Step 36 of 219, loss = 0.5011547510512173\n",
            "Epoch 0 Step 37 of 219, loss = 0.49510961025953293\n",
            "Epoch 0 Step 38 of 219, loss = 0.30979409581050277\n",
            "Epoch 0 Step 39 of 219, loss = 0.5464512859471142\n",
            "Epoch 0 Step 40 of 219, loss = 0.3033157419413328\n",
            "Epoch 0 Step 41 of 219, loss = 0.38681908044964075\n",
            "Epoch 0 Step 42 of 219, loss = 0.3905875198543072\n",
            "Epoch 0 Step 43 of 219, loss = 0.2807204518467188\n",
            "Epoch 0 Step 44 of 219, loss = 0.24628375377506018\n",
            "Epoch 0 Step 45 of 219, loss = 0.6015431880950928\n",
            "Epoch 0 Step 46 of 219, loss = 0.44964367523789406\n",
            "Epoch 0 Step 47 of 219, loss = 1.5279605048708618\n",
            "Epoch 0 Step 48 of 219, loss = 0.21721873572096229\n",
            "Epoch 0 Step 49 of 219, loss = 1.169415406882763\n",
            "Epoch 0 Step 50 of 219, loss = 0.21371772233396769\n",
            "Epoch 0 Step 51 of 219, loss = 0.21727060712873936\n",
            "Epoch 0 Step 52 of 219, loss = 0.2145911632105708\n",
            "Epoch 0 Step 53 of 219, loss = 0.4149413453415036\n",
            "Epoch 0 Step 54 of 219, loss = 0.5723543893545866\n",
            "Epoch 0 Step 55 of 219, loss = 0.1819418971426785\n",
            "Epoch 0 Step 56 of 219, loss = 0.39886949164792895\n",
            "Epoch 0 Step 57 of 219, loss = 0.15274985786527395\n",
            "Epoch 0 Step 58 of 219, loss = 0.4053311664611101\n",
            "Epoch 0 Step 59 of 219, loss = 0.3941977717913687\n",
            "Epoch 0 Step 60 of 219, loss = 0.6414227921050042\n",
            "Epoch 0 Step 61 of 219, loss = 0.6210409197956324\n",
            "Epoch 0 Step 62 of 219, loss = 0.6438004958909005\n",
            "Epoch 0 Step 63 of 219, loss = 0.374454066157341\n",
            "Epoch 0 Step 64 of 219, loss = 0.1259767736773938\n",
            "Epoch 0 Step 65 of 219, loss = 0.10811115754768252\n",
            "Epoch 0 Step 66 of 219, loss = 0.37796060205437243\n",
            "Epoch 0 Step 67 of 219, loss = 1.2090520698111504\n",
            "Epoch 0 Step 68 of 219, loss = 1.4857552126049995\n",
            "Epoch 0 Step 69 of 219, loss = 0.37868332397192717\n",
            "Epoch 0 Step 70 of 219, loss = 0.3181506316177547\n",
            "Epoch 0 Step 71 of 219, loss = 1.363944360986352\n",
            "Epoch 0 Step 72 of 219, loss = 0.38272574031725526\n",
            "Epoch 0 Step 73 of 219, loss = 0.44263512711040676\n",
            "Epoch 0 Step 74 of 219, loss = 0.39850597083568573\n",
            "Epoch 0 Step 75 of 219, loss = 0.4179120445623994\n",
            "Epoch 0 Step 76 of 219, loss = 0.7129148996900767\n",
            "Epoch 0 Step 77 of 219, loss = 0.5918371716979891\n",
            "Epoch 0 Step 78 of 219, loss = 0.388491838471964\n",
            "Epoch 0 Step 79 of 219, loss = 0.3741878350265324\n",
            "Epoch 0 Step 80 of 219, loss = 0.6708754594437778\n",
            "Epoch 0 Step 81 of 219, loss = 0.11785131529904902\n",
            "Epoch 0 Step 82 of 219, loss = 0.4082228378392756\n",
            "Epoch 0 Step 83 of 219, loss = 0.9673036364838481\n",
            "Epoch 0 Step 84 of 219, loss = 0.13809486478567123\n",
            "Epoch 0 Step 85 of 219, loss = 0.6661731414496899\n",
            "Epoch 0 Step 86 of 219, loss = 0.3998865238390863\n",
            "Epoch 0 Step 87 of 219, loss = 1.4245677697472274\n",
            "Epoch 0 Step 88 of 219, loss = 0.5698796976357698\n",
            "Epoch 0 Step 89 of 219, loss = 1.4599499758332968\n",
            "Epoch 0 Step 90 of 219, loss = 0.37504811584949493\n",
            "Epoch 0 Step 91 of 219, loss = 0.9606719622388482\n",
            "Epoch 0 Step 92 of 219, loss = 0.579610978718847\n",
            "Epoch 0 Step 93 of 219, loss = 0.6386252976953983\n",
            "Epoch 0 Step 94 of 219, loss = 0.6095429649576545\n",
            "Epoch 0 Step 95 of 219, loss = 0.2263087360188365\n",
            "Epoch 0 Step 96 of 219, loss = 0.6880205455236137\n",
            "Epoch 0 Step 97 of 219, loss = 0.2539063300937414\n",
            "Epoch 0 Step 98 of 219, loss = 0.9208997911773622\n",
            "Epoch 0 Step 99 of 219, loss = 1.1066925376653671\n",
            "Epoch 0 Step 100 of 219, loss = 0.744878304656595\n",
            "Epoch 0 Step 101 of 219, loss = 0.8798200748860836\n",
            "Epoch 0 Step 102 of 219, loss = 0.7328163180500269\n",
            "Epoch 0 Step 103 of 219, loss = 0.35878303553909063\n",
            "Epoch 0 Step 104 of 219, loss = 0.5802456773817539\n",
            "Epoch 0 Step 105 of 219, loss = 0.3019194584339857\n",
            "Epoch 0 Step 106 of 219, loss = 0.5560205010697246\n",
            "Epoch 0 Step 107 of 219, loss = 0.5813484638929367\n",
            "Epoch 0 Step 108 of 219, loss = 0.4596977895125747\n",
            "Epoch 0 Step 109 of 219, loss = 0.7256092219613492\n",
            "Epoch 0 Step 110 of 219, loss = 0.780916964635253\n",
            "Epoch 0 Step 111 of 219, loss = 0.596060374751687\n",
            "Epoch 0 Step 112 of 219, loss = 0.6067753918468952\n",
            "Epoch 0 Step 113 of 219, loss = 0.6301807248964906\n",
            "Epoch 0 Step 114 of 219, loss = 0.8114125588908792\n",
            "Epoch 0 Step 115 of 219, loss = 0.635965570807457\n",
            "Epoch 0 Step 116 of 219, loss = 0.7882699687033892\n",
            "Epoch 0 Step 117 of 219, loss = 0.4680651193484664\n",
            "Epoch 0 Step 118 of 219, loss = 0.4198287259787321\n",
            "Epoch 0 Step 119 of 219, loss = 0.5323450285941362\n",
            "Epoch 0 Step 120 of 219, loss = 0.457520485855639\n",
            "Epoch 0 Step 121 of 219, loss = 0.5649294760078192\n",
            "Epoch 0 Step 122 of 219, loss = 0.6182337024947628\n",
            "Epoch 0 Step 123 of 219, loss = 0.42047468468081206\n",
            "Epoch 0 Step 124 of 219, loss = 0.6463410975411534\n",
            "Epoch 0 Step 125 of 219, loss = 0.4085392253473401\n",
            "Epoch 0 Step 126 of 219, loss = 0.6147929737344384\n",
            "Epoch 0 Step 127 of 219, loss = 0.38278527557849884\n",
            "Epoch 0 Step 128 of 219, loss = 0.46404152270406485\n",
            "Epoch 0 Step 129 of 219, loss = 0.5549841830506921\n",
            "Epoch 0 Step 130 of 219, loss = 0.5730092218145728\n",
            "Epoch 0 Step 131 of 219, loss = 0.3111046124249697\n",
            "Epoch 0 Step 132 of 219, loss = 0.5247400226071477\n",
            "Epoch 0 Step 133 of 219, loss = 0.8910553567111492\n",
            "Epoch 0 Step 134 of 219, loss = 0.8940339805558324\n",
            "Epoch 0 Step 135 of 219, loss = 0.8269763384014368\n",
            "Epoch 0 Step 136 of 219, loss = 0.673526069149375\n",
            "Epoch 0 Step 137 of 219, loss = 1.0034045837819576\n",
            "Epoch 0 Step 138 of 219, loss = 1.1262880889698863\n",
            "Epoch 0 Step 139 of 219, loss = 0.5475693885236979\n",
            "Epoch 0 Step 140 of 219, loss = 1.1836111713200808\n",
            "Epoch 0 Step 141 of 219, loss = 0.7710988093167543\n",
            "Epoch 0 Step 142 of 219, loss = 0.7654634956270456\n",
            "Epoch 0 Step 143 of 219, loss = 0.8888285709545016\n",
            "Epoch 0 Step 144 of 219, loss = 1.1557405786588788\n",
            "Epoch 0 Step 145 of 219, loss = 0.9055011766031384\n",
            "Epoch 0 Step 146 of 219, loss = 0.704608503729105\n",
            "Epoch 0 Step 147 of 219, loss = 0.5723515171557665\n",
            "Epoch 0 Step 148 of 219, loss = 0.6456505442038178\n",
            "Epoch 0 Step 149 of 219, loss = 0.6488569509238005\n",
            "Epoch 0 Step 150 of 219, loss = 1.0901491241529584\n",
            "Epoch 0 Step 151 of 219, loss = 0.8122062282636762\n",
            "Epoch 0 Step 152 of 219, loss = 0.9356964994221926\n",
            "Epoch 0 Step 153 of 219, loss = 0.8649051692336798\n",
            "Epoch 0 Step 154 of 219, loss = 0.7668278822675347\n",
            "Epoch 0 Step 155 of 219, loss = 0.825905479490757\n",
            "Epoch 0 Step 156 of 219, loss = 0.7312619825825095\n",
            "Epoch 0 Step 157 of 219, loss = 0.8368881929200143\n",
            "Epoch 0 Step 158 of 219, loss = 0.710294050630182\n",
            "Epoch 0 Step 159 of 219, loss = 0.6920264576328918\n",
            "Epoch 0 Step 160 of 219, loss = 0.6906147939153016\n",
            "Epoch 0 Step 161 of 219, loss = 0.6957232366548851\n",
            "Epoch 0 Step 162 of 219, loss = 0.6532820373540744\n",
            "Epoch 0 Step 163 of 219, loss = 0.6199306123889983\n",
            "Epoch 0 Step 164 of 219, loss = 0.6620608901139349\n",
            "Epoch 0 Step 165 of 219, loss = 0.7400710918009281\n",
            "Epoch 0 Step 166 of 219, loss = 0.5737751359120011\n",
            "Epoch 0 Step 167 of 219, loss = 0.9228170500136912\n",
            "Epoch 0 Step 168 of 219, loss = 0.6202831696718931\n",
            "Epoch 0 Step 169 of 219, loss = 0.5042982026934624\n",
            "Epoch 0 Step 170 of 219, loss = 0.6602884382009506\n",
            "Epoch 0 Step 171 of 219, loss = 0.6603249572217464\n",
            "Epoch 0 Step 172 of 219, loss = 0.6738175647333264\n",
            "Epoch 0 Step 173 of 219, loss = 0.558479318395257\n",
            "Epoch 0 Step 174 of 219, loss = 0.8093883134424686\n",
            "Epoch 0 Step 175 of 219, loss = 0.9124294817447662\n",
            "Epoch 0 Step 176 of 219, loss = 0.544518162496388\n",
            "Epoch 0 Step 177 of 219, loss = 0.8519793339073658\n",
            "Epoch 0 Step 178 of 219, loss = 0.42939899303019047\n",
            "Epoch 0 Step 179 of 219, loss = 0.6629182510077953\n",
            "Epoch 0 Step 180 of 219, loss = 0.5795702710747719\n",
            "Epoch 0 Step 181 of 219, loss = 0.525753858499229\n",
            "Epoch 0 Step 182 of 219, loss = 0.43021699879318476\n",
            "Epoch 0 Step 183 of 219, loss = 0.4165778299793601\n",
            "Epoch 0 Step 184 of 219, loss = 0.6575713464990258\n",
            "Epoch 0 Step 185 of 219, loss = 0.5722079891711473\n",
            "Epoch 0 Step 186 of 219, loss = 0.5579455411061645\n",
            "Epoch 0 Step 187 of 219, loss = 0.5435587121173739\n",
            "Epoch 0 Step 188 of 219, loss = 0.835590043105185\n",
            "Epoch 0 Step 189 of 219, loss = 0.5583703564479947\n",
            "Epoch 0 Step 190 of 219, loss = 0.7867253208532929\n",
            "Epoch 0 Step 191 of 219, loss = 0.8838944416493177\n",
            "Epoch 0 Step 192 of 219, loss = 0.7148726768791676\n",
            "Epoch 0 Step 193 of 219, loss = 0.4148664576932788\n",
            "Epoch 0 Step 194 of 219, loss = 0.42626409977674484\n",
            "Epoch 0 Step 195 of 219, loss = 0.7143855029717088\n",
            "Epoch 0 Step 196 of 219, loss = 0.75331019051373\n",
            "Epoch 0 Step 197 of 219, loss = 0.6965746311470866\n",
            "Epoch 0 Step 198 of 219, loss = 0.6552295172587037\n",
            "Epoch 0 Step 199 of 219, loss = 0.8520466946065426\n",
            "Epoch 0 Step 200 of 219, loss = 0.9329894073307514\n",
            "Epoch 0 Step 201 of 219, loss = 0.923250169493258\n",
            "Epoch 0 Step 202 of 219, loss = 0.5526877092197537\n",
            "Epoch 0 Step 203 of 219, loss = 0.7392201088368893\n",
            "Epoch 0 Step 204 of 219, loss = 0.7207266725599766\n",
            "Epoch 0 Step 205 of 219, loss = 0.36363465525209904\n",
            "Epoch 0 Step 206 of 219, loss = 0.7611583974212408\n",
            "Epoch 0 Step 207 of 219, loss = 0.5077242404222488\n",
            "Epoch 0 Step 208 of 219, loss = 0.5230867378413677\n",
            "Epoch 0 Step 209 of 219, loss = 0.5501604210585356\n",
            "Epoch 0 Step 210 of 219, loss = 0.7688454333692789\n",
            "Epoch 0 Step 211 of 219, loss = 0.7959909997880459\n",
            "Epoch 0 Step 212 of 219, loss = 0.842641169205308\n",
            "Epoch 0 Step 213 of 219, loss = 0.6953166723251343\n",
            "Epoch 0 Step 214 of 219, loss = 0.6012296080589294\n",
            "Epoch 0 Step 215 of 219, loss = 0.7011096430942416\n",
            "Epoch 0 Step 216 of 219, loss = 0.7758558290079236\n",
            "Epoch 0 Step 217 of 219, loss = 0.6161257633939385\n",
            "Epoch 0 Step 218 of 219, loss = 0.9371245006720225\n",
            "Epoch 0 average train_loss: 0.671158 test_loss: 0.746779 test_score 0.49%\n",
            "Epoch 1 Step 0 of 219, loss = 0.8235202879877761\n",
            "Epoch 1 Step 1 of 219, loss = 0.7077656839974225\n",
            "Epoch 1 Step 2 of 219, loss = 0.756251655286178\n",
            "Epoch 1 Step 3 of 219, loss = 0.782913074363023\n",
            "Epoch 1 Step 4 of 219, loss = 0.7218147857347503\n",
            "Epoch 1 Step 5 of 219, loss = 0.7179426202201284\n",
            "Epoch 1 Step 6 of 219, loss = 0.7244575971271843\n",
            "Epoch 1 Step 7 of 219, loss = 0.643362426199019\n",
            "Epoch 1 Step 8 of 219, loss = 0.5995320520596579\n",
            "Epoch 1 Step 9 of 219, loss = 0.6039107535034418\n",
            "Epoch 1 Step 10 of 219, loss = 0.5997235774993896\n",
            "Epoch 1 Step 11 of 219, loss = 0.53024329431355\n",
            "Epoch 1 Step 12 of 219, loss = 0.5553985936567187\n",
            "Epoch 1 Step 13 of 219, loss = 0.691975424066186\n",
            "Epoch 1 Step 14 of 219, loss = 0.5042622033506632\n",
            "Epoch 1 Step 15 of 219, loss = 0.4422756042331457\n",
            "Epoch 1 Step 16 of 219, loss = 0.4901692708954215\n",
            "Epoch 1 Step 17 of 219, loss = 1.193291706033051\n",
            "Epoch 1 Step 18 of 219, loss = 0.6510831424966455\n",
            "Epoch 1 Step 19 of 219, loss = 0.8799721030518413\n",
            "Epoch 1 Step 20 of 219, loss = 0.970353914424777\n",
            "Epoch 1 Step 21 of 219, loss = 0.7838249057531357\n",
            "Epoch 1 Step 22 of 219, loss = 0.9260568087920547\n",
            "Epoch 1 Step 23 of 219, loss = 1.233781337738037\n",
            "Epoch 1 Step 24 of 219, loss = 1.1842551911249757\n",
            "Epoch 1 Step 25 of 219, loss = 0.5557417599484324\n",
            "Epoch 1 Step 26 of 219, loss = 0.39199738297611475\n",
            "Epoch 1 Step 27 of 219, loss = 0.9284429047256708\n",
            "Epoch 1 Step 28 of 219, loss = 0.28115665074437857\n",
            "Epoch 1 Step 29 of 219, loss = 0.6663553873077035\n",
            "Epoch 1 Step 30 of 219, loss = 0.530182234942913\n",
            "Epoch 1 Step 31 of 219, loss = 0.876303369179368\n",
            "Epoch 1 Step 32 of 219, loss = 0.4133413191884756\n",
            "Epoch 1 Step 33 of 219, loss = 1.0744307450950146\n",
            "Epoch 1 Step 34 of 219, loss = 0.8617949513718486\n",
            "Epoch 1 Step 35 of 219, loss = 0.2934894757345319\n",
            "Epoch 1 Step 36 of 219, loss = 0.44790387991815805\n",
            "Epoch 1 Step 37 of 219, loss = 0.41274443082511425\n",
            "Epoch 1 Step 38 of 219, loss = 0.2842348534613848\n",
            "Epoch 1 Step 39 of 219, loss = 0.5827860832214355\n",
            "Epoch 1 Step 40 of 219, loss = 0.3280995013192296\n",
            "Epoch 1 Step 41 of 219, loss = 0.3576459242030978\n",
            "Epoch 1 Step 42 of 219, loss = 0.41145636793226004\n",
            "Epoch 1 Step 43 of 219, loss = 0.2589221512898803\n",
            "Epoch 1 Step 44 of 219, loss = 0.25857340078800917\n",
            "Epoch 1 Step 45 of 219, loss = 0.5578346075490117\n",
            "Epoch 1 Step 46 of 219, loss = 0.38757780846208334\n",
            "Epoch 1 Step 47 of 219, loss = 1.4171650186181068\n",
            "Epoch 1 Step 48 of 219, loss = 0.22748062666505575\n",
            "Epoch 1 Step 49 of 219, loss = 1.146974136121571\n",
            "Epoch 1 Step 50 of 219, loss = 0.20601370930671692\n",
            "Epoch 1 Step 51 of 219, loss = 0.18765494227409363\n",
            "Epoch 1 Step 52 of 219, loss = 0.18924076203256845\n",
            "Epoch 1 Step 53 of 219, loss = 0.4065220830962062\n",
            "Epoch 1 Step 54 of 219, loss = 0.587086406070739\n",
            "Epoch 1 Step 55 of 219, loss = 0.15283992420881987\n",
            "Epoch 1 Step 56 of 219, loss = 0.3520113346166909\n",
            "Epoch 1 Step 57 of 219, loss = 0.13646302092820406\n",
            "Epoch 1 Step 58 of 219, loss = 0.3929552691988647\n",
            "Epoch 1 Step 59 of 219, loss = 0.3937700195237994\n",
            "Epoch 1 Step 60 of 219, loss = 0.66496038204059\n",
            "Epoch 1 Step 61 of 219, loss = 0.6943851623218507\n",
            "Epoch 1 Step 62 of 219, loss = 0.6710938813630491\n",
            "Epoch 1 Step 63 of 219, loss = 0.4131970969028771\n",
            "Epoch 1 Step 64 of 219, loss = 0.10322357388213277\n",
            "Epoch 1 Step 65 of 219, loss = 0.10568773746490479\n",
            "Epoch 1 Step 66 of 219, loss = 0.4093132233247161\n",
            "Epoch 1 Step 67 of 219, loss = 1.2531018974259496\n",
            "Epoch 1 Step 68 of 219, loss = 1.5446941533591598\n",
            "Epoch 1 Step 69 of 219, loss = 0.33831312227994204\n",
            "Epoch 1 Step 70 of 219, loss = 0.3324098358862102\n",
            "Epoch 1 Step 71 of 219, loss = 1.2616776218637824\n",
            "Epoch 1 Step 72 of 219, loss = 0.3806322368327528\n",
            "Epoch 1 Step 73 of 219, loss = 0.38243231549859047\n",
            "Epoch 1 Step 74 of 219, loss = 0.3925658627413213\n",
            "Epoch 1 Step 75 of 219, loss = 0.3826142540201545\n",
            "Epoch 1 Step 76 of 219, loss = 0.6218250887468457\n",
            "Epoch 1 Step 77 of 219, loss = 0.6137435915879905\n",
            "Epoch 1 Step 78 of 219, loss = 0.4194131544791162\n",
            "Epoch 1 Step 79 of 219, loss = 0.3600492374971509\n",
            "Epoch 1 Step 80 of 219, loss = 0.5486647505313158\n",
            "Epoch 1 Step 81 of 219, loss = 0.13831105828285217\n",
            "Epoch 1 Step 82 of 219, loss = 0.3490500617772341\n",
            "Epoch 1 Step 83 of 219, loss = 0.8406381239183247\n",
            "Epoch 1 Step 84 of 219, loss = 0.13527214154601097\n",
            "Epoch 1 Step 85 of 219, loss = 0.6201779050752521\n",
            "Epoch 1 Step 86 of 219, loss = 0.3933422230184078\n",
            "Epoch 1 Step 87 of 219, loss = 1.2860223902389407\n",
            "Epoch 1 Step 88 of 219, loss = 0.5929544530808926\n",
            "Epoch 1 Step 89 of 219, loss = 1.3823329624719918\n",
            "Epoch 1 Step 90 of 219, loss = 0.316559337079525\n",
            "Epoch 1 Step 91 of 219, loss = 0.882852298207581\n",
            "Epoch 1 Step 92 of 219, loss = 0.589998772367835\n",
            "Epoch 1 Step 93 of 219, loss = 0.6186956455931067\n",
            "Epoch 1 Step 94 of 219, loss = 0.5684332167729735\n",
            "Epoch 1 Step 95 of 219, loss = 0.263631178997457\n",
            "Epoch 1 Step 96 of 219, loss = 0.6690646884962916\n",
            "Epoch 1 Step 97 of 219, loss = 0.2807012898847461\n",
            "Epoch 1 Step 98 of 219, loss = 0.8506238330155611\n",
            "Epoch 1 Step 99 of 219, loss = 0.9251546813175082\n",
            "Epoch 1 Step 100 of 219, loss = 0.698599829338491\n",
            "Epoch 1 Step 101 of 219, loss = 0.8244215724989772\n",
            "Epoch 1 Step 102 of 219, loss = 0.7265148498117924\n",
            "Epoch 1 Step 103 of 219, loss = 0.3933266419917345\n",
            "Epoch 1 Step 104 of 219, loss = 0.4950344115495682\n",
            "Epoch 1 Step 105 of 219, loss = 0.3604992162436247\n",
            "Epoch 1 Step 106 of 219, loss = 0.5910251177847385\n",
            "Epoch 1 Step 107 of 219, loss = 0.5457321330904961\n",
            "Epoch 1 Step 108 of 219, loss = 0.4720435310155153\n",
            "Epoch 1 Step 109 of 219, loss = 0.6780460691079497\n",
            "Epoch 1 Step 110 of 219, loss = 0.7552686519920826\n",
            "Epoch 1 Step 111 of 219, loss = 0.5599592123180628\n",
            "Epoch 1 Step 112 of 219, loss = 0.5752705316990614\n",
            "Epoch 1 Step 113 of 219, loss = 0.6157898865640163\n",
            "Epoch 1 Step 114 of 219, loss = 0.7145722829736769\n",
            "Epoch 1 Step 115 of 219, loss = 0.7215933483093977\n",
            "Epoch 1 Step 116 of 219, loss = 0.7226801998913288\n",
            "Epoch 1 Step 117 of 219, loss = 0.5424268692731857\n",
            "Epoch 1 Step 118 of 219, loss = 0.4422547686845064\n",
            "Epoch 1 Step 119 of 219, loss = 0.5069032367318869\n",
            "Epoch 1 Step 120 of 219, loss = 0.5153232589364052\n",
            "Epoch 1 Step 121 of 219, loss = 0.615284388884902\n",
            "Epoch 1 Step 122 of 219, loss = 0.5814443146809936\n",
            "Epoch 1 Step 123 of 219, loss = 0.3765275124460459\n",
            "Epoch 1 Step 124 of 219, loss = 0.685759755782783\n",
            "Epoch 1 Step 125 of 219, loss = 0.3783658631145954\n",
            "Epoch 1 Step 126 of 219, loss = 0.5742676546797156\n",
            "Epoch 1 Step 127 of 219, loss = 0.3582698265090585\n",
            "Epoch 1 Step 128 of 219, loss = 0.48239752650260925\n",
            "Epoch 1 Step 129 of 219, loss = 0.5403087250888348\n",
            "Epoch 1 Step 130 of 219, loss = 0.5362440189346671\n",
            "Epoch 1 Step 131 of 219, loss = 0.31645928230136633\n",
            "Epoch 1 Step 132 of 219, loss = 0.5012513510882854\n",
            "Epoch 1 Step 133 of 219, loss = 0.824284078553319\n",
            "Epoch 1 Step 134 of 219, loss = 0.883777717128396\n",
            "Epoch 1 Step 135 of 219, loss = 0.8981894683092833\n",
            "Epoch 1 Step 136 of 219, loss = 0.7269928259775043\n",
            "Epoch 1 Step 137 of 219, loss = 1.0559477172791958\n",
            "Epoch 1 Step 138 of 219, loss = 1.1012376034632325\n",
            "Epoch 1 Step 139 of 219, loss = 0.5167436879128218\n",
            "Epoch 1 Step 140 of 219, loss = 1.1444241264835\n",
            "Epoch 1 Step 141 of 219, loss = 0.6717815203592181\n",
            "Epoch 1 Step 142 of 219, loss = 0.7122314171865582\n",
            "Epoch 1 Step 143 of 219, loss = 0.7740189991891384\n",
            "Epoch 1 Step 144 of 219, loss = 1.109902928583324\n",
            "Epoch 1 Step 145 of 219, loss = 0.8574983524158597\n",
            "Epoch 1 Step 146 of 219, loss = 0.6559460181742907\n",
            "Epoch 1 Step 147 of 219, loss = 0.5991228669881821\n",
            "Epoch 1 Step 148 of 219, loss = 0.6767181940376759\n",
            "Epoch 1 Step 149 of 219, loss = 0.6463054697960615\n",
            "Epoch 1 Step 150 of 219, loss = 1.0081976875662804\n",
            "Epoch 1 Step 151 of 219, loss = 0.8408259451389313\n",
            "Epoch 1 Step 152 of 219, loss = 0.8565638726577163\n",
            "Epoch 1 Step 153 of 219, loss = 0.7527880854904652\n",
            "Epoch 1 Step 154 of 219, loss = 0.7586001153104007\n",
            "Epoch 1 Step 155 of 219, loss = 0.704214524012059\n",
            "Epoch 1 Step 156 of 219, loss = 0.6947949421592057\n",
            "Epoch 1 Step 157 of 219, loss = 0.6918143360817339\n",
            "Epoch 1 Step 158 of 219, loss = 0.7001493913121521\n",
            "Epoch 1 Step 159 of 219, loss = 0.678514534025453\n",
            "Epoch 1 Step 160 of 219, loss = 0.697873123921454\n",
            "Epoch 1 Step 161 of 219, loss = 0.5734271500259638\n",
            "Epoch 1 Step 162 of 219, loss = 0.49475577659904957\n",
            "Epoch 1 Step 163 of 219, loss = 0.6116778310388327\n",
            "Epoch 1 Step 164 of 219, loss = 0.6114027556031942\n",
            "Epoch 1 Step 165 of 219, loss = 0.7710530208423734\n",
            "Epoch 1 Step 166 of 219, loss = 0.45157585944980383\n",
            "Epoch 1 Step 167 of 219, loss = 1.0678807254880667\n",
            "Epoch 1 Step 168 of 219, loss = 0.5641058422625065\n",
            "Epoch 1 Step 169 of 219, loss = 0.4045121199451387\n",
            "Epoch 1 Step 170 of 219, loss = 0.7324302438646555\n",
            "Epoch 1 Step 171 of 219, loss = 0.6584632731974125\n",
            "Epoch 1 Step 172 of 219, loss = 0.7457485590130091\n",
            "Epoch 1 Step 173 of 219, loss = 0.5629643611609936\n",
            "Epoch 1 Step 174 of 219, loss = 0.8927549291402102\n",
            "Epoch 1 Step 175 of 219, loss = 1.0828505214303732\n",
            "Epoch 1 Step 176 of 219, loss = 0.5941885695792735\n",
            "Epoch 1 Step 177 of 219, loss = 1.0061566550284624\n",
            "Epoch 1 Step 178 of 219, loss = 0.4341168310493231\n",
            "Epoch 1 Step 179 of 219, loss = 0.6981373797170818\n",
            "Epoch 1 Step 180 of 219, loss = 0.4927981197834015\n",
            "Epoch 1 Step 181 of 219, loss = 0.5565472710877657\n",
            "Epoch 1 Step 182 of 219, loss = 0.40822244714945555\n",
            "Epoch 1 Step 183 of 219, loss = 0.39967027213424444\n",
            "Epoch 1 Step 184 of 219, loss = 0.6804966703057289\n",
            "Epoch 1 Step 185 of 219, loss = 0.5886389529332519\n",
            "Epoch 1 Step 186 of 219, loss = 0.6052457736805081\n",
            "Epoch 1 Step 187 of 219, loss = 0.5422571720555425\n",
            "Epoch 1 Step 188 of 219, loss = 0.8015878209844232\n",
            "Epoch 1 Step 189 of 219, loss = 0.5757147232070565\n",
            "Epoch 1 Step 190 of 219, loss = 0.8200593646615744\n",
            "Epoch 1 Step 191 of 219, loss = 0.7644076738506556\n",
            "Epoch 1 Step 192 of 219, loss = 0.6437497744336724\n",
            "Epoch 1 Step 193 of 219, loss = 0.4388571474701166\n",
            "Epoch 1 Step 194 of 219, loss = 0.42097875662148\n",
            "Epoch 1 Step 195 of 219, loss = 0.7041413709521294\n",
            "Epoch 1 Step 196 of 219, loss = 0.6626191530376673\n",
            "Epoch 1 Step 197 of 219, loss = 0.6462489059194922\n",
            "Epoch 1 Step 198 of 219, loss = 0.6370965815149248\n",
            "Epoch 1 Step 199 of 219, loss = 0.796099878847599\n",
            "Epoch 1 Step 200 of 219, loss = 0.8493201239034534\n",
            "Epoch 1 Step 201 of 219, loss = 0.8695907276123762\n",
            "Epoch 1 Step 202 of 219, loss = 0.5841173678636551\n",
            "Epoch 1 Step 203 of 219, loss = 0.7129054702818394\n",
            "Epoch 1 Step 204 of 219, loss = 0.6692400481551886\n",
            "Epoch 1 Step 205 of 219, loss = 0.4835039060562849\n",
            "Epoch 1 Step 206 of 219, loss = 0.7208334719762206\n",
            "Epoch 1 Step 207 of 219, loss = 0.5492726657539606\n",
            "Epoch 1 Step 208 of 219, loss = 0.5888764150440693\n",
            "Epoch 1 Step 209 of 219, loss = 0.5389323203125969\n",
            "Epoch 1 Step 210 of 219, loss = 0.722390818875283\n",
            "Epoch 1 Step 211 of 219, loss = 0.7220670226961374\n",
            "Epoch 1 Step 212 of 219, loss = 0.750635719159618\n",
            "Epoch 1 Step 213 of 219, loss = 0.717376877553761\n",
            "Epoch 1 Step 214 of 219, loss = 0.6331330584362149\n",
            "Epoch 1 Step 215 of 219, loss = 0.6812059570802376\n",
            "Epoch 1 Step 216 of 219, loss = 0.7828805558071963\n",
            "Epoch 1 Step 217 of 219, loss = 0.5859841752098873\n",
            "Epoch 1 Step 218 of 219, loss = 0.7961767244463165\n",
            "Epoch 1 average train_loss: 0.630222 test_loss: 0.731343 test_score 0.55%\n",
            "Epoch 2 Step 0 of 219, loss = 0.8343227724544704\n",
            "Epoch 2 Step 1 of 219, loss = 0.6713820129880332\n",
            "Epoch 2 Step 2 of 219, loss = 0.743791650980711\n",
            "Epoch 2 Step 3 of 219, loss = 0.7318117897957563\n",
            "Epoch 2 Step 4 of 219, loss = 0.6675460202968679\n",
            "Epoch 2 Step 5 of 219, loss = 0.6441959150834009\n",
            "Epoch 2 Step 6 of 219, loss = 0.628343627351569\n",
            "Epoch 2 Step 7 of 219, loss = 0.5917565105482936\n",
            "Epoch 2 Step 8 of 219, loss = 0.5061390800401568\n",
            "Epoch 2 Step 9 of 219, loss = 0.5328755269292742\n",
            "Epoch 2 Step 10 of 219, loss = 0.526843142695725\n",
            "Epoch 2 Step 11 of 219, loss = 0.42627306189388037\n",
            "Epoch 2 Step 12 of 219, loss = 0.46538504119962454\n",
            "Epoch 2 Step 13 of 219, loss = 0.7419899553060532\n",
            "Epoch 2 Step 14 of 219, loss = 0.4701302144676447\n",
            "Epoch 2 Step 15 of 219, loss = 0.40426025725901127\n",
            "Epoch 2 Step 16 of 219, loss = 0.4715640884824097\n",
            "Epoch 2 Step 17 of 219, loss = 1.3942643031477928\n",
            "Epoch 2 Step 18 of 219, loss = 0.6372567787766457\n",
            "Epoch 2 Step 19 of 219, loss = 1.012060683220625\n",
            "Epoch 2 Step 20 of 219, loss = 0.96600461890921\n",
            "Epoch 2 Step 21 of 219, loss = 0.9135403241962194\n",
            "Epoch 2 Step 22 of 219, loss = 1.0224152943119407\n",
            "Epoch 2 Step 23 of 219, loss = 1.3264744626358151\n",
            "Epoch 2 Step 24 of 219, loss = 1.1379557866603136\n",
            "Epoch 2 Step 25 of 219, loss = 0.6083109267055988\n",
            "Epoch 2 Step 26 of 219, loss = 0.33450991241261363\n",
            "Epoch 2 Step 27 of 219, loss = 1.038188498467207\n",
            "Epoch 2 Step 28 of 219, loss = 0.22564147226512432\n",
            "Epoch 2 Step 29 of 219, loss = 0.713107786141336\n",
            "Epoch 2 Step 30 of 219, loss = 0.5408683689311147\n",
            "Epoch 2 Step 31 of 219, loss = 0.9172452865168452\n",
            "Epoch 2 Step 32 of 219, loss = 0.39955002116039395\n",
            "Epoch 2 Step 33 of 219, loss = 1.190860322676599\n",
            "Epoch 2 Step 34 of 219, loss = 0.9234969303943217\n",
            "Epoch 2 Step 35 of 219, loss = 0.2823483180254698\n",
            "Epoch 2 Step 36 of 219, loss = 0.4013036089017987\n",
            "Epoch 2 Step 37 of 219, loss = 0.38535528955981135\n",
            "Epoch 2 Step 38 of 219, loss = 0.27824797108769417\n",
            "Epoch 2 Step 39 of 219, loss = 0.5859879944473505\n",
            "Epoch 2 Step 40 of 219, loss = 0.32965486589819193\n",
            "Epoch 2 Step 41 of 219, loss = 0.41944747138768435\n",
            "Epoch 2 Step 42 of 219, loss = 0.3602322321385145\n",
            "Epoch 2 Step 43 of 219, loss = 0.220952152274549\n",
            "Epoch 2 Step 44 of 219, loss = 0.2475343607366085\n",
            "Epoch 2 Step 45 of 219, loss = 0.5906110419891775\n",
            "Epoch 2 Step 46 of 219, loss = 0.36693436559289694\n",
            "Epoch 2 Step 47 of 219, loss = 1.5672263600863516\n",
            "Epoch 2 Step 48 of 219, loss = 0.18664424028247595\n",
            "Epoch 2 Step 49 of 219, loss = 1.3041811045259237\n",
            "Epoch 2 Step 50 of 219, loss = 0.20881902799010277\n",
            "Epoch 2 Step 51 of 219, loss = 0.15285817626863718\n",
            "Epoch 2 Step 52 of 219, loss = 0.148658849298954\n",
            "Epoch 2 Step 53 of 219, loss = 0.3317473803181201\n",
            "Epoch 2 Step 54 of 219, loss = 0.6090551253873855\n",
            "Epoch 2 Step 55 of 219, loss = 0.12910877796821296\n",
            "Epoch 2 Step 56 of 219, loss = 0.3217840928118676\n",
            "Epoch 2 Step 57 of 219, loss = 0.10280719585716724\n",
            "Epoch 2 Step 58 of 219, loss = 0.3416527980007231\n",
            "Epoch 2 Step 59 of 219, loss = 0.40122827095910907\n",
            "Epoch 2 Step 60 of 219, loss = 0.7312146027106792\n",
            "Epoch 2 Step 61 of 219, loss = 0.6840989347547293\n",
            "Epoch 2 Step 62 of 219, loss = 0.6135251193773001\n",
            "Epoch 2 Step 63 of 219, loss = 0.390134557033889\n",
            "Epoch 2 Step 64 of 219, loss = 0.07778410078026354\n",
            "Epoch 2 Step 65 of 219, loss = 0.07591413846239448\n",
            "Epoch 2 Step 66 of 219, loss = 0.36996576248202473\n",
            "Epoch 2 Step 67 of 219, loss = 1.3548324583098292\n",
            "Epoch 2 Step 68 of 219, loss = 1.6709960793377832\n",
            "Epoch 2 Step 69 of 219, loss = 0.3601728326175362\n",
            "Epoch 2 Step 70 of 219, loss = 0.3532333371695131\n",
            "Epoch 2 Step 71 of 219, loss = 1.327897152863443\n",
            "Epoch 2 Step 72 of 219, loss = 0.38705448526889086\n",
            "Epoch 2 Step 73 of 219, loss = 0.33830086328089237\n",
            "Epoch 2 Step 74 of 219, loss = 0.41928571881726384\n",
            "Epoch 2 Step 75 of 219, loss = 0.4354682406410575\n",
            "Epoch 2 Step 76 of 219, loss = 0.6652837155852467\n",
            "Epoch 2 Step 77 of 219, loss = 0.6372630351688713\n",
            "Epoch 2 Step 78 of 219, loss = 0.33581673447042704\n",
            "Epoch 2 Step 79 of 219, loss = 0.3632294526323676\n",
            "Epoch 2 Step 80 of 219, loss = 0.5013504116795957\n",
            "Epoch 2 Step 81 of 219, loss = 0.14718141267076135\n",
            "Epoch 2 Step 82 of 219, loss = 0.34607783053070307\n",
            "Epoch 2 Step 83 of 219, loss = 0.8162830923683941\n",
            "Epoch 2 Step 84 of 219, loss = 0.17685571871697903\n",
            "Epoch 2 Step 85 of 219, loss = 0.6171612720936537\n",
            "Epoch 2 Step 86 of 219, loss = 0.3065948514267802\n",
            "Epoch 2 Step 87 of 219, loss = 1.1430736370384693\n",
            "Epoch 2 Step 88 of 219, loss = 0.5227258787490427\n",
            "Epoch 2 Step 89 of 219, loss = 1.2919111652299762\n",
            "Epoch 2 Step 90 of 219, loss = 0.2888991702347994\n",
            "Epoch 2 Step 91 of 219, loss = 0.6354763605631888\n",
            "Epoch 2 Step 92 of 219, loss = 0.5506509384140372\n",
            "Epoch 2 Step 93 of 219, loss = 0.5800179904326797\n",
            "Epoch 2 Step 94 of 219, loss = 0.5390226086601615\n",
            "Epoch 2 Step 95 of 219, loss = 0.3050944795832038\n",
            "Epoch 2 Step 96 of 219, loss = 0.5899520749226213\n",
            "Epoch 2 Step 97 of 219, loss = 0.29632294829934835\n",
            "Epoch 2 Step 98 of 219, loss = 0.8070519352331758\n",
            "Epoch 2 Step 99 of 219, loss = 0.9020488099195063\n",
            "Epoch 2 Step 100 of 219, loss = 0.6040441514924169\n",
            "Epoch 2 Step 101 of 219, loss = 0.7213771203532815\n",
            "Epoch 2 Step 102 of 219, loss = 0.7446161461994052\n",
            "Epoch 2 Step 103 of 219, loss = 0.3763420758768916\n",
            "Epoch 2 Step 104 of 219, loss = 0.48917402047663927\n",
            "Epoch 2 Step 105 of 219, loss = 0.36169576831161976\n",
            "Epoch 2 Step 106 of 219, loss = 0.45320977875962853\n",
            "Epoch 2 Step 107 of 219, loss = 0.47276287607382983\n",
            "Epoch 2 Step 108 of 219, loss = 0.41234726551920176\n",
            "Epoch 2 Step 109 of 219, loss = 0.591798537876457\n",
            "Epoch 2 Step 110 of 219, loss = 0.7432856191881001\n",
            "Epoch 2 Step 111 of 219, loss = 0.6352780712768435\n",
            "Epoch 2 Step 112 of 219, loss = 0.5727745722979307\n",
            "Epoch 2 Step 113 of 219, loss = 0.6353899960231502\n",
            "Epoch 2 Step 114 of 219, loss = 0.6304282830096781\n",
            "Epoch 2 Step 115 of 219, loss = 0.7028064418118447\n",
            "Epoch 2 Step 116 of 219, loss = 0.7514026993885636\n",
            "Epoch 2 Step 117 of 219, loss = 0.38176264241337776\n",
            "Epoch 2 Step 118 of 219, loss = 0.27757636830210686\n",
            "Epoch 2 Step 119 of 219, loss = 0.39182525873184204\n",
            "Epoch 2 Step 120 of 219, loss = 0.44117093132808805\n",
            "Epoch 2 Step 121 of 219, loss = 0.4444830813445151\n",
            "Epoch 2 Step 122 of 219, loss = 0.47144447488244623\n",
            "Epoch 2 Step 123 of 219, loss = 0.17514798697084188\n",
            "Epoch 2 Step 124 of 219, loss = 0.8343518651090562\n",
            "Epoch 2 Step 125 of 219, loss = 0.20773977786302567\n",
            "Epoch 2 Step 126 of 219, loss = 0.4807289675809443\n",
            "Epoch 2 Step 127 of 219, loss = 0.13428799575194716\n",
            "Epoch 2 Step 128 of 219, loss = 0.37618624209426343\n",
            "Epoch 2 Step 129 of 219, loss = 0.6365738157182932\n",
            "Epoch 2 Step 130 of 219, loss = 0.6085541637148708\n",
            "Epoch 2 Step 131 of 219, loss = 0.07856543082743883\n",
            "Epoch 2 Step 132 of 219, loss = 0.5785165538545698\n",
            "Epoch 2 Step 133 of 219, loss = 1.1803349445108324\n",
            "Epoch 2 Step 134 of 219, loss = 1.2411339078098536\n",
            "Epoch 2 Step 135 of 219, loss = 1.1571014223154634\n",
            "Epoch 2 Step 136 of 219, loss = 0.8979467053432018\n",
            "Epoch 2 Step 137 of 219, loss = 1.3986297291703522\n",
            "Epoch 2 Step 138 of 219, loss = 1.4451182917691767\n",
            "Epoch 2 Step 139 of 219, loss = 0.5084649161435664\n",
            "Epoch 2 Step 140 of 219, loss = 1.0237449202686548\n",
            "Epoch 2 Step 141 of 219, loss = 0.6448526289314032\n",
            "Epoch 2 Step 142 of 219, loss = 0.6964154178276658\n",
            "Epoch 2 Step 143 of 219, loss = 0.7402089275419712\n",
            "Epoch 2 Step 144 of 219, loss = 1.0147356316447258\n",
            "Epoch 2 Step 145 of 219, loss = 0.7307834066450596\n",
            "Epoch 2 Step 146 of 219, loss = 0.5772377436514944\n",
            "Epoch 2 Step 147 of 219, loss = 0.6459119194187224\n",
            "Epoch 2 Step 148 of 219, loss = 0.7637929953634739\n",
            "Epoch 2 Step 149 of 219, loss = 0.6948135315724357\n",
            "Epoch 2 Step 150 of 219, loss = 0.6769263816531748\n",
            "Epoch 2 Step 151 of 219, loss = 0.6921721526887268\n",
            "Epoch 2 Step 152 of 219, loss = 0.595664017368108\n",
            "Epoch 2 Step 153 of 219, loss = 0.7232648618519306\n",
            "Epoch 2 Step 154 of 219, loss = 0.6268310435116291\n",
            "Epoch 2 Step 155 of 219, loss = 0.6520853480324149\n",
            "Epoch 2 Step 156 of 219, loss = 0.5656618171487935\n",
            "Epoch 2 Step 157 of 219, loss = 0.4424746409058571\n",
            "Epoch 2 Step 158 of 219, loss = 0.6884203953668475\n",
            "Epoch 2 Step 159 of 219, loss = 0.7688583713024855\n",
            "Epoch 2 Step 160 of 219, loss = 0.6781364800408483\n",
            "Epoch 2 Step 161 of 219, loss = 0.4394804798066616\n",
            "Epoch 2 Step 162 of 219, loss = 0.4088169038295746\n",
            "Epoch 2 Step 163 of 219, loss = 0.4940241826698184\n",
            "Epoch 2 Step 164 of 219, loss = 0.555412326939404\n",
            "Epoch 2 Step 165 of 219, loss = 0.9041526722721756\n",
            "Epoch 2 Step 166 of 219, loss = 0.4574300395324826\n",
            "Epoch 2 Step 167 of 219, loss = 1.2577083567157388\n",
            "Epoch 2 Step 168 of 219, loss = 0.5975969941355288\n",
            "Epoch 2 Step 169 of 219, loss = 0.3370982832275331\n",
            "Epoch 2 Step 170 of 219, loss = 0.7066652714274824\n",
            "Epoch 2 Step 171 of 219, loss = 0.6963517367839813\n",
            "Epoch 2 Step 172 of 219, loss = 0.7567552123218775\n",
            "Epoch 2 Step 173 of 219, loss = 0.559400093741715\n",
            "Epoch 2 Step 174 of 219, loss = 0.8785451399162412\n",
            "Epoch 2 Step 175 of 219, loss = 1.0088030844926834\n",
            "Epoch 2 Step 176 of 219, loss = 0.5644665351137519\n",
            "Epoch 2 Step 177 of 219, loss = 0.8761136569082737\n",
            "Epoch 2 Step 178 of 219, loss = 0.5026770299300551\n",
            "Epoch 2 Step 179 of 219, loss = 0.6385592119768262\n",
            "Epoch 2 Step 180 of 219, loss = 0.5472100966144353\n",
            "Epoch 2 Step 181 of 219, loss = 0.5542417969554663\n",
            "Epoch 2 Step 182 of 219, loss = 0.4565801601856947\n",
            "Epoch 2 Step 183 of 219, loss = 0.4589625149965286\n",
            "Epoch 2 Step 184 of 219, loss = 0.5617730570957065\n",
            "Epoch 2 Step 185 of 219, loss = 0.5353122404776514\n",
            "Epoch 2 Step 186 of 219, loss = 0.5698886159807444\n",
            "Epoch 2 Step 187 of 219, loss = 0.53039894066751\n",
            "Epoch 2 Step 188 of 219, loss = 0.7485753316432238\n",
            "Epoch 2 Step 189 of 219, loss = 0.52989721018821\n",
            "Epoch 2 Step 190 of 219, loss = 0.7169548012316227\n",
            "Epoch 2 Step 191 of 219, loss = 0.7553407344967127\n",
            "Epoch 2 Step 192 of 219, loss = 0.5998927466571331\n",
            "Epoch 2 Step 193 of 219, loss = 0.42419842816889286\n",
            "Epoch 2 Step 194 of 219, loss = 0.46383841149508953\n",
            "Epoch 2 Step 195 of 219, loss = 0.6183941885828972\n",
            "Epoch 2 Step 196 of 219, loss = 0.6331520061939955\n",
            "Epoch 2 Step 197 of 219, loss = 0.6366742409300059\n",
            "Epoch 2 Step 198 of 219, loss = 0.6089598536491394\n",
            "Epoch 2 Step 199 of 219, loss = 0.7392587554641068\n",
            "Epoch 2 Step 200 of 219, loss = 0.8586282068863511\n",
            "Epoch 2 Step 201 of 219, loss = 0.8809743411839008\n",
            "Epoch 2 Step 202 of 219, loss = 0.5158112375065684\n",
            "Epoch 2 Step 203 of 219, loss = 0.7197817068081349\n",
            "Epoch 2 Step 204 of 219, loss = 0.5584924662070989\n",
            "Epoch 2 Step 205 of 219, loss = 0.424026919528842\n",
            "Epoch 2 Step 206 of 219, loss = 0.6876433654688299\n",
            "Epoch 2 Step 207 of 219, loss = 0.5471400846727192\n",
            "Epoch 2 Step 208 of 219, loss = 0.530410747975111\n",
            "Epoch 2 Step 209 of 219, loss = 0.561954396776855\n",
            "Epoch 2 Step 210 of 219, loss = 0.6760027105920017\n",
            "Epoch 2 Step 211 of 219, loss = 0.7416640073060989\n",
            "Epoch 2 Step 212 of 219, loss = 0.8319708984345198\n",
            "Epoch 2 Step 213 of 219, loss = 0.6871592099778354\n",
            "Epoch 2 Step 214 of 219, loss = 0.5747637543827295\n",
            "Epoch 2 Step 215 of 219, loss = 0.5880286603642162\n",
            "Epoch 2 Step 216 of 219, loss = 0.6459215213544667\n",
            "Epoch 2 Step 217 of 219, loss = 0.527442894410342\n",
            "Epoch 2 Step 218 of 219, loss = 0.8472640265244991\n",
            "Epoch 2 average train_loss: 0.612849 test_loss: 0.701606 test_score 0.53%\n"
          ]
        }
      ],
      "source": [
        "for iEpoch in range(15):\n",
        "\n",
        "    # train\n",
        "    train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n",
        "        model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n",
        "    \n",
        "    bert_train_acc.append(train_score)\n",
        "    bert_avg_train_loss.append(avg_train_loss)\n",
        "    bert_train_predict.append(y_train_prediction)\n",
        "\n",
        "    ### evaluate\n",
        "    test_score, avg_test_loss, y_test_prediction = evaluateModel(\n",
        "        model, test_dataloader, device)\n",
        "\n",
        "    bert_test_acc.append(test_score)\n",
        "    bert_avg_test_loss.append(avg_test_loss)\n",
        "    bert_test_predict.append(y_test_prediction)\n",
        "\n",
        "    # log epoch\n",
        "    print(f'Epoch {iEpoch} average train_loss: {avg_train_loss:.6f} test_loss: {avg_test_loss:.6f} test_score {test_score:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Graphics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x20832f40250>]"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(bert_train_acc)\n",
        "plt.plot(bert_test_acc)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(bert_avg_train_loss)\n",
        "plt.plot(bert_avg_test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model RuBert-Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ruRoberta-large' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://huggingface.co/sberbank-ai/ruRoberta-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model_path = os.path.join(base_path, 'ruRoberta-large/')\n",
        "#print(model_path)\n",
        "#\n",
        "#config_path = os.path.join(base_path, model_path, 'config.json')\n",
        "#conf = BertConfig.from_json_file(config_path)\n",
        "#conf.num_labels = len(l2i)\n",
        "#\n",
        "#output_model_file = os.path.join(base_path, model_path, 'pytorch_model.bin')\n",
        "#\n",
        "#model = BertModel(conf)\n",
        "#\n",
        "#model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "#model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/ruRoberta-large\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"sberbank-ai/ruRoberta-large\")\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=LEARNING_RATE, \n",
        "    correct_bias=False)\n",
        "    \n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=LEARNING_RATE, \n",
        "    steps_per_epoch=len(train_dataloader), \n",
        "    epochs=EPOCHS_LIMIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lberta_train_predict = []\n",
        "lberta_test_predict = []\n",
        "\n",
        "lberta_avg_train_loss = []\n",
        "lberta_avg_test_loss = []\n",
        "\n",
        "lberta_train_acc = []\n",
        "lberta_test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2064\\2608895368.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n\u001b[0m\u001b[0;32m      5\u001b[0m         model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2064\\1998468137.py\u001b[0m in \u001b[0;36mtrainModelIteration\u001b[1;34m(model, optimizer, scheduler, in_dataloader, MAX_GRAD_NORM, EPOCH_INDEX)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         outputs = model(\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_token_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m         outputs = self.roberta(\n\u001b[0m\u001b[0;32m   1092\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         )\n\u001b[1;32m--> 844\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    845\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m                 )\n\u001b[0;32m    519\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    521\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    406\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 332\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "for iEpoch in range(3):\n",
        "\n",
        "    # train\n",
        "    train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n",
        "        model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n",
        "    \n",
        "    lberta_train_acc.append(train_score)\n",
        "    lberta_avg_train_loss.append(avg_train_loss)\n",
        "    lberta_train_predict.append(y_train_prediction)\n",
        "\n",
        "    ### evaluate\n",
        "    test_score, avg_test_loss, y_test_prediction = evaluateModel(\n",
        "        model, test_dataloader, device)\n",
        "\n",
        "    lberta_test_acc.append(test_score)\n",
        "    lberta_avg_test_loss.append(avg_test_loss)\n",
        "    lberta_test_predict.append(y_test_prediction)\n",
        "\n",
        "    # log epoch\n",
        "    print(f'Epoch {iEpoch} average train_loss: {avg_train_loss:.6f} test_loss: {avg_test_loss:.6f} test_score {test_score:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch_seq2seq_with_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "846dd53c5a100503afcb3f5301bb10f61481596a80ae839ecd432be859b5d4d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
