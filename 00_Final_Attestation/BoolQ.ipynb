{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "__COLAB_ACTIVE = False\n",
        "__POOL_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Проект 3. Решить задачу DaNetQA / BoolQ\n",
        "\n",
        "Можно решить как задачу для русского, так и для английского.\n",
        "\n",
        "Либо провести эксперименты с многоязычной моделью\n",
        "\n",
        "https://russiansuperglue.com/ru/tasks/task_info/DaNetQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Описание\n",
        "Причинно-следственная связь, логический вывод, Natural Language Inference\n",
        "\n",
        "DaNetQA - это набор да/нет вопросов с ответами и фрагментом текста, содержащим ответ. Все вопросы были написаны авторами без каких-либо искусственных ограничений.\n",
        "\n",
        "Каждый пример представляет собой триплет (вопрос, фрагмент текста, ответ) с заголовком страницы в качестве необязательного дополнительного контекста.\n",
        "\n",
        "Настройка классификации текстовых пар аналогична существующим задачам логического вывода (NLI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тип задачи\n",
        "Логика, Commonsense, Знания о мире. Бинарная классификация: true/false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root path: 'd:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation'\n",
            "Dataset path: d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\DaNetQA\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "base_path = os.path.abspath('')\n",
        "if __COLAB_ACTIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = os.path.join(base_path, 'drive/MyDrive/DSnML_Innopolis2022')\n",
        "\n",
        "print(f\"Root path: '{base_path}'\")\n",
        "\n",
        "trainPartNameRaw = 'raw_train'\n",
        "testPartNameRaw = 'raw_val'\n",
        "validatePartNameRaw = 'raw_test'\n",
        "\n",
        "trainPartName = 'train_v1'\n",
        "testPartName = 'val_v1'\n",
        "validatePartName = 'test_v1'\n",
        "parts = [trainPartName, testPartName]\n",
        "data_path = os.path.join(base_path, 'DaNetQA')\n",
        "print(f\"Dataset path: {data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fileNameData(s):\n",
        "    return f\"{os.path.join(data_path, s)}.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\leysh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadJSONL(path, name):\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    print(name)\n",
        "    display(df.head())\n",
        "    if (df.columns.values == 'label').any():\n",
        "        s = np.unique(df['label'].to_numpy(), return_counts=True)[1]\n",
        "        print(f\"True answer: {s[1]}\")\n",
        "        print(f\"False answer: {s[0]}\")\n",
        "        print(\"\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>«Вы́ставочный центр» — станция Московского мон...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>Вы́ставка достиже́ний наро́дного хозя́йства  ,...</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Был ли джиган в black star?</td>\n",
              "      <td>Вместе с этим треком они выступили на церемони...</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Xiaomi конкурент apple?</td>\n",
              "      <td>Xiaomi — китайская компания, основанная в 2010...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли автомат калашникова в вов?</td>\n",
              "      <td>Отметив некоторые недостатки и в целом удачную...</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            question  \\\n",
              "0      Вднх - это выставочный центр?   \n",
              "1      Вднх - это выставочный центр?   \n",
              "2        Был ли джиган в black star?   \n",
              "3            Xiaomi конкурент apple?   \n",
              "4  Был ли автомат калашникова в вов?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  «Вы́ставочный центр» — станция Московского мон...   True    0  \n",
              "1  Вы́ставка достиже́ний наро́дного хозя́йства  ,...   True    1  \n",
              "2  Вместе с этим треком они выступили на церемони...   True    2  \n",
              "3  Xiaomi — китайская компания, основанная в 2010...   True    3  \n",
              "4  Отметив некоторые недостатки и в целом удачную...  False    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 1061\n",
            "False answer: 688\n",
            "\n",
            "Test set:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>вода марсе ?</td>\n",
              "      <td>гидросфера марса — это совокупность водных зап...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>состоит англия евросоюзе ?</td>\n",
              "      <td>полночь 31 января 1 февраля 2020 года централь...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>деиствительно ссср адвокатов ?</td>\n",
              "      <td>семен львович ария — советскии россиискии юрис...</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>чума оране ?</td>\n",
              "      <td>чума — это абсурд , осмысливается форма сущест...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>кетчуп читосе ?</td>\n",
              "      <td>текущии каталог продукции размещен саите произ...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         question  \\\n",
              "0                    вода марсе ?   \n",
              "1      состоит англия евросоюзе ?   \n",
              "2  деиствительно ссср адвокатов ?   \n",
              "3                    чума оране ?   \n",
              "4                 кетчуп читосе ?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  гидросфера марса — это совокупность водных зап...   True    0  \n",
              "1  полночь 31 января 1 февраля 2020 года централь...  False    1  \n",
              "2  семен львович ария — советскии россиискии юрис...  False    2  \n",
              "3  чума — это абсурд , осмысливается форма сущест...   True    3  \n",
              "4  текущии каталог продукции размещен саите произ...   True    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 412\n",
            "False answer: 409\n",
            "\n",
            "Validation set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полезна ли ртуть с градусника?</td>\n",
              "      <td>Отравления ртутью  — расстройства здоровья, св...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Являются ли сапрофаги хищниками?</td>\n",
              "      <td>Фауна лесных почв — совокупность видов животны...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Водятся ли в индии крокодилы?</td>\n",
              "      <td>Болотный крокодил, или магер  — пресмыкающееся...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Есть ли в батате крахмал?</td>\n",
              "      <td>Клубневидно вздутые корни  весят до 15 кг, сод...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли человек в железной маске?</td>\n",
              "      <td>Остров Сент-Маргерит  — крупнейший из Лерински...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           question  \\\n",
              "0    Полезна ли ртуть с градусника?   \n",
              "1  Являются ли сапрофаги хищниками?   \n",
              "2     Водятся ли в индии крокодилы?   \n",
              "3         Есть ли в батате крахмал?   \n",
              "4  Был ли человек в железной маске?   \n",
              "\n",
              "                                             passage  idx  \n",
              "0  Отравления ртутью  — расстройства здоровья, св...    0  \n",
              "1  Фауна лесных почв — совокупность видов животны...    1  \n",
              "2  Болотный крокодил, или магер  — пресмыкающееся...    2  \n",
              "3  Клубневидно вздутые корни  весят до 15 кг, сод...    3  \n",
              "4  Остров Сент-Маргерит  — крупнейший из Лерински...    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_train = loadJSONL(fileNameData(trainPartNameRaw), \"Train set\")\n",
        "df_test = loadJSONL(fileNameData(testPartNameRaw), \"Test set:\")\n",
        "df_validation = loadJSONL(fileNameData(validatePartNameRaw), \"Validation set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Очистка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataCleaner:\n",
        "    def __init__(self) -> None:\n",
        "        self.flag_verbose = True\n",
        "\n",
        "        self.stop_words = stopwords.words('russian')\n",
        "        self.stemmer = SnowballStemmer('russian')\n",
        "\n",
        "        self.count_removed_symbols = dict()\n",
        "        self.count_removed_words = dict()\n",
        "\n",
        "        self.count_replaced_symbols = dict()\n",
        "        self.dict_replaced_symbols = dict()\n",
        "\n",
        "        self.count_replaced_words = dict()\n",
        "        self.dict_replaced_words = dict()\n",
        "\n",
        "        self.char_to_remove = ['«', '»', '—', ',', '.', '-', '/', ':', '!', \"?\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"=\", \"|\", \"\\\\\", \">\", \"<\"]\n",
        "        self.char_to_replace = [['ё', 'е']]\n",
        "\n",
        "    # функция подсчета количества измененных слов\n",
        "    def addReplacedWord(self, s_from, s_to = ' '):\n",
        "        if not self.count_replaced_words.keys().__contains__(s_from):\n",
        "            self.count_replaced_words[s_from] = 0\n",
        "        self.count_replaced_words[s_from] += 1\n",
        "        self.dict_replaced_words[s_from] = s_to\n",
        "\n",
        "    # функция подсчета количества удаленных слов\n",
        "    def addRemovedWord(self, w):\n",
        "        if w == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(w):\n",
        "                self.count_removed_symbols[w] = 0\n",
        "            self.count_removed_symbols[w] += 1\n",
        "\n",
        "    # функция подсчета количества удаленных символов\n",
        "    def addReplacedSymbol(self, s_from, s_to = ' '):\n",
        "        if s_to == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(s_from):\n",
        "                self.count_removed_symbols[s_from] = 0\n",
        "            self.count_removed_symbols[s_from] += 1\n",
        "        else:\n",
        "            if not self.count_replaced_symbols.keys().__contains__(s_from):\n",
        "                self.count_replaced_symbols[s_from] = 0\n",
        "            self.count_replaced_symbols[s_from] += 1\n",
        "            self.dict_replaced_symbols[s_from] = s_to\n",
        "\n",
        "    # удаление знаков ударения и прочих символов unicode\n",
        "    def unicodeToAscii(self, s):\n",
        "        tmp = []\n",
        "        for c in unicodedata.normalize('NFD', s):\n",
        "            if unicodedata.category(c) != 'Mn':\n",
        "                tmp.append(c)\n",
        "            else:\n",
        "                self.addReplacedSymbol(c)\n",
        "        return ''.join(tmp)\n",
        "\n",
        "    # если нужно удалить, то заменяем на пробел чтоб не потерят разделения слов\n",
        "    def replaceChar(self, s):\n",
        "        tmp = []\n",
        "        for i, c in enumerate(s):\n",
        "            if self.char_to_remove.__contains__(c):\n",
        "                self.addReplacedSymbol(c, s[i])\n",
        "                tmp.append(' ')\n",
        "            else:\n",
        "                tmp.append(c)\n",
        "        s = \"\".join(tmp)\n",
        "\n",
        "        for s_from, s_to in self.char_to_replace:\n",
        "            if c == s_from:\n",
        "                s[i] = s_to\n",
        "                self.addReplacedSymbol(s_from, s_to)\n",
        "        return s\n",
        "\n",
        "    # удаляем лишние пробелы\n",
        "    def trimSpaces(self, s):\n",
        "        while s.__contains__('  '):\n",
        "            s = s.replace('  ', ' ')\n",
        "        s = s.strip()\n",
        "        return s\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def removeStopWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            if word not in self.stop_words:\n",
        "                tmp.append(word)\n",
        "            else:\n",
        "                self.addRemovedWord(word)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def StemmWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            wordStemmed = self.stemmer.stem(word)\n",
        "            tmp.append(wordStemmed)\n",
        "            if word != wordStemmed:\n",
        "                self.addReplacedWord(word, wordStemmed)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    def clean(self, df, column):\n",
        "        for i in range(len(df)):\n",
        "            df[column][i] = self.unicodeToAscii(df[column][i])\n",
        "            df[column][i] = df[column][i].lower()\n",
        "            df[column][i] = self.replaceChar(df[column][i])\n",
        "            df[column][i] = self.removeStopWords(df[column][i])\n",
        "            df[column][i] = self.StemmWords(df[column][i])\n",
        "            df[column][i] = self.trimSpaces(df[column][i])\n",
        "        return df\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def print(self, vals):\n",
        "        if self.flag_verbose == True:\n",
        "            print(vals)\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def display(self, vals):\n",
        "            if self.flag_verbose == True:\n",
        "                display(vals)\n",
        "\n",
        "    # сбор лога в dataframe, опциональный вывод на экран \n",
        "    def summary(self, verbose = True):\n",
        "        self.flag_verbose = verbose\n",
        "        dfs = []\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Chars        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_symbols:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_words:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Words', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Replaced Chars       ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol_from\", \"symbol_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_symbols:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_symbols[c], self.count_replaced_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Replaced Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Stemmed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word_from\", \"word_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_words:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_words[c], self.count_replaced_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Stemmed Words', dfRemoved])\n",
        "\n",
        "        return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:102: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.unicodeToAscii(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:103: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = df[column][i].lower()\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.replaceChar(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:105: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.removeStopWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.StemmWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_2064\\3611868607.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.trimSpaces(df[column][i])\n"
          ]
        }
      ],
      "source": [
        "t = DataCleaner()\n",
        "df_train = t.clean(df_train, 'passage')\n",
        "df_test = t.clean(df_test, 'passage')\n",
        "df_validation = t.clean(df_validation, 'passage')\n",
        "df_train = t.clean(df_train, 'question')\n",
        "df_test = t.clean(df_test, 'question')\n",
        "df_validation = t.clean(df_validation, 'question')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Chars        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [symbol, count_removed]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [word, count_removed]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Replaced Chars       ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol_from</th>\n",
              "      <th>symbol_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«</td>\n",
              "      <td>«</td>\n",
              "      <td>2246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>»</td>\n",
              "      <td>»</td>\n",
              "      <td>2233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>—</td>\n",
              "      <td>—</td>\n",
              "      <td>4524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>19367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>4010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>27875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>)</td>\n",
              "      <td>)</td>\n",
              "      <td>1430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>:</td>\n",
              "      <td>:</td>\n",
              "      <td>1395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>%</td>\n",
              "      <td>%</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/</td>\n",
              "      <td>/</td>\n",
              "      <td>198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>|</td>\n",
              "      <td>|</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>]</td>\n",
              "      <td>]</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[</td>\n",
              "      <td>[</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>!</td>\n",
              "      <td>!</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>&lt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>3439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>&amp;</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>=</td>\n",
              "      <td>=</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>$</td>\n",
              "      <td>$</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>#</td>\n",
              "      <td>#</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(</td>\n",
              "      <td>(</td>\n",
              "      <td>1337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>{</td>\n",
              "      <td>{</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>}</td>\n",
              "      <td>}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol_from symbol_to count_replaced\n",
              "0            «         «           2246\n",
              "1            »         »           2233\n",
              "2            —         —           4524\n",
              "3            .         .          19367\n",
              "4            -         -           4010\n",
              "5            ,         ,          27875\n",
              "6            )         )           1430\n",
              "7            :         :           1395\n",
              "8            %         %            348\n",
              "9            /         /            198\n",
              "10           |         |              7\n",
              "11           ]         ]            154\n",
              "12           [         [            149\n",
              "13           !         !             54\n",
              "14           *         *             13\n",
              "15           <         <              6\n",
              "16           >         >             19\n",
              "17           ?         ?           3439\n",
              "18           &         &              5\n",
              "19           =         =              9\n",
              "20           $         $             10\n",
              "21           #         #              6\n",
              "22           (         (           1337\n",
              "23           {         {              3\n",
              "24           }         }              1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Stemmed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_from</th>\n",
              "      <th>word_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>выставочныи</td>\n",
              "      <td>выставочны</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>станция</td>\n",
              "      <td>станц</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>московского</td>\n",
              "      <td>московск</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>монорельса</td>\n",
              "      <td>монорельс</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>расположена</td>\n",
              "      <td>располож</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50960</th>\n",
              "      <td>себестоимость</td>\n",
              "      <td>себестоим</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50961</th>\n",
              "      <td>ювелирная</td>\n",
              "      <td>ювелирн</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50962</th>\n",
              "      <td>завоеван</td>\n",
              "      <td>завоева</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50963</th>\n",
              "      <td>новорожденные</td>\n",
              "      <td>новорожден</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50964</th>\n",
              "      <td>бодрит</td>\n",
              "      <td>бодр</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50965 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word_from     word_to count_replaced\n",
              "0        выставочныи  выставочны              7\n",
              "1            станция       станц              7\n",
              "2        московского    московск             26\n",
              "3         монорельса   монорельс              2\n",
              "4        расположена    располож             21\n",
              "...              ...         ...            ...\n",
              "50960  себестоимость   себестоим              1\n",
              "50961      ювелирная     ювелирн              1\n",
              "50962       завоеван     завоева              1\n",
              "50963  новорожденные  новорожден              1\n",
              "50964         бодрит        бодр              1\n",
              "\n",
              "[50965 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dfs = t.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'выставк достижен народн хозяиств 1959 1991 год выставк достижен народн хозяиств ссср 1992 2014 год всероссииск выставочны центр выставочны комплекс останкинск раион север восточн административн округ город москв второ величин выставочны комплекс город вход 50 крупнеиш выставочн центр мир ежегодн вднх посеща 30 млн гост 1 август 2019 год выставк отпразднова 80 летн юбил территориальн вднх объедин парк останкин главн ботаническ сад общ площад составля 700 га 240 2 га площад вднх 75 6 га площад парк останкин 361 га площад гбс 9 5 га музеин выставочны центр рабоч колхозниц площад арко главн вход территор выставк располож множеств шедевр архитектур 49 объект вднх призна памятник культурн наслед'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.passage[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.to_json(fileNameData(trainPartName), force_ascii=False, lines=True, orient='records')\n",
        "df_test.to_json(fileNameData(testPartNameRaw), force_ascii=False, lines=True, orient='records')\n",
        "df_validation.to_json(fileNameData(validatePartName), force_ascii=False, lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Number Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>вода марсе ?</td>\n",
              "      <td>гидросфера марса — это совокупность водных зап...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>состоит англия евросоюзе ?</td>\n",
              "      <td>полночь 31 января 1 февраля 2020 года централь...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>деиствительно ссср адвокатов ?</td>\n",
              "      <td>семен львович ария — советскии россиискии юрис...</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>чума оране ?</td>\n",
              "      <td>чума — это абсурд , осмысливается форма сущест...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>кетчуп читосе ?</td>\n",
              "      <td>текущии каталог продукции размещен саите произ...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         question  \\\n",
              "0                    вода марсе ?   \n",
              "1      состоит англия евросоюзе ?   \n",
              "2  деиствительно ссср адвокатов ?   \n",
              "3                    чума оране ?   \n",
              "4                 кетчуп читосе ?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  гидросфера марса — это совокупность водных зап...   True    0  \n",
              "1  полночь 31 января 1 февраля 2020 года централь...  False    1  \n",
              "2  семен львович ария — советскии россиискии юрис...  False    2  \n",
              "3  чума — это абсурд , осмысливается форма сущест...   True    3  \n",
              "4  текущии каталог продукции размещен саите произ...   True    4  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test = loadJSONL(fileNameData(testPartName), lines=True)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ True False False  True  True  True  True  True  True False]\n"
          ]
        }
      ],
      "source": [
        "y_test = df_test['label'].to_numpy()\n",
        "print(y_test[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng_score = []\n",
        "for _ in range(5):\n",
        "    validation_pred = [(True if b == 1 else False) for b in np.random.randint(2, size=( len(y_test)))]\n",
        "    rng_score.append(accuracy_score(y_test, validation_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5066991473812423,\n",
              " 0.48964677222898906,\n",
              " 0.4725943970767357,\n",
              " 0.5140073081607796,\n",
              " 0.47990255785627284]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF + LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import codecs\n",
        "import json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_feature_DaNetQA(row):\n",
        "    res = str(row[\"question\"]).strip()\n",
        "    label = row.get(\"label\")\n",
        "    return res, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_features_DaNetQA(path, vect):\n",
        "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
        "        lines = reader.read().split(\"\\n\")\n",
        "        lines = list(map(json.loads, filter(None, lines)))\n",
        "    res = list(map(build_feature_DaNetQA, lines))\n",
        "    texts = list(map(lambda x: x[0], res))\n",
        "    labels = list(map(lambda x: x[1], res))\n",
        "    ids = [x[\"idx\"] for x in lines]\n",
        "    return (vect.transform(texts), labels), ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_DaNetQA(train, labels):\n",
        "    clf = LogisticRegression()\n",
        "    return clf.fit(train, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_DaNetQA(train_path, val_path, test_path, vect):\n",
        "    train, _ = build_features_DaNetQA(train_path, vect)\n",
        "    val, _ = build_features_DaNetQA(val_path, vect)\n",
        "    test, ids = build_features_DaNetQA(test_path, vect)\n",
        "    clf = fit_DaNetQA(*train)\n",
        "    try:\n",
        "        test_score = clf.score(*test)\n",
        "    except ValueError:\n",
        "        test_score = None\n",
        "    test_pred = clf.predict(test[0])\n",
        "    return clf, {\n",
        "        \"train\": clf.score(*train),\n",
        "        \"val\": clf.score(*val),\n",
        "        \"test\": test_score,\n",
        "        \"test_pred\": [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(ids, test_pred)]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pre-Trained TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://russiansuperglue.com/tasks/tf_idf\n",
        "!unzip tf_idf_baseline.zip\n",
        "!rm tf_idf_baseline.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vect = joblib.load(\"tfidf.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Score Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = fileNameData(trainPartNameRaw)\n",
        "test_path = fileNameData(testPartNameRaw)\n",
        "val_path = fileNameData(validatePartNameRaw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.8010291595197255\n",
            "Accuracy on validation data = 0.37393422655298414\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_scores = eval_DaNetQA(train_path, test_path, val_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_scores[\"val\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Pre-Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = fileNameData(trainPartName)\n",
        "test_path = fileNameData(testPartName)\n",
        "val_path = fileNameData(validatePartName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.8061749571183533\n",
            "Accuracy on validation data = 0.5481120584652862\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_Cleared_scores = eval_DaNetQA(train_path, test_path, val_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_Cleared_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_Cleared_scores[\"val\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 0:\n",
        "    !pip install tensorflow\n",
        "    !pip install pandas\n",
        "    !pip install scipy\n",
        "    !pip install transformers\n",
        "    !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available: True\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import torch\n",
        "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm \n",
        "\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers.optimization import AdamW\n",
        "\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import seed_everything\n",
        "from utils import seed_worker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encode text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectAttentionMask(seq):\n",
        "    return [float(i > 0) for i in seq]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectTokenType(row, sepTokenIdx):\n",
        "    row = np.array(row)\n",
        "    mask = row == sepTokenIdx\n",
        "\n",
        "    whereMask = np.where(mask)[0]\n",
        "    idx = whereMask[0]\n",
        "    idx1 = whereMask[1]\n",
        "\n",
        "    token_type_row = np.zeros(row.shape[0], dtype=np.int32)\n",
        "    token_type_row[idx + 1:idx1 + 1] = 1\n",
        "    return token_type_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_text_pairs(tokenizer, sentences):\n",
        "    ENCODE_BATCH_SIZE = 20000\n",
        "    input_ids, attention_masks, token_type_ids = [], [], []\n",
        "    \n",
        "    clsTokenText = '[CLS]'\n",
        "    sepTokenText = '[SEP]'\n",
        "    sepTokenIdx = tokenizer.convert_tokens_to_ids(sepTokenText)\n",
        "\n",
        "    TEXT1_MAX = int(MAX_LEN*.75) # выделяет 75% размера слов для контекста\n",
        "    TEXT2_MAX = MAX_LEN - TEXT1_MAX # остальные слова это вопрос\n",
        "    for _, i in enumerate(range(0, len(sentences), ENCODE_BATCH_SIZE)):\n",
        "        # обрезаем предложение слов больше чем MAX_LEN\n",
        "        tokenized_texts = []\n",
        "        for sentence_context, sentence_question  in sentences[i:i + ENCODE_BATCH_SIZE]:\n",
        "            p1 = [clsTokenText] + tokenizer.tokenize(sentence_context)\n",
        "            p2 = [sepTokenText] + tokenizer.tokenize(sentence_question) + [sepTokenText]\n",
        "            final_tokens = p1[:TEXT1_MAX] + p2[:TEXT2_MAX]\n",
        "            tokenized_texts.append(final_tokens)\n",
        "\n",
        "        # токенизируем\n",
        "        b_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "        b_input_ids = pad_sequences(\n",
        "            b_input_ids, \n",
        "            maxlen=MAX_LEN, \n",
        "            dtype='long', \n",
        "            truncating='post', \n",
        "            padding='post')\n",
        "        input_ids.append(b_input_ids)\n",
        "\n",
        "        # маска внимания\n",
        "        b_attention_masks = [collectAttentionMask(seq) for seq in b_input_ids]\n",
        "        attention_masks.append(b_attention_masks)\n",
        "\n",
        "        # тип токена\n",
        "        b_token_type_ids = [collectTokenType(row, sepTokenIdx) for row in b_input_ids]\n",
        "        token_type_ids.append(b_token_type_ids)\n",
        "        \n",
        "    return np.vstack(input_ids), np.vstack(attention_masks), np.vstack(token_type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createDataLoader(set_ids, all_ids, input_ids, attention_masks, token_type_ids, all_labels, BATCH_SIZE_LOADER):\n",
        "    mask = np.array([sid in set_ids for sid in all_ids])\n",
        "\n",
        "    inputs = input_ids[mask]\n",
        "    masks = attention_masks[mask]\n",
        "    type_ids_dev = token_type_ids[mask]\n",
        "    labels = all_labels[mask]\n",
        "\n",
        "    t_inputs = torch.tensor(inputs)\n",
        "    t_masks = torch.tensor(masks)\n",
        "    t_type_ids_dev = torch.tensor(type_ids_dev)\n",
        "    t_labels = torch.tensor(labels)\n",
        "\n",
        "    t_dataset = TensorDataset(\n",
        "        t_inputs, \n",
        "        t_masks, \n",
        "        t_type_ids_dev, \n",
        "        t_labels)\n",
        "    t_sampler = SequentialSampler(t_dataset)\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset=t_dataset, \n",
        "        sampler=t_sampler, \n",
        "        batch_size=BATCH_SIZE_LOADER, \n",
        "        worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getYFromDataLoader(dl):\n",
        "    return np.argmax(dl.dataset.__dict__['tensors'][3], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluateModel(model, in_dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    accumulate_loss = 0\n",
        "    accumulate_step = 0\n",
        "    for _, batch in enumerate(in_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids = b_token_type_ids, \n",
        "                attention_mask = b_input_mask, \n",
        "                labels = b_labels)\n",
        "            loss, logits = outputs[:2]\n",
        "            \n",
        "            accumulate_loss += loss.item()\n",
        "            accumulate_step += 1\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            predictions.append(logits)\n",
        "\n",
        "    avg_loss = accumulate_loss / accumulate_step\n",
        "\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    y_true = getYFromDataLoader(in_dataloader)\n",
        "    score = accuracy_score(y_true, predictions)\n",
        "\n",
        "    return score, avg_loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train One Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainModelIteration(model, optimizer, scheduler, in_dataloader, MAX_GRAD_NORM, EPOCH_INDEX):\n",
        "    model.train() \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    predictions = []\n",
        "    accumulate_loss = 0\n",
        "    accumulate_step = 0\n",
        "\n",
        "    nStep = len(in_dataloader)\n",
        "    for step, batch in enumerate(in_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            token_type_ids = b_token_type_ids, \n",
        "            attention_mask = b_input_mask, \n",
        "            labels = b_labels\n",
        "            )\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epochLoss = loss.item()\n",
        "        accumulate_loss += epochLoss\n",
        "        accumulate_step += 1\n",
        "        \n",
        "        print(f\"Epoch {EPOCH_INDEX} Step {step} of {nStep}, loss = {epochLoss}\")\n",
        "\n",
        "    avg_loss = accumulate_loss / accumulate_step\n",
        "\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    y_true = getYFromDataLoader(in_dataloader)\n",
        "    score = accuracy_score(y_true, predictions)\n",
        "\n",
        "    return score, avg_loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1_id, text2_id, label_id, index_id = 'passage', 'question', 'label', 'idx'\n",
        "l2i = {False: 0, True:1}\n",
        "part2indices = {p:set() for p in parts}\n",
        "\n",
        "all_ids, all_sentences, all_labels = [], [], []\n",
        "idxMax = 0\n",
        "for p in parts:\n",
        "    df = pd.read_json(fileNameData(p), lines=True)\n",
        "    ids = idxMax + df[index_id].to_numpy()\n",
        "    all_ids.extend(ids)\n",
        "    idxMax = np.max(all_ids)\n",
        "    \n",
        "    part2indices[p] = ids\n",
        "    all_labels.extend(df[label_id].to_numpy())\n",
        "    all_sentences.extend(\n",
        "        np.array(\n",
        "            np.column_stack([df[text1_id].to_numpy(), \n",
        "            df[text2_id].to_numpy()])\n",
        "        ).tolist())\n",
        "\n",
        "all_ids = np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(total) 2570\n",
            "len(l2i) 2\n"
          ]
        }
      ],
      "source": [
        "print ('len(total)', len(all_sentences))\n",
        "i2l = {l2i[l]:l for l in l2i}\n",
        "print ( 'len(l2i)', len(l2i) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_indices = np.array([l2i[l] for l in all_labels])\n",
        "labels = np.zeros((len(all_ids), len(l2i)))\n",
        "for _, i in enumerate(label_indices):\n",
        "    labels[_, i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test =  label_indices[np.array([sid in part2indices['val_v1'] for sid in all_ids])]\n",
        "y_train =  label_indices[np.array([sid in part2indices['train_v1'] for sid in all_ids])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model RuBert-Cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __POOL_MODEL:\n",
        "    from utils import PoolBertForSequenceClassification as BertModel\n",
        "else:\n",
        "    from transformers import BertForSequenceClassification as BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 128\n",
        "MAX_LEN = 256\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_LOADER = 8\n",
        "EPOCHS_LIMIT = 25\n",
        "LEARNING_RATE = 3e-5\n",
        "MAX_GRAD_NORM = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget \"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
        "!tar -xvzf rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!rm rubert_cased_L-12_H-768_A-12_pt.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\rubert_cased_L-12_H-768_A-12_pt/\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join(base_path, 'rubert_cased_L-12_H-768_A-12_pt/')\n",
        "print(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### One-Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path = os.path.join(base_path, model_path),\n",
        "    do_lower_case=True,\n",
        "    max_length=MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, attention_masks, token_type_ids = encode_text_pairs(tokenizer, all_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepeare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "train_dataloader = createDataLoader(part2indices['train_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "validate_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: torch.Size([1750, 256])\n",
            "Validation set shape: torch.Size([822, 256])\n",
            "Validation set shape: torch.Size([822, 256])\n"
          ]
        }
      ],
      "source": [
        "print (f'Training set shape: {train_dataloader.dataset.__dict__[\"tensors\"][0].shape}')\n",
        "print (f'Validation set shape: {test_dataloader.dataset.__dict__[\"tensors\"][0].shape}')\n",
        "print (f'Validation set shape: {validate_dataloader.dataset.__dict__[\"tensors\"][0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-Trained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "config_path = os.path.join(base_path, model_path, 'bert_config.json')\n",
        "conf = BertConfig.from_json_file(config_path)\n",
        "conf.num_labels = len(l2i)\n",
        "\n",
        "output_model_file = os.path.join(base_path, model_path, 'pytorch_model.bin')\n",
        "\n",
        "model = BertModel(conf)\n",
        "\n",
        "model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### Limit learning for BERT layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizer & Scheduler\n",
        "Задаем гиперпараметры для цикла обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "219\n"
          ]
        }
      ],
      "source": [
        "nStep = len(train_dataloader)\n",
        "print(nStep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=LEARNING_RATE, \n",
        "    correct_bias=False)\n",
        "    \n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=LEARNING_RATE, \n",
        "    steps_per_epoch=nStep, \n",
        "    epochs=EPOCHS_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_train_predict = []\n",
        "all_test_predict = []\n",
        "\n",
        "all_avg_train_loss = []\n",
        "all_avg_test_loss = []\n",
        "\n",
        "all_train_acc = []\n",
        "all_test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for iEpoch in range(3):\n",
        "\n",
        "    # train\n",
        "    train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n",
        "        model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n",
        "    \n",
        "    all_train_acc.append(train_score)\n",
        "    all_avg_train_loss.append(avg_train_loss)\n",
        "    all_train_predict.append(y_train_prediction)\n",
        "\n",
        "    ### evaluate\n",
        "    test_score, avg_test_loss, y_test_prediction = evaluateModel(\n",
        "        model, test_dataloader, device)\n",
        "\n",
        "    all_test_acc.append(test_score)\n",
        "    all_avg_test_loss.append(avg_test_loss)\n",
        "    all_test_predict.append(y_test_prediction)\n",
        "\n",
        "    # log epoch\n",
        "    print(f'Epoch {iEpoch} average train_loss: {avg_train_loss:.6f} test_loss: {avg_test_loss:.6f} test_score {test_score:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Graphics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x20812eeae80>]"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(all_train_acc)\n",
        "plt.plot(all_test_acc)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(all_avg_train_loss)\n",
        "plt.plot(all_avg_test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model RuBert-Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'ruRoberta-large'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/sberbank-ai/ruRoberta-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/ruRoberta-large\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"sberbank-ai/ruRoberta-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\ruRoberta-large/\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join(base_path, 'ruRoberta-large/')\n",
        "print(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'l2i' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7608\\3824650115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput_model_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pytorch_model.bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'l2i' is not defined"
          ]
        }
      ],
      "source": [
        "config_path = os.path.join(base_path, model_path, 'config.json')\n",
        "conf = BertConfig.from_json_file(config_path)\n",
        "conf.num_labels = len(l2i)\n",
        "\n",
        "output_model_file = os.path.join(base_path, model_path, 'pytorch_model.bin')\n",
        "\n",
        "model = BertModel(conf)\n",
        "\n",
        "model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaForMaskedLM(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (18): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (19): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (20): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (21): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (22): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (23): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): RobertaLMHead(\n",
            "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "sample() missing 1 required positional argument: 'input_ids'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7608\\1709407715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m: sample() missing 1 required positional argument: 'input_ids'"
          ]
        }
      ],
      "source": [
        "model.sample()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch_seq2seq_with_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "846dd53c5a100503afcb3f5301bb10f61481596a80ae839ecd432be859b5d4d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
