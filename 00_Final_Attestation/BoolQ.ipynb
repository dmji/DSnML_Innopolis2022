{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "__COLAB_ACTIVE = False\n",
        "__POOL_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Проект 3. Решить задачу DaNetQA / BoolQ\n",
        "\n",
        "Можно решить как задачу для русского, так и для английского.\n",
        "\n",
        "Либо провести эксперименты с многоязычной моделью\n",
        "\n",
        "https://russiansuperglue.com/ru/tasks/task_info/DaNetQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Описание\n",
        "Причинно-следственная связь, логический вывод, Natural Language Inference\n",
        "\n",
        "DaNetQA - это набор да/нет вопросов с ответами и фрагментом текста, содержащим ответ. Все вопросы были написаны авторами без каких-либо искусственных ограничений.\n",
        "\n",
        "Каждый пример представляет собой триплет (вопрос, фрагмент текста, ответ) с заголовком страницы в качестве необязательного дополнительного контекста.\n",
        "\n",
        "Настройка классификации текстовых пар аналогична существующим задачам логического вывода (NLI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тип задачи\n",
        "Логика, Commonsense, Знания о мире. Бинарная классификация: true/false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root path: 'd:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation'\n",
            "Dataset path: d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\DaNetQA\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "\n",
        "base_path = os.path.abspath('')\n",
        "if __COLAB_ACTIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_path = os.path.join(base_path, 'drive/MyDrive/DSnML_Innopolis2022')\n",
        "\n",
        "print(f\"Root path: '{base_path}'\")\n",
        "\n",
        "trainPartNameRaw = 'raw_train'\n",
        "testPartNameRaw = 'raw_val'\n",
        "validatePartNameRaw = 'raw_test'\n",
        "\n",
        "trainPartName = 'train_v1'\n",
        "testPartName = 'val_v1'\n",
        "validatePartName = 'test_v1'\n",
        "parts = [trainPartName, testPartName]\n",
        "data_path = os.path.join(base_path, 'DaNetQA')\n",
        "print(f\"Dataset path: {data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fileNameData(s):\n",
        "    return f\"{os.path.join(data_path, s)}.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\leysh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadJSONL(path, name):\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    print(name)\n",
        "    display(df.head())\n",
        "    if (df.columns.values == 'label').any():\n",
        "        s = np.unique(df['label'].to_numpy(), return_counts=True)[1]\n",
        "        print(f\"True answer: {s[1]}\")\n",
        "        print(f\"False answer: {s[0]}\")\n",
        "        print(\"\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>«Вы́ставочный центр» — станция Московского мон...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>Вы́ставка достиже́ний наро́дного хозя́йства  ,...</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Был ли джиган в black star?</td>\n",
              "      <td>Вместе с этим треком они выступили на церемони...</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Xiaomi конкурент apple?</td>\n",
              "      <td>Xiaomi — китайская компания, основанная в 2010...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли автомат калашникова в вов?</td>\n",
              "      <td>Отметив некоторые недостатки и в целом удачную...</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            question  \\\n",
              "0      Вднх - это выставочный центр?   \n",
              "1      Вднх - это выставочный центр?   \n",
              "2        Был ли джиган в black star?   \n",
              "3            Xiaomi конкурент apple?   \n",
              "4  Был ли автомат калашникова в вов?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  «Вы́ставочный центр» — станция Московского мон...   True    0  \n",
              "1  Вы́ставка достиже́ний наро́дного хозя́йства  ,...   True    1  \n",
              "2  Вместе с этим треком они выступили на церемони...   True    2  \n",
              "3  Xiaomi — китайская компания, основанная в 2010...   True    3  \n",
              "4  Отметив некоторые недостатки и в целом удачную...  False    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 1061\n",
            "False answer: 688\n",
            "\n",
            "Test set:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>вод марс</td>\n",
              "      <td>гидросфер марс эт совокупн водн запас планет м...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>состо англ евросоюз</td>\n",
              "      <td>полноч 31 январ 1 феврал 2020 год центральноев...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>деиствительн ссср адвокат</td>\n",
              "      <td>сем львович ар советск россииск юрист крупнеиш...</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>чум оран</td>\n",
              "      <td>чум эт абсурд осмыслива форм существован зла э...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>кетчуп читос</td>\n",
              "      <td>текущ каталог продукц размещ са производител к...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    question  \\\n",
              "0                   вод марс   \n",
              "1        состо англ евросоюз   \n",
              "2  деиствительн ссср адвокат   \n",
              "3                   чум оран   \n",
              "4               кетчуп читос   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  гидросфер марс эт совокупн водн запас планет м...   True    0  \n",
              "1  полноч 31 январ 1 феврал 2020 год центральноев...  False    1  \n",
              "2  сем львович ар советск россииск юрист крупнеиш...  False    2  \n",
              "3  чум эт абсурд осмыслива форм существован зла э...   True    3  \n",
              "4  текущ каталог продукц размещ са производител к...   True    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 412\n",
            "False answer: 409\n",
            "\n",
            "Validation set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полезна ли ртуть с градусника?</td>\n",
              "      <td>Отравления ртутью  — расстройства здоровья, св...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Являются ли сапрофаги хищниками?</td>\n",
              "      <td>Фауна лесных почв — совокупность видов животны...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Водятся ли в индии крокодилы?</td>\n",
              "      <td>Болотный крокодил, или магер  — пресмыкающееся...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Есть ли в батате крахмал?</td>\n",
              "      <td>Клубневидно вздутые корни  весят до 15 кг, сод...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли человек в железной маске?</td>\n",
              "      <td>Остров Сент-Маргерит  — крупнейший из Лерински...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           question  \\\n",
              "0    Полезна ли ртуть с градусника?   \n",
              "1  Являются ли сапрофаги хищниками?   \n",
              "2     Водятся ли в индии крокодилы?   \n",
              "3         Есть ли в батате крахмал?   \n",
              "4  Был ли человек в железной маске?   \n",
              "\n",
              "                                             passage  idx  \n",
              "0  Отравления ртутью  — расстройства здоровья, св...    0  \n",
              "1  Фауна лесных почв — совокупность видов животны...    1  \n",
              "2  Болотный крокодил, или магер  — пресмыкающееся...    2  \n",
              "3  Клубневидно вздутые корни  весят до 15 кг, сод...    3  \n",
              "4  Остров Сент-Маргерит  — крупнейший из Лерински...    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_train = loadJSONL(fileNameData(trainPartNameRaw), \"Train set\")\n",
        "df_test = loadJSONL(fileNameData(testPartNameRaw), \"Test set:\")\n",
        "df_validation = loadJSONL(fileNameData(validatePartNameRaw), \"Validation set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Очистка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataCleaner:\n",
        "    def __init__(self) -> None:\n",
        "        self.flag_verbose = True\n",
        "\n",
        "        self.stop_words = stopwords.words('russian')\n",
        "        self.stemmer = SnowballStemmer('russian')\n",
        "\n",
        "        self.count_removed_symbols = dict()\n",
        "        self.count_removed_words = dict()\n",
        "\n",
        "        self.count_replaced_symbols = dict()\n",
        "        self.dict_replaced_symbols = dict()\n",
        "\n",
        "        self.count_replaced_words = dict()\n",
        "        self.dict_replaced_words = dict()\n",
        "\n",
        "        self.char_to_remove = ['«', '»', '—', ',', '.', '-', '/', ':', '!', \"?\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"=\", \"|\", \"\\\\\", \">\", \"<\"]\n",
        "        self.char_to_replace = [['ё', 'е']]\n",
        "\n",
        "    # функция подсчета количества измененных слов\n",
        "    def addReplacedWord(self, s_from, s_to = ' '):\n",
        "        if not self.count_replaced_words.keys().__contains__(s_from):\n",
        "            self.count_replaced_words[s_from] = 0\n",
        "        self.count_replaced_words[s_from] += 1\n",
        "        self.dict_replaced_words[s_from] = s_to\n",
        "\n",
        "    # функция подсчета количества удаленных слов\n",
        "    def addRemovedWord(self, w):\n",
        "        if w == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(w):\n",
        "                self.count_removed_symbols[w] = 0\n",
        "            self.count_removed_symbols[w] += 1\n",
        "\n",
        "    # функция подсчета количества удаленных символов\n",
        "    def addReplacedSymbol(self, s_from, s_to = ' '):\n",
        "        if s_to == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(s_from):\n",
        "                self.count_removed_symbols[s_from] = 0\n",
        "            self.count_removed_symbols[s_from] += 1\n",
        "        else:\n",
        "            if not self.count_replaced_symbols.keys().__contains__(s_from):\n",
        "                self.count_replaced_symbols[s_from] = 0\n",
        "            self.count_replaced_symbols[s_from] += 1\n",
        "            self.dict_replaced_symbols[s_from] = s_to\n",
        "\n",
        "    # удаление знаков ударения и прочих символов unicode\n",
        "    def unicodeToAscii(self, s):\n",
        "        tmp = []\n",
        "        for c in unicodedata.normalize('NFD', s):\n",
        "            if unicodedata.category(c) != 'Mn':\n",
        "                tmp.append(c)\n",
        "            else:\n",
        "                self.addReplacedSymbol(c)\n",
        "        return ''.join(tmp)\n",
        "\n",
        "    # если нужно удалить, то заменяем на пробел чтоб не потерят разделения слов\n",
        "    def replaceChar(self, s):\n",
        "        tmp = []\n",
        "        for i, c in enumerate(s):\n",
        "            if self.char_to_remove.__contains__(c):\n",
        "                self.addReplacedSymbol(c, s[i])\n",
        "                tmp.append(' ')\n",
        "            else:\n",
        "                tmp.append(c)\n",
        "        s = \"\".join(tmp)\n",
        "\n",
        "        for s_from, s_to in self.char_to_replace:\n",
        "            if c == s_from:\n",
        "                s[i] = s_to\n",
        "                self.addReplacedSymbol(s_from, s_to)\n",
        "        return s\n",
        "\n",
        "    # удаляем лишние пробелы\n",
        "    def trimSpaces(self, s):\n",
        "        while s.__contains__('  '):\n",
        "            s = s.replace('  ', ' ')\n",
        "        s = s.strip()\n",
        "        return s\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def removeStopWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            if word not in self.stop_words:\n",
        "                tmp.append(word)\n",
        "            else:\n",
        "                self.addRemovedWord(word)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def StemmWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            wordStemmed = self.stemmer.stem(word)\n",
        "            tmp.append(wordStemmed)\n",
        "            if word != wordStemmed:\n",
        "                self.addReplacedWord(word, wordStemmed)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    def clean(self, df, column):\n",
        "        for i in range(len(df)):\n",
        "            df[column][i] = self.unicodeToAscii(df[column][i])\n",
        "            df[column][i] = df[column][i].lower()\n",
        "            df[column][i] = self.replaceChar(df[column][i])\n",
        "            df[column][i] = self.removeStopWords(df[column][i])\n",
        "            df[column][i] = self.StemmWords(df[column][i])\n",
        "            df[column][i] = self.trimSpaces(df[column][i])\n",
        "        return df\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def print(self, vals):\n",
        "        if self.flag_verbose == True:\n",
        "            print(vals)\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def display(self, vals):\n",
        "            if self.flag_verbose == True:\n",
        "                display(vals)\n",
        "\n",
        "    # сбор лога в dataframe, опциональный вывод на экран \n",
        "    def summary(self, verbose = True):\n",
        "        self.flag_verbose = verbose\n",
        "        dfs = []\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Chars        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_symbols:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_words:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Words', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Replaced Chars       ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol_from\", \"symbol_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_symbols:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_symbols[c], self.count_replaced_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Replaced Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Stemmed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word_from\", \"word_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_words:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_words[c], self.count_replaced_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Stemmed Words', dfRemoved])\n",
        "\n",
        "        return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_11648\\3611868607.py:102: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.unicodeToAscii(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_11648\\3611868607.py:103: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = df[column][i].lower()\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_11648\\3611868607.py:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.replaceChar(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_11648\\3611868607.py:105: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.removeStopWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_11648\\3611868607.py:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.StemmWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_11648\\3611868607.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.trimSpaces(df[column][i])\n"
          ]
        }
      ],
      "source": [
        "t = DataCleaner()\n",
        "df_train = t.clean(df_train, 'passage')\n",
        "df_test = t.clean(df_test, 'passage')\n",
        "df_validation = t.clean(df_validation, 'passage')\n",
        "df_train = t.clean(df_train, 'question')\n",
        "df_test = t.clean(df_test, 'question')\n",
        "df_validation = t.clean(df_validation, 'question')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Chars        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>́</td>\n",
              "      <td>1131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>̆</td>\n",
              "      <td>20853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>̈</td>\n",
              "      <td>4020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>̔</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>̀</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>̣</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>̌</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>̥</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>̂</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>̊</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>̓</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>͂</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ُ</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>̄</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>̃</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol count_removed\n",
              "0       ́          1131\n",
              "1       ̆         20853\n",
              "2       ̈          4020\n",
              "3       ̔             5\n",
              "4       ̀             2\n",
              "5       ̣             6\n",
              "6       ̌             1\n",
              "7       ̥             1\n",
              "8       ̂             2\n",
              "9       ̊             2\n",
              "10      ̓             3\n",
              "11      ͂             1\n",
              "12      ُ             2\n",
              "13      ̄             1\n",
              "14      ̃             1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [word, count_removed]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Replaced Chars       ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol_from</th>\n",
              "      <th>symbol_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>«</td>\n",
              "      <td>«</td>\n",
              "      <td>1593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>»</td>\n",
              "      <td>»</td>\n",
              "      <td>1585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>—</td>\n",
              "      <td>—</td>\n",
              "      <td>3296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>14887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>3113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>21500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>)</td>\n",
              "      <td>)</td>\n",
              "      <td>1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>:</td>\n",
              "      <td>:</td>\n",
              "      <td>1029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>%</td>\n",
              "      <td>%</td>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/</td>\n",
              "      <td>/</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>|</td>\n",
              "      <td>|</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>]</td>\n",
              "      <td>]</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[</td>\n",
              "      <td>[</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>!</td>\n",
              "      <td>!</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>&lt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>2605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>&amp;</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>=</td>\n",
              "      <td>=</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>$</td>\n",
              "      <td>$</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>#</td>\n",
              "      <td>#</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(</td>\n",
              "      <td>(</td>\n",
              "      <td>1334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>{</td>\n",
              "      <td>{</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>}</td>\n",
              "      <td>}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol_from symbol_to count_replaced\n",
              "0            «         «           1593\n",
              "1            »         »           1585\n",
              "2            —         —           3296\n",
              "3            .         .          14887\n",
              "4            -         -           3113\n",
              "5            ,         ,          21500\n",
              "6            )         )           1400\n",
              "7            :         :           1029\n",
              "8            %         %            242\n",
              "9            /         /            156\n",
              "10           |         |              3\n",
              "11           ]         ]            131\n",
              "12           [         [            128\n",
              "13           !         !             48\n",
              "14           *         *             13\n",
              "15           <         <              5\n",
              "16           >         >             18\n",
              "17           ?         ?           2605\n",
              "18           &         &              5\n",
              "19           =         =              8\n",
              "20           $         $              7\n",
              "21           #         #              5\n",
              "22           (         (           1334\n",
              "23           {         {              1\n",
              "24           }         }              1\n",
              "25           ^         ^              1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Stemmed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_from</th>\n",
              "      <th>word_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>выставочныи</td>\n",
              "      <td>выставочны</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>станция</td>\n",
              "      <td>станц</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>московского</td>\n",
              "      <td>московск</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>монорельса</td>\n",
              "      <td>монорельс</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>расположена</td>\n",
              "      <td>располож</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47258</th>\n",
              "      <td>зарплата</td>\n",
              "      <td>зарплат</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47259</th>\n",
              "      <td>себестоимость</td>\n",
              "      <td>себестоим</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47260</th>\n",
              "      <td>ювелирная</td>\n",
              "      <td>ювелирн</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47261</th>\n",
              "      <td>новорожденные</td>\n",
              "      <td>новорожден</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47262</th>\n",
              "      <td>бодрит</td>\n",
              "      <td>бодр</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47263 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word_from     word_to count_replaced\n",
              "0        выставочныи  выставочны              7\n",
              "1            станция       станц              7\n",
              "2        московского    московск             18\n",
              "3         монорельса   монорельс              2\n",
              "4        расположена    располож             15\n",
              "...              ...         ...            ...\n",
              "47258       зарплата     зарплат              1\n",
              "47259  себестоимость   себестоим              1\n",
              "47260      ювелирная     ювелирн              1\n",
              "47261  новорожденные  новорожден              1\n",
              "47262         бодрит        бодр              1\n",
              "\n",
              "[47263 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dfs = t.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'выставк достижен народн хозяиств 1959 1991 год выставк достижен народн хозяиств ссср 1992 2014 год всероссииск выставочны центр выставочны комплекс останкинск раион север восточн административн округ город москв второ величин выставочны комплекс город вход 50 крупнеиш выставочн центр мир ежегодн вднх посеща 30 млн гост 1 август 2019 год выставк отпразднова 80 летн юбил территориальн вднх объедин парк останкин главн ботаническ сад общ площад составля 700 га 240 2 га площад вднх 75 6 га площад парк останкин 361 га площад гбс 9 5 га музеин выставочны центр рабоч колхозниц площад арко главн вход территор выставк располож множеств шедевр архитектур 49 объект вднх призна памятник культурн наслед'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.passage[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.to_json(fileNameData(trainPartName), force_ascii=False, lines=True, orient='records')\n",
        "df_test.to_json(fileNameData(testPartNameRaw), force_ascii=False, lines=True, orient='records')\n",
        "df_validation.to_json(fileNameData(validatePartName), force_ascii=False, lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF + LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import codecs\n",
        "import json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_feature_DaNetQA(row):\n",
        "    res = str(row[\"question\"]).strip()\n",
        "    label = row.get(\"label\")\n",
        "    return res, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_features_DaNetQA(path, vect):\n",
        "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
        "        lines = reader.read().split(\"\\n\")\n",
        "        lines = list(map(json.loads, filter(None, lines)))\n",
        "    res = list(map(build_feature_DaNetQA, lines))\n",
        "    texts = list(map(lambda x: x[0], res))\n",
        "    labels = list(map(lambda x: x[1], res))\n",
        "    ids = [x[\"idx\"] for x in lines]\n",
        "    return (vect.transform(texts), labels), ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_DaNetQA(train, labels):\n",
        "    clf = LogisticRegression()\n",
        "    return clf.fit(train, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_DaNetQA(train_path, val_path, test_path, vect):\n",
        "    train, _ = build_features_DaNetQA(train_path, vect)\n",
        "    val, _ = build_features_DaNetQA(val_path, vect)\n",
        "    test, ids = build_features_DaNetQA(test_path, vect)\n",
        "    clf = fit_DaNetQA(*train)\n",
        "    try:\n",
        "        test_score = clf.score(*test)\n",
        "    except ValueError:\n",
        "        test_score = None\n",
        "    test_pred = clf.predict(test[0])\n",
        "    return clf, {\n",
        "        \"train\": clf.score(*train),\n",
        "        \"val\": clf.score(*val),\n",
        "        \"test\": test_score,\n",
        "        \"test_pred\": [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(ids, test_pred)]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pre-Trained TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://russiansuperglue.com/tasks/tf_idf\n",
        "!unzip tf_idf_baseline.zip\n",
        "!rm tf_idf_baseline.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vect = joblib.load(\"tfidf.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Score Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = fileNameData(trainPartNameRaw)\n",
        "test_path = fileNameData(testPartNameRaw)\n",
        "val_path = fileNameData(validatePartNameRaw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.8010291595197255\n",
            "Accuracy on validation data = 0.5091352009744214\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_scores = eval_DaNetQA(train_path, test_path, val_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_scores[\"val\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Pre-Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = fileNameData(trainPartName)\n",
        "test_path = fileNameData(testPartName)\n",
        "val_path = fileNameData(validatePartName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.7004002287021155\n",
            "Accuracy on validation data = 0.5249695493300852\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_Cleared_scores = eval_DaNetQA(train_path, test_path, val_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_Cleared_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_Cleared_scores[\"val\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 0:\n",
        "    !pip install tensorflow\n",
        "    !pip install pandas\n",
        "    !pip install scipy\n",
        "    !pip install transformers\n",
        "    !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available: True\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import torch\n",
        "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm \n",
        "\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers.optimization import AdamW\n",
        "\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import seed_everything\n",
        "from utils import seed_worker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encode text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectAttentionMask(seq):\n",
        "    return [float(i > 0) for i in seq]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectTokenType(row, sepTokenIdx):\n",
        "    row = np.array(row)\n",
        "    mask = row == sepTokenIdx\n",
        "\n",
        "    whereMask = np.where(mask)[0]\n",
        "    idx = whereMask[0]\n",
        "    idx1 = whereMask[1]\n",
        "\n",
        "    token_type_row = np.zeros(row.shape[0], dtype=np.int32)\n",
        "    token_type_row[idx + 1:idx1 + 1] = 1\n",
        "    return token_type_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_text_pairs(tokenizer, sentences):\n",
        "    ENCODE_BATCH_SIZE = 20000\n",
        "    input_ids, attention_masks, token_type_ids = [], [], []\n",
        "    \n",
        "    clsTokenText = '[CLS]'\n",
        "    sepTokenText = '[SEP]'\n",
        "    sepTokenIdx = tokenizer.convert_tokens_to_ids(sepTokenText)\n",
        "\n",
        "    TEXT1_MAX = int(MAX_LEN*.75) # выделяет 75% размера слов для контекста\n",
        "    TEXT2_MAX = MAX_LEN - TEXT1_MAX # остальные слова это вопрос\n",
        "    for _, i in enumerate(range(0, len(sentences), ENCODE_BATCH_SIZE)):\n",
        "        # обрезаем предложение слов больше чем MAX_LEN\n",
        "        tokenized_texts = []\n",
        "        for sentence_context, sentence_question  in sentences[i:i + ENCODE_BATCH_SIZE]:\n",
        "            p1 = [clsTokenText] + tokenizer.tokenize(sentence_context)\n",
        "            p2 = [sepTokenText] + tokenizer.tokenize(sentence_question) + [sepTokenText]\n",
        "            final_tokens = p1[:TEXT1_MAX] + p2[:TEXT2_MAX]\n",
        "            tokenized_texts.append(final_tokens)\n",
        "\n",
        "        # токенизируем\n",
        "        b_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "        b_input_ids = pad_sequences(\n",
        "            b_input_ids, \n",
        "            maxlen=MAX_LEN, \n",
        "            dtype='long', \n",
        "            truncating='post', \n",
        "            padding='post')\n",
        "        input_ids.append(b_input_ids)\n",
        "\n",
        "        # маска внимания\n",
        "        b_attention_masks = [collectAttentionMask(seq) for seq in b_input_ids]\n",
        "        attention_masks.append(b_attention_masks)\n",
        "\n",
        "        # тип токена\n",
        "        b_token_type_ids = [collectTokenType(row, sepTokenIdx) for row in b_input_ids]\n",
        "        token_type_ids.append(b_token_type_ids)\n",
        "        \n",
        "    return np.vstack(input_ids), np.vstack(attention_masks), np.vstack(token_type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createDataLoader(set_ids, all_ids, input_ids, attention_masks, token_type_ids, all_labels, BATCH_SIZE_LOADER):\n",
        "    mask = np.array([sid in set_ids for sid in all_ids])\n",
        "\n",
        "    inputs = input_ids[mask]\n",
        "    masks = attention_masks[mask]\n",
        "    type_ids_dev = token_type_ids[mask]\n",
        "    labels = all_labels[mask]\n",
        "\n",
        "    t_inputs = torch.tensor(inputs)\n",
        "    t_masks = torch.tensor(masks)\n",
        "    t_type_ids_dev = torch.tensor(type_ids_dev)\n",
        "    t_labels = torch.tensor(labels)\n",
        "\n",
        "    t_dataset = TensorDataset(\n",
        "        t_inputs, \n",
        "        t_masks, \n",
        "        t_type_ids_dev, \n",
        "        t_labels)\n",
        "    t_sampler = SequentialSampler(t_dataset)\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset=t_dataset, \n",
        "        sampler=t_sampler, \n",
        "        batch_size=BATCH_SIZE_LOADER, \n",
        "        worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getYFromDataLoader(dl):\n",
        "    return np.argmax(dl.dataset.__dict__['tensors'][3], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluateModel(model, in_dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    accumulate_loss = 0\n",
        "    accumulate_step = 0\n",
        "    for _, batch in enumerate(in_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids = b_token_type_ids, \n",
        "                attention_mask = b_input_mask, \n",
        "                labels = b_labels)\n",
        "            loss, logits = outputs[:2]\n",
        "            \n",
        "            accumulate_loss += loss.item()\n",
        "            accumulate_step += 1\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            predictions.append(logits)\n",
        "\n",
        "    avg_loss = accumulate_loss / accumulate_step\n",
        "\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    y_true = getYFromDataLoader(in_dataloader)\n",
        "    score = accuracy_score(y_true, predictions)\n",
        "\n",
        "    return score, avg_loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train One Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainModelIteration(model, optimizer, scheduler, in_dataloader, MAX_GRAD_NORM, EPOCH_INDEX):\n",
        "    model.train() \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    predictions = []\n",
        "    accumulate_loss = 0\n",
        "    accumulate_step = 0\n",
        "\n",
        "    nStep = len(in_dataloader)\n",
        "    for step, batch in enumerate(in_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            token_type_ids = b_token_type_ids, \n",
        "            attention_mask = b_input_mask, \n",
        "            labels = b_labels\n",
        "            )\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epochLoss = loss.item()\n",
        "        accumulate_loss += epochLoss\n",
        "        accumulate_step += 1\n",
        "        \n",
        "        print(f\"Epoch {EPOCH_INDEX} Step {step} of {nStep}, loss = {epochLoss}\")\n",
        "\n",
        "    avg_loss = accumulate_loss / accumulate_step\n",
        "\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    y_true = getYFromDataLoader(in_dataloader)\n",
        "    score = accuracy_score(y_true, predictions)\n",
        "\n",
        "    return score, avg_loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1_id, text2_id, label_id, index_id = 'passage', 'question', 'label', 'idx'\n",
        "l2i = {False: 0, True:1}\n",
        "part2indices = {p:set() for p in parts}\n",
        "\n",
        "all_ids, all_sentences, all_labels = [], [], []\n",
        "idxMax = 0\n",
        "for p in parts:\n",
        "    df = pd.read_json(fileNameData(p), lines=True)\n",
        "    ids = idxMax + df[index_id].to_numpy()\n",
        "    all_ids.extend(ids)\n",
        "    idxMax = np.max(all_ids)\n",
        "    \n",
        "    part2indices[p] = ids\n",
        "    all_labels.extend(df[label_id].to_numpy())\n",
        "    all_sentences.extend(\n",
        "        np.array(\n",
        "            np.column_stack([df[text1_id].to_numpy(), \n",
        "            df[text2_id].to_numpy()])\n",
        "        ).tolist())\n",
        "\n",
        "all_ids = np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(total) 2570\n",
            "len(l2i) 2\n"
          ]
        }
      ],
      "source": [
        "print ('len(total)', len(all_sentences))\n",
        "i2l = {l2i[l]:l for l in l2i}\n",
        "print ( 'len(l2i)', len(l2i) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_indices = np.array([l2i[l] for l in all_labels])\n",
        "labels = np.zeros((len(all_ids), len(l2i)))\n",
        "for _, i in enumerate(label_indices):\n",
        "    labels[_, i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test =  label_indices[np.array([sid in part2indices['val_v1'] for sid in all_ids])]\n",
        "y_train =  label_indices[np.array([sid in part2indices['train_v1'] for sid in all_ids])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model RuBert-Cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __POOL_MODEL:\n",
        "    from utils import PoolBertForSequenceClassification as BertModel\n",
        "else:\n",
        "    from transformers import BertForSequenceClassification as BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 128\n",
        "MAX_LEN = 256\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_LOADER = 8\n",
        "EPOCHS_LIMIT = 25\n",
        "LEARNING_RATE = 3e-5\n",
        "MAX_GRAD_NORM = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open 'rubert_cased_L-12_H-768_A-12_pt.tar.gz'\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget \"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
        "!tar -xvzf rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!rm rubert_cased_L-12_H-768_A-12_pt.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\rubert_cased_L-12_H-768_A-12_pt/\n"
          ]
        }
      ],
      "source": [
        "model_path = os.path.join(base_path, 'rubert_cased_L-12_H-768_A-12_pt/')\n",
        "print(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### One-Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path = os.path.join(base_path, model_path),\n",
        "    do_lower_case=True,\n",
        "    max_length=MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, attention_masks, token_type_ids = encode_text_pairs(tokenizer, all_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepeare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "train_dataloader = createDataLoader(part2indices['train_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "validate_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: torch.Size([1750, 256])\n",
            "Validation set shape: torch.Size([822, 256])\n",
            "Validation set shape: torch.Size([822, 256])\n"
          ]
        }
      ],
      "source": [
        "print (f'Training set shape: {train_dataloader.dataset.__dict__[\"tensors\"][0].shape}')\n",
        "print (f'Validation set shape: {test_dataloader.dataset.__dict__[\"tensors\"][0].shape}')\n",
        "print (f'Validation set shape: {validate_dataloader.dataset.__dict__[\"tensors\"][0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-Trained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "config_path = os.path.join(base_path, model_path, 'bert_config.json')\n",
        "conf = BertConfig.from_json_file(config_path)\n",
        "conf.num_labels = len(l2i)\n",
        "\n",
        "output_model_file = os.path.join(base_path, model_path, 'pytorch_model.bin')\n",
        "\n",
        "model = BertModel(conf)\n",
        "\n",
        "model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### Limit learning for BERT layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizer & Scheduler\n",
        "Задаем гиперпараметры для цикла обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=LEARNING_RATE, \n",
        "    correct_bias=False)\n",
        "    \n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=LEARNING_RATE, \n",
        "    steps_per_epoch=len(train_dataloader), \n",
        "    epochs=EPOCHS_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert_train_predict = []\n",
        "bert_test_predict = []\n",
        "\n",
        "bert_avg_train_loss = []\n",
        "bert_avg_test_loss = []\n",
        "\n",
        "bert_train_acc = []\n",
        "bert_test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Step 0 of 219, loss = 1.1648840170819312\n",
            "Epoch 0 Step 1 of 219, loss = 0.9742748062126338\n",
            "Epoch 0 Step 2 of 219, loss = 1.069280429277569\n",
            "Epoch 0 Step 3 of 219, loss = 1.0660201665014029\n",
            "Epoch 0 Step 4 of 219, loss = 1.0121560343541205\n",
            "Epoch 0 Step 5 of 219, loss = 0.878244386985898\n",
            "Epoch 0 Step 6 of 219, loss = 0.8139510974287987\n",
            "Epoch 0 Step 7 of 219, loss = 0.7129489202052355\n",
            "Epoch 0 Step 8 of 219, loss = 0.7512796418741345\n",
            "Epoch 0 Step 9 of 219, loss = 0.587627271655947\n",
            "Epoch 0 Step 10 of 219, loss = 0.5304310782812536\n",
            "Epoch 0 Step 11 of 219, loss = 0.4581133765168488\n",
            "Epoch 0 Step 12 of 219, loss = 0.48028083937242627\n",
            "Epoch 0 Step 13 of 219, loss = 0.9146116706542671\n",
            "Epoch 0 Step 14 of 219, loss = 0.4167641280218959\n",
            "Epoch 0 Step 15 of 219, loss = 0.4478187318891287\n",
            "Epoch 0 Step 16 of 219, loss = 0.4130536369048059\n",
            "Epoch 0 Step 17 of 219, loss = 1.9040669035166502\n",
            "Epoch 0 Step 18 of 219, loss = 0.8152113617397845\n",
            "Epoch 0 Step 19 of 219, loss = 1.2541905138641596\n",
            "Epoch 0 Step 20 of 219, loss = 1.3174855469260365\n",
            "Epoch 0 Step 21 of 219, loss = 1.0602633678354323\n",
            "Epoch 0 Step 22 of 219, loss = 1.3045746069401503\n",
            "Epoch 0 Step 23 of 219, loss = 1.710839090636\n",
            "Epoch 0 Step 24 of 219, loss = 1.4279132755473256\n",
            "Epoch 0 Step 25 of 219, loss = 0.616090040653944\n",
            "Epoch 0 Step 26 of 219, loss = 0.4623240097425878\n",
            "Epoch 0 Step 27 of 219, loss = 1.1460612451191992\n",
            "Epoch 0 Step 28 of 219, loss = 0.232850308297202\n",
            "Epoch 0 Step 29 of 219, loss = 0.7980932465288788\n",
            "Epoch 0 Step 30 of 219, loss = 0.6707243067212403\n",
            "Epoch 0 Step 31 of 219, loss = 1.0265836962498724\n",
            "Epoch 0 Step 32 of 219, loss = 0.44021719112060964\n",
            "Epoch 0 Step 33 of 219, loss = 1.233529387973249\n",
            "Epoch 0 Step 34 of 219, loss = 0.8962225685827434\n",
            "Epoch 0 Step 35 of 219, loss = 0.3089415952563286\n",
            "Epoch 0 Step 36 of 219, loss = 0.5011547510512173\n",
            "Epoch 0 Step 37 of 219, loss = 0.49510961025953293\n",
            "Epoch 0 Step 38 of 219, loss = 0.30979409581050277\n",
            "Epoch 0 Step 39 of 219, loss = 0.5464512859471142\n",
            "Epoch 0 Step 40 of 219, loss = 0.3033157419413328\n",
            "Epoch 0 Step 41 of 219, loss = 0.38681908044964075\n",
            "Epoch 0 Step 42 of 219, loss = 0.3905875198543072\n",
            "Epoch 0 Step 43 of 219, loss = 0.2807204518467188\n",
            "Epoch 0 Step 44 of 219, loss = 0.24628375377506018\n",
            "Epoch 0 Step 45 of 219, loss = 0.6015431880950928\n",
            "Epoch 0 Step 46 of 219, loss = 0.44964367523789406\n",
            "Epoch 0 Step 47 of 219, loss = 1.5279605048708618\n",
            "Epoch 0 Step 48 of 219, loss = 0.21721873572096229\n",
            "Epoch 0 Step 49 of 219, loss = 1.169415406882763\n",
            "Epoch 0 Step 50 of 219, loss = 0.21371772233396769\n",
            "Epoch 0 Step 51 of 219, loss = 0.21727060712873936\n",
            "Epoch 0 Step 52 of 219, loss = 0.2145911632105708\n",
            "Epoch 0 Step 53 of 219, loss = 0.4149413453415036\n",
            "Epoch 0 Step 54 of 219, loss = 0.5723543893545866\n",
            "Epoch 0 Step 55 of 219, loss = 0.1819418971426785\n",
            "Epoch 0 Step 56 of 219, loss = 0.39886949164792895\n",
            "Epoch 0 Step 57 of 219, loss = 0.15274985786527395\n",
            "Epoch 0 Step 58 of 219, loss = 0.4053311664611101\n",
            "Epoch 0 Step 59 of 219, loss = 0.3941977717913687\n",
            "Epoch 0 Step 60 of 219, loss = 0.6414227921050042\n",
            "Epoch 0 Step 61 of 219, loss = 0.6210409197956324\n",
            "Epoch 0 Step 62 of 219, loss = 0.6438004958909005\n",
            "Epoch 0 Step 63 of 219, loss = 0.374454066157341\n",
            "Epoch 0 Step 64 of 219, loss = 0.1259767736773938\n",
            "Epoch 0 Step 65 of 219, loss = 0.10811115754768252\n",
            "Epoch 0 Step 66 of 219, loss = 0.37796060205437243\n",
            "Epoch 0 Step 67 of 219, loss = 1.2090520698111504\n",
            "Epoch 0 Step 68 of 219, loss = 1.4857552126049995\n",
            "Epoch 0 Step 69 of 219, loss = 0.37868332397192717\n",
            "Epoch 0 Step 70 of 219, loss = 0.3181506316177547\n",
            "Epoch 0 Step 71 of 219, loss = 1.363944360986352\n",
            "Epoch 0 Step 72 of 219, loss = 0.38272574031725526\n",
            "Epoch 0 Step 73 of 219, loss = 0.44263512711040676\n",
            "Epoch 0 Step 74 of 219, loss = 0.39850597083568573\n",
            "Epoch 0 Step 75 of 219, loss = 0.4179120445623994\n",
            "Epoch 0 Step 76 of 219, loss = 0.7129148996900767\n",
            "Epoch 0 Step 77 of 219, loss = 0.5918371716979891\n",
            "Epoch 0 Step 78 of 219, loss = 0.388491838471964\n",
            "Epoch 0 Step 79 of 219, loss = 0.3741878350265324\n",
            "Epoch 0 Step 80 of 219, loss = 0.6708754594437778\n",
            "Epoch 0 Step 81 of 219, loss = 0.11785131529904902\n",
            "Epoch 0 Step 82 of 219, loss = 0.4082228378392756\n",
            "Epoch 0 Step 83 of 219, loss = 0.9673036364838481\n",
            "Epoch 0 Step 84 of 219, loss = 0.13809486478567123\n",
            "Epoch 0 Step 85 of 219, loss = 0.6661731414496899\n",
            "Epoch 0 Step 86 of 219, loss = 0.3998865238390863\n",
            "Epoch 0 Step 87 of 219, loss = 1.4245677697472274\n",
            "Epoch 0 Step 88 of 219, loss = 0.5698796976357698\n",
            "Epoch 0 Step 89 of 219, loss = 1.4599499758332968\n",
            "Epoch 0 Step 90 of 219, loss = 0.37504811584949493\n",
            "Epoch 0 Step 91 of 219, loss = 0.9606719622388482\n",
            "Epoch 0 Step 92 of 219, loss = 0.579610978718847\n",
            "Epoch 0 Step 93 of 219, loss = 0.6386252976953983\n",
            "Epoch 0 Step 94 of 219, loss = 0.6095429649576545\n",
            "Epoch 0 Step 95 of 219, loss = 0.2263087360188365\n",
            "Epoch 0 Step 96 of 219, loss = 0.6880205455236137\n",
            "Epoch 0 Step 97 of 219, loss = 0.2539063300937414\n",
            "Epoch 0 Step 98 of 219, loss = 0.9208997911773622\n",
            "Epoch 0 Step 99 of 219, loss = 1.1066925376653671\n",
            "Epoch 0 Step 100 of 219, loss = 0.744878304656595\n",
            "Epoch 0 Step 101 of 219, loss = 0.8798200748860836\n",
            "Epoch 0 Step 102 of 219, loss = 0.7328163180500269\n",
            "Epoch 0 Step 103 of 219, loss = 0.35878303553909063\n",
            "Epoch 0 Step 104 of 219, loss = 0.5802456773817539\n",
            "Epoch 0 Step 105 of 219, loss = 0.3019194584339857\n",
            "Epoch 0 Step 106 of 219, loss = 0.5560205010697246\n",
            "Epoch 0 Step 107 of 219, loss = 0.5813484638929367\n",
            "Epoch 0 Step 108 of 219, loss = 0.4596977895125747\n",
            "Epoch 0 Step 109 of 219, loss = 0.7256092219613492\n",
            "Epoch 0 Step 110 of 219, loss = 0.780916964635253\n",
            "Epoch 0 Step 111 of 219, loss = 0.596060374751687\n",
            "Epoch 0 Step 112 of 219, loss = 0.6067753918468952\n",
            "Epoch 0 Step 113 of 219, loss = 0.6301807248964906\n",
            "Epoch 0 Step 114 of 219, loss = 0.8114125588908792\n",
            "Epoch 0 Step 115 of 219, loss = 0.635965570807457\n",
            "Epoch 0 Step 116 of 219, loss = 0.7882699687033892\n",
            "Epoch 0 Step 117 of 219, loss = 0.4680651193484664\n",
            "Epoch 0 Step 118 of 219, loss = 0.4198287259787321\n",
            "Epoch 0 Step 119 of 219, loss = 0.5323450285941362\n",
            "Epoch 0 Step 120 of 219, loss = 0.457520485855639\n",
            "Epoch 0 Step 121 of 219, loss = 0.5649294760078192\n",
            "Epoch 0 Step 122 of 219, loss = 0.6182337024947628\n",
            "Epoch 0 Step 123 of 219, loss = 0.42047468468081206\n",
            "Epoch 0 Step 124 of 219, loss = 0.6463410975411534\n",
            "Epoch 0 Step 125 of 219, loss = 0.4085392253473401\n",
            "Epoch 0 Step 126 of 219, loss = 0.6147929737344384\n",
            "Epoch 0 Step 127 of 219, loss = 0.38278527557849884\n",
            "Epoch 0 Step 128 of 219, loss = 0.46404152270406485\n",
            "Epoch 0 Step 129 of 219, loss = 0.5549841830506921\n",
            "Epoch 0 Step 130 of 219, loss = 0.5730092218145728\n",
            "Epoch 0 Step 131 of 219, loss = 0.3111046124249697\n",
            "Epoch 0 Step 132 of 219, loss = 0.5247400226071477\n",
            "Epoch 0 Step 133 of 219, loss = 0.8910553567111492\n",
            "Epoch 0 Step 134 of 219, loss = 0.8940339805558324\n",
            "Epoch 0 Step 135 of 219, loss = 0.8269763384014368\n",
            "Epoch 0 Step 136 of 219, loss = 0.673526069149375\n",
            "Epoch 0 Step 137 of 219, loss = 1.0034045837819576\n",
            "Epoch 0 Step 138 of 219, loss = 1.1262880889698863\n",
            "Epoch 0 Step 139 of 219, loss = 0.5475693885236979\n",
            "Epoch 0 Step 140 of 219, loss = 1.1836111713200808\n",
            "Epoch 0 Step 141 of 219, loss = 0.7710988093167543\n",
            "Epoch 0 Step 142 of 219, loss = 0.7654634956270456\n",
            "Epoch 0 Step 143 of 219, loss = 0.8888285709545016\n",
            "Epoch 0 Step 144 of 219, loss = 1.1557405786588788\n",
            "Epoch 0 Step 145 of 219, loss = 0.9055011766031384\n",
            "Epoch 0 Step 146 of 219, loss = 0.704608503729105\n",
            "Epoch 0 Step 147 of 219, loss = 0.5723515171557665\n",
            "Epoch 0 Step 148 of 219, loss = 0.6456505442038178\n",
            "Epoch 0 Step 149 of 219, loss = 0.6488569509238005\n",
            "Epoch 0 Step 150 of 219, loss = 1.0901491241529584\n",
            "Epoch 0 Step 151 of 219, loss = 0.8122062282636762\n",
            "Epoch 0 Step 152 of 219, loss = 0.9356964994221926\n",
            "Epoch 0 Step 153 of 219, loss = 0.8649051692336798\n",
            "Epoch 0 Step 154 of 219, loss = 0.7668278822675347\n",
            "Epoch 0 Step 155 of 219, loss = 0.825905479490757\n",
            "Epoch 0 Step 156 of 219, loss = 0.7312619825825095\n",
            "Epoch 0 Step 157 of 219, loss = 0.8368881929200143\n",
            "Epoch 0 Step 158 of 219, loss = 0.710294050630182\n",
            "Epoch 0 Step 159 of 219, loss = 0.6920264576328918\n",
            "Epoch 0 Step 160 of 219, loss = 0.6906147939153016\n",
            "Epoch 0 Step 161 of 219, loss = 0.6957232366548851\n",
            "Epoch 0 Step 162 of 219, loss = 0.6532820373540744\n",
            "Epoch 0 Step 163 of 219, loss = 0.6199306123889983\n",
            "Epoch 0 Step 164 of 219, loss = 0.6620608901139349\n",
            "Epoch 0 Step 165 of 219, loss = 0.7400710918009281\n",
            "Epoch 0 Step 166 of 219, loss = 0.5737751359120011\n",
            "Epoch 0 Step 167 of 219, loss = 0.9228170500136912\n",
            "Epoch 0 Step 168 of 219, loss = 0.6202831696718931\n",
            "Epoch 0 Step 169 of 219, loss = 0.5042982026934624\n",
            "Epoch 0 Step 170 of 219, loss = 0.6602884382009506\n",
            "Epoch 0 Step 171 of 219, loss = 0.6603249572217464\n",
            "Epoch 0 Step 172 of 219, loss = 0.6738175647333264\n",
            "Epoch 0 Step 173 of 219, loss = 0.558479318395257\n",
            "Epoch 0 Step 174 of 219, loss = 0.8093883134424686\n",
            "Epoch 0 Step 175 of 219, loss = 0.9124294817447662\n",
            "Epoch 0 Step 176 of 219, loss = 0.544518162496388\n",
            "Epoch 0 Step 177 of 219, loss = 0.8519793339073658\n",
            "Epoch 0 Step 178 of 219, loss = 0.42939899303019047\n",
            "Epoch 0 Step 179 of 219, loss = 0.6629182510077953\n",
            "Epoch 0 Step 180 of 219, loss = 0.5795702710747719\n",
            "Epoch 0 Step 181 of 219, loss = 0.525753858499229\n",
            "Epoch 0 Step 182 of 219, loss = 0.43021699879318476\n",
            "Epoch 0 Step 183 of 219, loss = 0.4165778299793601\n",
            "Epoch 0 Step 184 of 219, loss = 0.6575713464990258\n",
            "Epoch 0 Step 185 of 219, loss = 0.5722079891711473\n",
            "Epoch 0 Step 186 of 219, loss = 0.5579455411061645\n",
            "Epoch 0 Step 187 of 219, loss = 0.5435587121173739\n",
            "Epoch 0 Step 188 of 219, loss = 0.835590043105185\n",
            "Epoch 0 Step 189 of 219, loss = 0.5583703564479947\n",
            "Epoch 0 Step 190 of 219, loss = 0.7867253208532929\n",
            "Epoch 0 Step 191 of 219, loss = 0.8838944416493177\n",
            "Epoch 0 Step 192 of 219, loss = 0.7148726768791676\n",
            "Epoch 0 Step 193 of 219, loss = 0.4148664576932788\n",
            "Epoch 0 Step 194 of 219, loss = 0.42626409977674484\n",
            "Epoch 0 Step 195 of 219, loss = 0.7143855029717088\n",
            "Epoch 0 Step 196 of 219, loss = 0.75331019051373\n",
            "Epoch 0 Step 197 of 219, loss = 0.6965746311470866\n",
            "Epoch 0 Step 198 of 219, loss = 0.6552295172587037\n",
            "Epoch 0 Step 199 of 219, loss = 0.8520466946065426\n",
            "Epoch 0 Step 200 of 219, loss = 0.9329894073307514\n",
            "Epoch 0 Step 201 of 219, loss = 0.923250169493258\n",
            "Epoch 0 Step 202 of 219, loss = 0.5526877092197537\n",
            "Epoch 0 Step 203 of 219, loss = 0.7392201088368893\n",
            "Epoch 0 Step 204 of 219, loss = 0.7207266725599766\n",
            "Epoch 0 Step 205 of 219, loss = 0.36363465525209904\n",
            "Epoch 0 Step 206 of 219, loss = 0.7611583974212408\n",
            "Epoch 0 Step 207 of 219, loss = 0.5077242404222488\n",
            "Epoch 0 Step 208 of 219, loss = 0.5230867378413677\n",
            "Epoch 0 Step 209 of 219, loss = 0.5501604210585356\n",
            "Epoch 0 Step 210 of 219, loss = 0.7688454333692789\n",
            "Epoch 0 Step 211 of 219, loss = 0.7959909997880459\n",
            "Epoch 0 Step 212 of 219, loss = 0.842641169205308\n",
            "Epoch 0 Step 213 of 219, loss = 0.6953166723251343\n",
            "Epoch 0 Step 214 of 219, loss = 0.6012296080589294\n",
            "Epoch 0 Step 215 of 219, loss = 0.7011096430942416\n",
            "Epoch 0 Step 216 of 219, loss = 0.7758558290079236\n",
            "Epoch 0 Step 217 of 219, loss = 0.6161257633939385\n",
            "Epoch 0 Step 218 of 219, loss = 0.9371245006720225\n",
            "Epoch 0 average train_loss: 0.671158 test_loss: 0.746779 test_score 0.49%\n",
            "Epoch 1 Step 0 of 219, loss = 0.8235202879877761\n",
            "Epoch 1 Step 1 of 219, loss = 0.7077656839974225\n",
            "Epoch 1 Step 2 of 219, loss = 0.756251655286178\n",
            "Epoch 1 Step 3 of 219, loss = 0.782913074363023\n",
            "Epoch 1 Step 4 of 219, loss = 0.7218147857347503\n",
            "Epoch 1 Step 5 of 219, loss = 0.7179426202201284\n",
            "Epoch 1 Step 6 of 219, loss = 0.7244575971271843\n",
            "Epoch 1 Step 7 of 219, loss = 0.643362426199019\n",
            "Epoch 1 Step 8 of 219, loss = 0.5995320520596579\n",
            "Epoch 1 Step 9 of 219, loss = 0.6039107535034418\n",
            "Epoch 1 Step 10 of 219, loss = 0.5997235774993896\n",
            "Epoch 1 Step 11 of 219, loss = 0.53024329431355\n",
            "Epoch 1 Step 12 of 219, loss = 0.5553985936567187\n",
            "Epoch 1 Step 13 of 219, loss = 0.691975424066186\n",
            "Epoch 1 Step 14 of 219, loss = 0.5042622033506632\n",
            "Epoch 1 Step 15 of 219, loss = 0.4422756042331457\n",
            "Epoch 1 Step 16 of 219, loss = 0.4901692708954215\n",
            "Epoch 1 Step 17 of 219, loss = 1.193291706033051\n",
            "Epoch 1 Step 18 of 219, loss = 0.6510831424966455\n",
            "Epoch 1 Step 19 of 219, loss = 0.8799721030518413\n",
            "Epoch 1 Step 20 of 219, loss = 0.970353914424777\n",
            "Epoch 1 Step 21 of 219, loss = 0.7838249057531357\n",
            "Epoch 1 Step 22 of 219, loss = 0.9260568087920547\n",
            "Epoch 1 Step 23 of 219, loss = 1.233781337738037\n",
            "Epoch 1 Step 24 of 219, loss = 1.1842551911249757\n",
            "Epoch 1 Step 25 of 219, loss = 0.5557417599484324\n",
            "Epoch 1 Step 26 of 219, loss = 0.39199738297611475\n",
            "Epoch 1 Step 27 of 219, loss = 0.9284429047256708\n",
            "Epoch 1 Step 28 of 219, loss = 0.28115665074437857\n",
            "Epoch 1 Step 29 of 219, loss = 0.6663553873077035\n",
            "Epoch 1 Step 30 of 219, loss = 0.530182234942913\n",
            "Epoch 1 Step 31 of 219, loss = 0.876303369179368\n",
            "Epoch 1 Step 32 of 219, loss = 0.4133413191884756\n",
            "Epoch 1 Step 33 of 219, loss = 1.0744307450950146\n",
            "Epoch 1 Step 34 of 219, loss = 0.8617949513718486\n",
            "Epoch 1 Step 35 of 219, loss = 0.2934894757345319\n",
            "Epoch 1 Step 36 of 219, loss = 0.44790387991815805\n",
            "Epoch 1 Step 37 of 219, loss = 0.41274443082511425\n",
            "Epoch 1 Step 38 of 219, loss = 0.2842348534613848\n",
            "Epoch 1 Step 39 of 219, loss = 0.5827860832214355\n",
            "Epoch 1 Step 40 of 219, loss = 0.3280995013192296\n",
            "Epoch 1 Step 41 of 219, loss = 0.3576459242030978\n",
            "Epoch 1 Step 42 of 219, loss = 0.41145636793226004\n",
            "Epoch 1 Step 43 of 219, loss = 0.2589221512898803\n",
            "Epoch 1 Step 44 of 219, loss = 0.25857340078800917\n",
            "Epoch 1 Step 45 of 219, loss = 0.5578346075490117\n",
            "Epoch 1 Step 46 of 219, loss = 0.38757780846208334\n",
            "Epoch 1 Step 47 of 219, loss = 1.4171650186181068\n",
            "Epoch 1 Step 48 of 219, loss = 0.22748062666505575\n",
            "Epoch 1 Step 49 of 219, loss = 1.146974136121571\n",
            "Epoch 1 Step 50 of 219, loss = 0.20601370930671692\n",
            "Epoch 1 Step 51 of 219, loss = 0.18765494227409363\n",
            "Epoch 1 Step 52 of 219, loss = 0.18924076203256845\n",
            "Epoch 1 Step 53 of 219, loss = 0.4065220830962062\n",
            "Epoch 1 Step 54 of 219, loss = 0.587086406070739\n",
            "Epoch 1 Step 55 of 219, loss = 0.15283992420881987\n",
            "Epoch 1 Step 56 of 219, loss = 0.3520113346166909\n",
            "Epoch 1 Step 57 of 219, loss = 0.13646302092820406\n",
            "Epoch 1 Step 58 of 219, loss = 0.3929552691988647\n",
            "Epoch 1 Step 59 of 219, loss = 0.3937700195237994\n",
            "Epoch 1 Step 60 of 219, loss = 0.66496038204059\n",
            "Epoch 1 Step 61 of 219, loss = 0.6943851623218507\n",
            "Epoch 1 Step 62 of 219, loss = 0.6710938813630491\n",
            "Epoch 1 Step 63 of 219, loss = 0.4131970969028771\n",
            "Epoch 1 Step 64 of 219, loss = 0.10322357388213277\n",
            "Epoch 1 Step 65 of 219, loss = 0.10568773746490479\n",
            "Epoch 1 Step 66 of 219, loss = 0.4093132233247161\n",
            "Epoch 1 Step 67 of 219, loss = 1.2531018974259496\n",
            "Epoch 1 Step 68 of 219, loss = 1.5446941533591598\n",
            "Epoch 1 Step 69 of 219, loss = 0.33831312227994204\n",
            "Epoch 1 Step 70 of 219, loss = 0.3324098358862102\n",
            "Epoch 1 Step 71 of 219, loss = 1.2616776218637824\n",
            "Epoch 1 Step 72 of 219, loss = 0.3806322368327528\n",
            "Epoch 1 Step 73 of 219, loss = 0.38243231549859047\n",
            "Epoch 1 Step 74 of 219, loss = 0.3925658627413213\n",
            "Epoch 1 Step 75 of 219, loss = 0.3826142540201545\n",
            "Epoch 1 Step 76 of 219, loss = 0.6218250887468457\n",
            "Epoch 1 Step 77 of 219, loss = 0.6137435915879905\n",
            "Epoch 1 Step 78 of 219, loss = 0.4194131544791162\n",
            "Epoch 1 Step 79 of 219, loss = 0.3600492374971509\n",
            "Epoch 1 Step 80 of 219, loss = 0.5486647505313158\n",
            "Epoch 1 Step 81 of 219, loss = 0.13831105828285217\n",
            "Epoch 1 Step 82 of 219, loss = 0.3490500617772341\n",
            "Epoch 1 Step 83 of 219, loss = 0.8406381239183247\n",
            "Epoch 1 Step 84 of 219, loss = 0.13527214154601097\n",
            "Epoch 1 Step 85 of 219, loss = 0.6201779050752521\n",
            "Epoch 1 Step 86 of 219, loss = 0.3933422230184078\n",
            "Epoch 1 Step 87 of 219, loss = 1.2860223902389407\n",
            "Epoch 1 Step 88 of 219, loss = 0.5929544530808926\n",
            "Epoch 1 Step 89 of 219, loss = 1.3823329624719918\n",
            "Epoch 1 Step 90 of 219, loss = 0.316559337079525\n",
            "Epoch 1 Step 91 of 219, loss = 0.882852298207581\n",
            "Epoch 1 Step 92 of 219, loss = 0.589998772367835\n",
            "Epoch 1 Step 93 of 219, loss = 0.6186956455931067\n",
            "Epoch 1 Step 94 of 219, loss = 0.5684332167729735\n",
            "Epoch 1 Step 95 of 219, loss = 0.263631178997457\n",
            "Epoch 1 Step 96 of 219, loss = 0.6690646884962916\n",
            "Epoch 1 Step 97 of 219, loss = 0.2807012898847461\n",
            "Epoch 1 Step 98 of 219, loss = 0.8506238330155611\n",
            "Epoch 1 Step 99 of 219, loss = 0.9251546813175082\n",
            "Epoch 1 Step 100 of 219, loss = 0.698599829338491\n",
            "Epoch 1 Step 101 of 219, loss = 0.8244215724989772\n",
            "Epoch 1 Step 102 of 219, loss = 0.7265148498117924\n",
            "Epoch 1 Step 103 of 219, loss = 0.3933266419917345\n",
            "Epoch 1 Step 104 of 219, loss = 0.4950344115495682\n",
            "Epoch 1 Step 105 of 219, loss = 0.3604992162436247\n",
            "Epoch 1 Step 106 of 219, loss = 0.5910251177847385\n",
            "Epoch 1 Step 107 of 219, loss = 0.5457321330904961\n",
            "Epoch 1 Step 108 of 219, loss = 0.4720435310155153\n",
            "Epoch 1 Step 109 of 219, loss = 0.6780460691079497\n",
            "Epoch 1 Step 110 of 219, loss = 0.7552686519920826\n",
            "Epoch 1 Step 111 of 219, loss = 0.5599592123180628\n",
            "Epoch 1 Step 112 of 219, loss = 0.5752705316990614\n",
            "Epoch 1 Step 113 of 219, loss = 0.6157898865640163\n",
            "Epoch 1 Step 114 of 219, loss = 0.7145722829736769\n",
            "Epoch 1 Step 115 of 219, loss = 0.7215933483093977\n",
            "Epoch 1 Step 116 of 219, loss = 0.7226801998913288\n",
            "Epoch 1 Step 117 of 219, loss = 0.5424268692731857\n",
            "Epoch 1 Step 118 of 219, loss = 0.4422547686845064\n",
            "Epoch 1 Step 119 of 219, loss = 0.5069032367318869\n",
            "Epoch 1 Step 120 of 219, loss = 0.5153232589364052\n",
            "Epoch 1 Step 121 of 219, loss = 0.615284388884902\n",
            "Epoch 1 Step 122 of 219, loss = 0.5814443146809936\n",
            "Epoch 1 Step 123 of 219, loss = 0.3765275124460459\n",
            "Epoch 1 Step 124 of 219, loss = 0.685759755782783\n",
            "Epoch 1 Step 125 of 219, loss = 0.3783658631145954\n",
            "Epoch 1 Step 126 of 219, loss = 0.5742676546797156\n",
            "Epoch 1 Step 127 of 219, loss = 0.3582698265090585\n",
            "Epoch 1 Step 128 of 219, loss = 0.48239752650260925\n",
            "Epoch 1 Step 129 of 219, loss = 0.5403087250888348\n",
            "Epoch 1 Step 130 of 219, loss = 0.5362440189346671\n",
            "Epoch 1 Step 131 of 219, loss = 0.31645928230136633\n",
            "Epoch 1 Step 132 of 219, loss = 0.5012513510882854\n",
            "Epoch 1 Step 133 of 219, loss = 0.824284078553319\n",
            "Epoch 1 Step 134 of 219, loss = 0.883777717128396\n",
            "Epoch 1 Step 135 of 219, loss = 0.8981894683092833\n",
            "Epoch 1 Step 136 of 219, loss = 0.7269928259775043\n",
            "Epoch 1 Step 137 of 219, loss = 1.0559477172791958\n",
            "Epoch 1 Step 138 of 219, loss = 1.1012376034632325\n",
            "Epoch 1 Step 139 of 219, loss = 0.5167436879128218\n",
            "Epoch 1 Step 140 of 219, loss = 1.1444241264835\n",
            "Epoch 1 Step 141 of 219, loss = 0.6717815203592181\n",
            "Epoch 1 Step 142 of 219, loss = 0.7122314171865582\n",
            "Epoch 1 Step 143 of 219, loss = 0.7740189991891384\n",
            "Epoch 1 Step 144 of 219, loss = 1.109902928583324\n",
            "Epoch 1 Step 145 of 219, loss = 0.8574983524158597\n",
            "Epoch 1 Step 146 of 219, loss = 0.6559460181742907\n",
            "Epoch 1 Step 147 of 219, loss = 0.5991228669881821\n",
            "Epoch 1 Step 148 of 219, loss = 0.6767181940376759\n",
            "Epoch 1 Step 149 of 219, loss = 0.6463054697960615\n",
            "Epoch 1 Step 150 of 219, loss = 1.0081976875662804\n",
            "Epoch 1 Step 151 of 219, loss = 0.8408259451389313\n",
            "Epoch 1 Step 152 of 219, loss = 0.8565638726577163\n",
            "Epoch 1 Step 153 of 219, loss = 0.7527880854904652\n",
            "Epoch 1 Step 154 of 219, loss = 0.7586001153104007\n",
            "Epoch 1 Step 155 of 219, loss = 0.704214524012059\n",
            "Epoch 1 Step 156 of 219, loss = 0.6947949421592057\n",
            "Epoch 1 Step 157 of 219, loss = 0.6918143360817339\n",
            "Epoch 1 Step 158 of 219, loss = 0.7001493913121521\n",
            "Epoch 1 Step 159 of 219, loss = 0.678514534025453\n",
            "Epoch 1 Step 160 of 219, loss = 0.697873123921454\n",
            "Epoch 1 Step 161 of 219, loss = 0.5734271500259638\n",
            "Epoch 1 Step 162 of 219, loss = 0.49475577659904957\n",
            "Epoch 1 Step 163 of 219, loss = 0.6116778310388327\n",
            "Epoch 1 Step 164 of 219, loss = 0.6114027556031942\n",
            "Epoch 1 Step 165 of 219, loss = 0.7710530208423734\n",
            "Epoch 1 Step 166 of 219, loss = 0.45157585944980383\n",
            "Epoch 1 Step 167 of 219, loss = 1.0678807254880667\n",
            "Epoch 1 Step 168 of 219, loss = 0.5641058422625065\n",
            "Epoch 1 Step 169 of 219, loss = 0.4045121199451387\n",
            "Epoch 1 Step 170 of 219, loss = 0.7324302438646555\n",
            "Epoch 1 Step 171 of 219, loss = 0.6584632731974125\n",
            "Epoch 1 Step 172 of 219, loss = 0.7457485590130091\n",
            "Epoch 1 Step 173 of 219, loss = 0.5629643611609936\n",
            "Epoch 1 Step 174 of 219, loss = 0.8927549291402102\n",
            "Epoch 1 Step 175 of 219, loss = 1.0828505214303732\n",
            "Epoch 1 Step 176 of 219, loss = 0.5941885695792735\n",
            "Epoch 1 Step 177 of 219, loss = 1.0061566550284624\n",
            "Epoch 1 Step 178 of 219, loss = 0.4341168310493231\n",
            "Epoch 1 Step 179 of 219, loss = 0.6981373797170818\n",
            "Epoch 1 Step 180 of 219, loss = 0.4927981197834015\n",
            "Epoch 1 Step 181 of 219, loss = 0.5565472710877657\n",
            "Epoch 1 Step 182 of 219, loss = 0.40822244714945555\n",
            "Epoch 1 Step 183 of 219, loss = 0.39967027213424444\n",
            "Epoch 1 Step 184 of 219, loss = 0.6804966703057289\n",
            "Epoch 1 Step 185 of 219, loss = 0.5886389529332519\n",
            "Epoch 1 Step 186 of 219, loss = 0.6052457736805081\n",
            "Epoch 1 Step 187 of 219, loss = 0.5422571720555425\n",
            "Epoch 1 Step 188 of 219, loss = 0.8015878209844232\n",
            "Epoch 1 Step 189 of 219, loss = 0.5757147232070565\n",
            "Epoch 1 Step 190 of 219, loss = 0.8200593646615744\n",
            "Epoch 1 Step 191 of 219, loss = 0.7644076738506556\n",
            "Epoch 1 Step 192 of 219, loss = 0.6437497744336724\n",
            "Epoch 1 Step 193 of 219, loss = 0.4388571474701166\n",
            "Epoch 1 Step 194 of 219, loss = 0.42097875662148\n",
            "Epoch 1 Step 195 of 219, loss = 0.7041413709521294\n",
            "Epoch 1 Step 196 of 219, loss = 0.6626191530376673\n",
            "Epoch 1 Step 197 of 219, loss = 0.6462489059194922\n",
            "Epoch 1 Step 198 of 219, loss = 0.6370965815149248\n",
            "Epoch 1 Step 199 of 219, loss = 0.796099878847599\n",
            "Epoch 1 Step 200 of 219, loss = 0.8493201239034534\n",
            "Epoch 1 Step 201 of 219, loss = 0.8695907276123762\n",
            "Epoch 1 Step 202 of 219, loss = 0.5841173678636551\n",
            "Epoch 1 Step 203 of 219, loss = 0.7129054702818394\n",
            "Epoch 1 Step 204 of 219, loss = 0.6692400481551886\n",
            "Epoch 1 Step 205 of 219, loss = 0.4835039060562849\n",
            "Epoch 1 Step 206 of 219, loss = 0.7208334719762206\n",
            "Epoch 1 Step 207 of 219, loss = 0.5492726657539606\n",
            "Epoch 1 Step 208 of 219, loss = 0.5888764150440693\n",
            "Epoch 1 Step 209 of 219, loss = 0.5389323203125969\n",
            "Epoch 1 Step 210 of 219, loss = 0.722390818875283\n",
            "Epoch 1 Step 211 of 219, loss = 0.7220670226961374\n",
            "Epoch 1 Step 212 of 219, loss = 0.750635719159618\n",
            "Epoch 1 Step 213 of 219, loss = 0.717376877553761\n",
            "Epoch 1 Step 214 of 219, loss = 0.6331330584362149\n",
            "Epoch 1 Step 215 of 219, loss = 0.6812059570802376\n",
            "Epoch 1 Step 216 of 219, loss = 0.7828805558071963\n",
            "Epoch 1 Step 217 of 219, loss = 0.5859841752098873\n",
            "Epoch 1 Step 218 of 219, loss = 0.7961767244463165\n",
            "Epoch 1 average train_loss: 0.630222 test_loss: 0.731343 test_score 0.55%\n",
            "Epoch 2 Step 0 of 219, loss = 0.8343227724544704\n",
            "Epoch 2 Step 1 of 219, loss = 0.6713820129880332\n",
            "Epoch 2 Step 2 of 219, loss = 0.743791650980711\n",
            "Epoch 2 Step 3 of 219, loss = 0.7318117897957563\n",
            "Epoch 2 Step 4 of 219, loss = 0.6675460202968679\n",
            "Epoch 2 Step 5 of 219, loss = 0.6441959150834009\n",
            "Epoch 2 Step 6 of 219, loss = 0.628343627351569\n",
            "Epoch 2 Step 7 of 219, loss = 0.5917565105482936\n",
            "Epoch 2 Step 8 of 219, loss = 0.5061390800401568\n",
            "Epoch 2 Step 9 of 219, loss = 0.5328755269292742\n",
            "Epoch 2 Step 10 of 219, loss = 0.526843142695725\n",
            "Epoch 2 Step 11 of 219, loss = 0.42627306189388037\n",
            "Epoch 2 Step 12 of 219, loss = 0.46538504119962454\n",
            "Epoch 2 Step 13 of 219, loss = 0.7419899553060532\n",
            "Epoch 2 Step 14 of 219, loss = 0.4701302144676447\n",
            "Epoch 2 Step 15 of 219, loss = 0.40426025725901127\n",
            "Epoch 2 Step 16 of 219, loss = 0.4715640884824097\n",
            "Epoch 2 Step 17 of 219, loss = 1.3942643031477928\n",
            "Epoch 2 Step 18 of 219, loss = 0.6372567787766457\n",
            "Epoch 2 Step 19 of 219, loss = 1.012060683220625\n",
            "Epoch 2 Step 20 of 219, loss = 0.96600461890921\n",
            "Epoch 2 Step 21 of 219, loss = 0.9135403241962194\n",
            "Epoch 2 Step 22 of 219, loss = 1.0224152943119407\n",
            "Epoch 2 Step 23 of 219, loss = 1.3264744626358151\n",
            "Epoch 2 Step 24 of 219, loss = 1.1379557866603136\n",
            "Epoch 2 Step 25 of 219, loss = 0.6083109267055988\n",
            "Epoch 2 Step 26 of 219, loss = 0.33450991241261363\n",
            "Epoch 2 Step 27 of 219, loss = 1.038188498467207\n",
            "Epoch 2 Step 28 of 219, loss = 0.22564147226512432\n",
            "Epoch 2 Step 29 of 219, loss = 0.713107786141336\n",
            "Epoch 2 Step 30 of 219, loss = 0.5408683689311147\n",
            "Epoch 2 Step 31 of 219, loss = 0.9172452865168452\n",
            "Epoch 2 Step 32 of 219, loss = 0.39955002116039395\n",
            "Epoch 2 Step 33 of 219, loss = 1.190860322676599\n",
            "Epoch 2 Step 34 of 219, loss = 0.9234969303943217\n",
            "Epoch 2 Step 35 of 219, loss = 0.2823483180254698\n",
            "Epoch 2 Step 36 of 219, loss = 0.4013036089017987\n",
            "Epoch 2 Step 37 of 219, loss = 0.38535528955981135\n",
            "Epoch 2 Step 38 of 219, loss = 0.27824797108769417\n",
            "Epoch 2 Step 39 of 219, loss = 0.5859879944473505\n",
            "Epoch 2 Step 40 of 219, loss = 0.32965486589819193\n",
            "Epoch 2 Step 41 of 219, loss = 0.41944747138768435\n",
            "Epoch 2 Step 42 of 219, loss = 0.3602322321385145\n",
            "Epoch 2 Step 43 of 219, loss = 0.220952152274549\n",
            "Epoch 2 Step 44 of 219, loss = 0.2475343607366085\n",
            "Epoch 2 Step 45 of 219, loss = 0.5906110419891775\n",
            "Epoch 2 Step 46 of 219, loss = 0.36693436559289694\n",
            "Epoch 2 Step 47 of 219, loss = 1.5672263600863516\n",
            "Epoch 2 Step 48 of 219, loss = 0.18664424028247595\n",
            "Epoch 2 Step 49 of 219, loss = 1.3041811045259237\n",
            "Epoch 2 Step 50 of 219, loss = 0.20881902799010277\n",
            "Epoch 2 Step 51 of 219, loss = 0.15285817626863718\n",
            "Epoch 2 Step 52 of 219, loss = 0.148658849298954\n",
            "Epoch 2 Step 53 of 219, loss = 0.3317473803181201\n",
            "Epoch 2 Step 54 of 219, loss = 0.6090551253873855\n",
            "Epoch 2 Step 55 of 219, loss = 0.12910877796821296\n",
            "Epoch 2 Step 56 of 219, loss = 0.3217840928118676\n",
            "Epoch 2 Step 57 of 219, loss = 0.10280719585716724\n",
            "Epoch 2 Step 58 of 219, loss = 0.3416527980007231\n",
            "Epoch 2 Step 59 of 219, loss = 0.40122827095910907\n",
            "Epoch 2 Step 60 of 219, loss = 0.7312146027106792\n",
            "Epoch 2 Step 61 of 219, loss = 0.6840989347547293\n",
            "Epoch 2 Step 62 of 219, loss = 0.6135251193773001\n",
            "Epoch 2 Step 63 of 219, loss = 0.390134557033889\n",
            "Epoch 2 Step 64 of 219, loss = 0.07778410078026354\n",
            "Epoch 2 Step 65 of 219, loss = 0.07591413846239448\n",
            "Epoch 2 Step 66 of 219, loss = 0.36996576248202473\n",
            "Epoch 2 Step 67 of 219, loss = 1.3548324583098292\n",
            "Epoch 2 Step 68 of 219, loss = 1.6709960793377832\n",
            "Epoch 2 Step 69 of 219, loss = 0.3601728326175362\n",
            "Epoch 2 Step 70 of 219, loss = 0.3532333371695131\n",
            "Epoch 2 Step 71 of 219, loss = 1.327897152863443\n",
            "Epoch 2 Step 72 of 219, loss = 0.38705448526889086\n",
            "Epoch 2 Step 73 of 219, loss = 0.33830086328089237\n",
            "Epoch 2 Step 74 of 219, loss = 0.41928571881726384\n",
            "Epoch 2 Step 75 of 219, loss = 0.4354682406410575\n",
            "Epoch 2 Step 76 of 219, loss = 0.6652837155852467\n",
            "Epoch 2 Step 77 of 219, loss = 0.6372630351688713\n",
            "Epoch 2 Step 78 of 219, loss = 0.33581673447042704\n",
            "Epoch 2 Step 79 of 219, loss = 0.3632294526323676\n",
            "Epoch 2 Step 80 of 219, loss = 0.5013504116795957\n",
            "Epoch 2 Step 81 of 219, loss = 0.14718141267076135\n",
            "Epoch 2 Step 82 of 219, loss = 0.34607783053070307\n",
            "Epoch 2 Step 83 of 219, loss = 0.8162830923683941\n",
            "Epoch 2 Step 84 of 219, loss = 0.17685571871697903\n",
            "Epoch 2 Step 85 of 219, loss = 0.6171612720936537\n",
            "Epoch 2 Step 86 of 219, loss = 0.3065948514267802\n",
            "Epoch 2 Step 87 of 219, loss = 1.1430736370384693\n",
            "Epoch 2 Step 88 of 219, loss = 0.5227258787490427\n",
            "Epoch 2 Step 89 of 219, loss = 1.2919111652299762\n",
            "Epoch 2 Step 90 of 219, loss = 0.2888991702347994\n",
            "Epoch 2 Step 91 of 219, loss = 0.6354763605631888\n",
            "Epoch 2 Step 92 of 219, loss = 0.5506509384140372\n",
            "Epoch 2 Step 93 of 219, loss = 0.5800179904326797\n",
            "Epoch 2 Step 94 of 219, loss = 0.5390226086601615\n",
            "Epoch 2 Step 95 of 219, loss = 0.3050944795832038\n",
            "Epoch 2 Step 96 of 219, loss = 0.5899520749226213\n",
            "Epoch 2 Step 97 of 219, loss = 0.29632294829934835\n",
            "Epoch 2 Step 98 of 219, loss = 0.8070519352331758\n",
            "Epoch 2 Step 99 of 219, loss = 0.9020488099195063\n",
            "Epoch 2 Step 100 of 219, loss = 0.6040441514924169\n",
            "Epoch 2 Step 101 of 219, loss = 0.7213771203532815\n",
            "Epoch 2 Step 102 of 219, loss = 0.7446161461994052\n",
            "Epoch 2 Step 103 of 219, loss = 0.3763420758768916\n",
            "Epoch 2 Step 104 of 219, loss = 0.48917402047663927\n",
            "Epoch 2 Step 105 of 219, loss = 0.36169576831161976\n",
            "Epoch 2 Step 106 of 219, loss = 0.45320977875962853\n",
            "Epoch 2 Step 107 of 219, loss = 0.47276287607382983\n",
            "Epoch 2 Step 108 of 219, loss = 0.41234726551920176\n",
            "Epoch 2 Step 109 of 219, loss = 0.591798537876457\n",
            "Epoch 2 Step 110 of 219, loss = 0.7432856191881001\n",
            "Epoch 2 Step 111 of 219, loss = 0.6352780712768435\n",
            "Epoch 2 Step 112 of 219, loss = 0.5727745722979307\n",
            "Epoch 2 Step 113 of 219, loss = 0.6353899960231502\n",
            "Epoch 2 Step 114 of 219, loss = 0.6304282830096781\n",
            "Epoch 2 Step 115 of 219, loss = 0.7028064418118447\n",
            "Epoch 2 Step 116 of 219, loss = 0.7514026993885636\n",
            "Epoch 2 Step 117 of 219, loss = 0.38176264241337776\n",
            "Epoch 2 Step 118 of 219, loss = 0.27757636830210686\n",
            "Epoch 2 Step 119 of 219, loss = 0.39182525873184204\n",
            "Epoch 2 Step 120 of 219, loss = 0.44117093132808805\n",
            "Epoch 2 Step 121 of 219, loss = 0.4444830813445151\n",
            "Epoch 2 Step 122 of 219, loss = 0.47144447488244623\n",
            "Epoch 2 Step 123 of 219, loss = 0.17514798697084188\n",
            "Epoch 2 Step 124 of 219, loss = 0.8343518651090562\n",
            "Epoch 2 Step 125 of 219, loss = 0.20773977786302567\n",
            "Epoch 2 Step 126 of 219, loss = 0.4807289675809443\n",
            "Epoch 2 Step 127 of 219, loss = 0.13428799575194716\n",
            "Epoch 2 Step 128 of 219, loss = 0.37618624209426343\n",
            "Epoch 2 Step 129 of 219, loss = 0.6365738157182932\n",
            "Epoch 2 Step 130 of 219, loss = 0.6085541637148708\n",
            "Epoch 2 Step 131 of 219, loss = 0.07856543082743883\n",
            "Epoch 2 Step 132 of 219, loss = 0.5785165538545698\n",
            "Epoch 2 Step 133 of 219, loss = 1.1803349445108324\n",
            "Epoch 2 Step 134 of 219, loss = 1.2411339078098536\n",
            "Epoch 2 Step 135 of 219, loss = 1.1571014223154634\n",
            "Epoch 2 Step 136 of 219, loss = 0.8979467053432018\n",
            "Epoch 2 Step 137 of 219, loss = 1.3986297291703522\n",
            "Epoch 2 Step 138 of 219, loss = 1.4451182917691767\n",
            "Epoch 2 Step 139 of 219, loss = 0.5084649161435664\n",
            "Epoch 2 Step 140 of 219, loss = 1.0237449202686548\n",
            "Epoch 2 Step 141 of 219, loss = 0.6448526289314032\n",
            "Epoch 2 Step 142 of 219, loss = 0.6964154178276658\n",
            "Epoch 2 Step 143 of 219, loss = 0.7402089275419712\n",
            "Epoch 2 Step 144 of 219, loss = 1.0147356316447258\n",
            "Epoch 2 Step 145 of 219, loss = 0.7307834066450596\n",
            "Epoch 2 Step 146 of 219, loss = 0.5772377436514944\n",
            "Epoch 2 Step 147 of 219, loss = 0.6459119194187224\n",
            "Epoch 2 Step 148 of 219, loss = 0.7637929953634739\n",
            "Epoch 2 Step 149 of 219, loss = 0.6948135315724357\n",
            "Epoch 2 Step 150 of 219, loss = 0.6769263816531748\n",
            "Epoch 2 Step 151 of 219, loss = 0.6921721526887268\n",
            "Epoch 2 Step 152 of 219, loss = 0.595664017368108\n",
            "Epoch 2 Step 153 of 219, loss = 0.7232648618519306\n",
            "Epoch 2 Step 154 of 219, loss = 0.6268310435116291\n",
            "Epoch 2 Step 155 of 219, loss = 0.6520853480324149\n",
            "Epoch 2 Step 156 of 219, loss = 0.5656618171487935\n",
            "Epoch 2 Step 157 of 219, loss = 0.4424746409058571\n",
            "Epoch 2 Step 158 of 219, loss = 0.6884203953668475\n",
            "Epoch 2 Step 159 of 219, loss = 0.7688583713024855\n",
            "Epoch 2 Step 160 of 219, loss = 0.6781364800408483\n",
            "Epoch 2 Step 161 of 219, loss = 0.4394804798066616\n",
            "Epoch 2 Step 162 of 219, loss = 0.4088169038295746\n",
            "Epoch 2 Step 163 of 219, loss = 0.4940241826698184\n",
            "Epoch 2 Step 164 of 219, loss = 0.555412326939404\n",
            "Epoch 2 Step 165 of 219, loss = 0.9041526722721756\n",
            "Epoch 2 Step 166 of 219, loss = 0.4574300395324826\n",
            "Epoch 2 Step 167 of 219, loss = 1.2577083567157388\n",
            "Epoch 2 Step 168 of 219, loss = 0.5975969941355288\n",
            "Epoch 2 Step 169 of 219, loss = 0.3370982832275331\n",
            "Epoch 2 Step 170 of 219, loss = 0.7066652714274824\n",
            "Epoch 2 Step 171 of 219, loss = 0.6963517367839813\n",
            "Epoch 2 Step 172 of 219, loss = 0.7567552123218775\n",
            "Epoch 2 Step 173 of 219, loss = 0.559400093741715\n",
            "Epoch 2 Step 174 of 219, loss = 0.8785451399162412\n",
            "Epoch 2 Step 175 of 219, loss = 1.0088030844926834\n",
            "Epoch 2 Step 176 of 219, loss = 0.5644665351137519\n",
            "Epoch 2 Step 177 of 219, loss = 0.8761136569082737\n",
            "Epoch 2 Step 178 of 219, loss = 0.5026770299300551\n",
            "Epoch 2 Step 179 of 219, loss = 0.6385592119768262\n",
            "Epoch 2 Step 180 of 219, loss = 0.5472100966144353\n",
            "Epoch 2 Step 181 of 219, loss = 0.5542417969554663\n",
            "Epoch 2 Step 182 of 219, loss = 0.4565801601856947\n",
            "Epoch 2 Step 183 of 219, loss = 0.4589625149965286\n",
            "Epoch 2 Step 184 of 219, loss = 0.5617730570957065\n",
            "Epoch 2 Step 185 of 219, loss = 0.5353122404776514\n",
            "Epoch 2 Step 186 of 219, loss = 0.5698886159807444\n",
            "Epoch 2 Step 187 of 219, loss = 0.53039894066751\n",
            "Epoch 2 Step 188 of 219, loss = 0.7485753316432238\n",
            "Epoch 2 Step 189 of 219, loss = 0.52989721018821\n",
            "Epoch 2 Step 190 of 219, loss = 0.7169548012316227\n",
            "Epoch 2 Step 191 of 219, loss = 0.7553407344967127\n",
            "Epoch 2 Step 192 of 219, loss = 0.5998927466571331\n",
            "Epoch 2 Step 193 of 219, loss = 0.42419842816889286\n",
            "Epoch 2 Step 194 of 219, loss = 0.46383841149508953\n",
            "Epoch 2 Step 195 of 219, loss = 0.6183941885828972\n",
            "Epoch 2 Step 196 of 219, loss = 0.6331520061939955\n",
            "Epoch 2 Step 197 of 219, loss = 0.6366742409300059\n",
            "Epoch 2 Step 198 of 219, loss = 0.6089598536491394\n",
            "Epoch 2 Step 199 of 219, loss = 0.7392587554641068\n",
            "Epoch 2 Step 200 of 219, loss = 0.8586282068863511\n",
            "Epoch 2 Step 201 of 219, loss = 0.8809743411839008\n",
            "Epoch 2 Step 202 of 219, loss = 0.5158112375065684\n",
            "Epoch 2 Step 203 of 219, loss = 0.7197817068081349\n",
            "Epoch 2 Step 204 of 219, loss = 0.5584924662070989\n",
            "Epoch 2 Step 205 of 219, loss = 0.424026919528842\n",
            "Epoch 2 Step 206 of 219, loss = 0.6876433654688299\n",
            "Epoch 2 Step 207 of 219, loss = 0.5471400846727192\n",
            "Epoch 2 Step 208 of 219, loss = 0.530410747975111\n",
            "Epoch 2 Step 209 of 219, loss = 0.561954396776855\n",
            "Epoch 2 Step 210 of 219, loss = 0.6760027105920017\n",
            "Epoch 2 Step 211 of 219, loss = 0.7416640073060989\n",
            "Epoch 2 Step 212 of 219, loss = 0.8319708984345198\n",
            "Epoch 2 Step 213 of 219, loss = 0.6871592099778354\n",
            "Epoch 2 Step 214 of 219, loss = 0.5747637543827295\n",
            "Epoch 2 Step 215 of 219, loss = 0.5880286603642162\n",
            "Epoch 2 Step 216 of 219, loss = 0.6459215213544667\n",
            "Epoch 2 Step 217 of 219, loss = 0.527442894410342\n",
            "Epoch 2 Step 218 of 219, loss = 0.8472640265244991\n",
            "Epoch 2 average train_loss: 0.612849 test_loss: 0.701606 test_score 0.53%\n",
            "Epoch 3 Step 0 of 219, loss = 0.7078004013746977\n",
            "Epoch 3 Step 1 of 219, loss = 0.5152194052934647\n",
            "Epoch 3 Step 2 of 219, loss = 0.7290521659888327\n",
            "Epoch 3 Step 3 of 219, loss = 0.6773700648336671\n",
            "Epoch 3 Step 4 of 219, loss = 0.5668355021625757\n",
            "Epoch 3 Step 5 of 219, loss = 0.45503595960326493\n",
            "Epoch 3 Step 6 of 219, loss = 0.4696282986551523\n",
            "Epoch 3 Step 7 of 219, loss = 0.5368105275556445\n",
            "Epoch 3 Step 8 of 219, loss = 0.41508918069303036\n",
            "Epoch 3 Step 9 of 219, loss = 0.32961976435035467\n",
            "Epoch 3 Step 10 of 219, loss = 0.347799448762089\n",
            "Epoch 3 Step 11 of 219, loss = 0.23743135621771216\n",
            "Epoch 3 Step 12 of 219, loss = 0.343390392139554\n",
            "Epoch 3 Step 13 of 219, loss = 0.8911227714270353\n",
            "Epoch 3 Step 14 of 219, loss = 0.33630337938666344\n",
            "Epoch 3 Step 15 of 219, loss = 0.2799157686531544\n",
            "Epoch 3 Step 16 of 219, loss = 0.36502798926085234\n",
            "Epoch 3 Step 17 of 219, loss = 1.5403478322550654\n",
            "Epoch 3 Step 18 of 219, loss = 0.6649859356693923\n",
            "Epoch 3 Step 19 of 219, loss = 1.0458857738412917\n",
            "Epoch 3 Step 20 of 219, loss = 0.8835342264501378\n",
            "Epoch 3 Step 21 of 219, loss = 0.8526473040692508\n",
            "Epoch 3 Step 22 of 219, loss = 1.2188648232258856\n",
            "Epoch 3 Step 23 of 219, loss = 1.3169626668095589\n",
            "Epoch 3 Step 24 of 219, loss = 1.050519841723144\n",
            "Epoch 3 Step 25 of 219, loss = 0.4732401450164616\n",
            "Epoch 3 Step 26 of 219, loss = 0.3495161924511194\n",
            "Epoch 3 Step 27 of 219, loss = 0.9749117134997505\n",
            "Epoch 3 Step 28 of 219, loss = 0.15522371977567673\n",
            "Epoch 3 Step 29 of 219, loss = 0.7226095204241574\n",
            "Epoch 3 Step 30 of 219, loss = 0.40753072826191783\n",
            "Epoch 3 Step 31 of 219, loss = 0.760811644140631\n",
            "Epoch 3 Step 32 of 219, loss = 0.34557333169505\n",
            "Epoch 3 Step 33 of 219, loss = 1.2296048095449805\n",
            "Epoch 3 Step 34 of 219, loss = 0.894311630167067\n",
            "Epoch 3 Step 35 of 219, loss = 0.25180579675361514\n",
            "Epoch 3 Step 36 of 219, loss = 0.46697675390169024\n",
            "Epoch 3 Step 37 of 219, loss = 0.3818927416577935\n",
            "Epoch 3 Step 38 of 219, loss = 0.2268952843733132\n",
            "Epoch 3 Step 39 of 219, loss = 0.5259251408278942\n",
            "Epoch 3 Step 40 of 219, loss = 0.41981088276952505\n",
            "Epoch 3 Step 41 of 219, loss = 0.36129631334915757\n",
            "Epoch 3 Step 42 of 219, loss = 0.37866411334834993\n",
            "Epoch 3 Step 43 of 219, loss = 0.18090712605044246\n",
            "Epoch 3 Step 44 of 219, loss = 0.24416578700765967\n",
            "Epoch 3 Step 45 of 219, loss = 0.45866579888388515\n",
            "Epoch 3 Step 46 of 219, loss = 0.3229872966185212\n",
            "Epoch 3 Step 47 of 219, loss = 1.771320729283616\n",
            "Epoch 3 Step 48 of 219, loss = 0.16052335710264742\n",
            "Epoch 3 Step 49 of 219, loss = 1.6314071568194777\n",
            "Epoch 3 Step 50 of 219, loss = 0.19633948267437518\n",
            "Epoch 3 Step 51 of 219, loss = 0.07159639988094568\n",
            "Epoch 3 Step 52 of 219, loss = 0.06135462818201631\n",
            "Epoch 3 Step 53 of 219, loss = 0.36866182717494667\n",
            "Epoch 3 Step 54 of 219, loss = 0.6652122169034556\n",
            "Epoch 3 Step 55 of 219, loss = 0.05516227416228503\n",
            "Epoch 3 Step 56 of 219, loss = 0.28141739091370255\n",
            "Epoch 3 Step 57 of 219, loss = 0.04949939448852092\n",
            "Epoch 3 Step 58 of 219, loss = 0.33467586315236986\n",
            "Epoch 3 Step 59 of 219, loss = 0.4783962194342166\n",
            "Epoch 3 Step 60 of 219, loss = 0.8348364908015355\n",
            "Epoch 3 Step 61 of 219, loss = 0.8463474980089813\n",
            "Epoch 3 Step 62 of 219, loss = 0.5790746994316578\n",
            "Epoch 3 Step 63 of 219, loss = 0.37633394997101277\n",
            "Epoch 3 Step 64 of 219, loss = 0.02596021449426189\n",
            "Epoch 3 Step 65 of 219, loss = 0.05443751730490476\n",
            "Epoch 3 Step 66 of 219, loss = 0.4114689943380654\n",
            "Epoch 3 Step 67 of 219, loss = 1.463873352156952\n",
            "Epoch 3 Step 68 of 219, loss = 1.6368813467561267\n",
            "Epoch 3 Step 69 of 219, loss = 0.25875777669716626\n",
            "Epoch 3 Step 70 of 219, loss = 0.26850532117532566\n",
            "Epoch 3 Step 71 of 219, loss = 1.1647453028708696\n",
            "Epoch 3 Step 72 of 219, loss = 0.257971053593792\n",
            "Epoch 3 Step 73 of 219, loss = 0.3419541942421347\n",
            "Epoch 3 Step 74 of 219, loss = 0.3710227869451046\n",
            "Epoch 3 Step 75 of 219, loss = 0.3426330254878849\n",
            "Epoch 3 Step 76 of 219, loss = 0.5567347633186728\n",
            "Epoch 3 Step 77 of 219, loss = 0.6031539035029709\n",
            "Epoch 3 Step 78 of 219, loss = 0.26006693323142827\n",
            "Epoch 3 Step 79 of 219, loss = 0.2815570840612054\n",
            "Epoch 3 Step 80 of 219, loss = 0.39286857564002275\n",
            "Epoch 3 Step 81 of 219, loss = 0.17469130409881473\n",
            "Epoch 3 Step 82 of 219, loss = 0.28152479440905154\n",
            "Epoch 3 Step 83 of 219, loss = 0.60920699685812\n",
            "Epoch 3 Step 84 of 219, loss = 0.11783396382816136\n",
            "Epoch 3 Step 85 of 219, loss = 0.5843893814599141\n",
            "Epoch 3 Step 86 of 219, loss = 0.23954606894403696\n",
            "Epoch 3 Step 87 of 219, loss = 1.0855823555029929\n",
            "Epoch 3 Step 88 of 219, loss = 0.3541302365483716\n",
            "Epoch 3 Step 89 of 219, loss = 1.6417349730618298\n",
            "Epoch 3 Step 90 of 219, loss = 0.18588283844292164\n",
            "Epoch 3 Step 91 of 219, loss = 0.5264378013089299\n",
            "Epoch 3 Step 92 of 219, loss = 0.2580022527836263\n",
            "Epoch 3 Step 93 of 219, loss = 0.6426308782538399\n",
            "Epoch 3 Step 94 of 219, loss = 0.4259059220785275\n",
            "Epoch 3 Step 95 of 219, loss = 0.1789868394844234\n",
            "Epoch 3 Step 96 of 219, loss = 0.561170625500381\n",
            "Epoch 3 Step 97 of 219, loss = 0.1470796747598797\n",
            "Epoch 3 Step 98 of 219, loss = 0.668065195903182\n",
            "Epoch 3 Step 99 of 219, loss = 0.8933514743112028\n",
            "Epoch 3 Step 100 of 219, loss = 0.4667704002931714\n",
            "Epoch 3 Step 101 of 219, loss = 0.7075565280392766\n",
            "Epoch 3 Step 102 of 219, loss = 0.7950908786151558\n",
            "Epoch 3 Step 103 of 219, loss = 0.1611836429219693\n",
            "Epoch 3 Step 104 of 219, loss = 0.33304727158974856\n",
            "Epoch 3 Step 105 of 219, loss = 0.30937396475928836\n",
            "Epoch 3 Step 106 of 219, loss = 0.3099180432036519\n",
            "Epoch 3 Step 107 of 219, loss = 0.39090425660833716\n",
            "Epoch 3 Step 108 of 219, loss = 0.245865126256831\n",
            "Epoch 3 Step 109 of 219, loss = 0.28692883905023336\n",
            "Epoch 3 Step 110 of 219, loss = 0.8610026864334941\n",
            "Epoch 3 Step 111 of 219, loss = 0.6173994906712323\n",
            "Epoch 3 Step 112 of 219, loss = 0.6470037484541535\n",
            "Epoch 3 Step 113 of 219, loss = 0.672625076957047\n",
            "Epoch 3 Step 114 of 219, loss = 0.445573135279119\n",
            "Epoch 3 Step 115 of 219, loss = 0.5286678524280433\n",
            "Epoch 3 Step 116 of 219, loss = 0.8703061897540465\n",
            "Epoch 3 Step 117 of 219, loss = 0.2551860702224076\n",
            "Epoch 3 Step 118 of 219, loss = 0.11652612336911261\n",
            "Epoch 3 Step 119 of 219, loss = 0.41826177784241736\n",
            "Epoch 3 Step 120 of 219, loss = 0.5283522859681398\n",
            "Epoch 3 Step 121 of 219, loss = 0.4538609648589045\n",
            "Epoch 3 Step 122 of 219, loss = 0.44176355050876737\n",
            "Epoch 3 Step 123 of 219, loss = 0.08309851225931197\n",
            "Epoch 3 Step 124 of 219, loss = 0.8815509781707078\n",
            "Epoch 3 Step 125 of 219, loss = 0.20163299003615975\n",
            "Epoch 3 Step 126 of 219, loss = 0.519792559556663\n",
            "Epoch 3 Step 127 of 219, loss = 0.0592603322584182\n",
            "Epoch 3 Step 128 of 219, loss = 0.28425555501598865\n",
            "Epoch 3 Step 129 of 219, loss = 0.8731632451526821\n",
            "Epoch 3 Step 130 of 219, loss = 0.5919308876618743\n",
            "Epoch 3 Step 131 of 219, loss = 0.11104324460029602\n",
            "Epoch 3 Step 132 of 219, loss = 0.3830833784304559\n",
            "Epoch 3 Step 133 of 219, loss = 1.1600992339663208\n",
            "Epoch 3 Step 134 of 219, loss = 1.1611513635143638\n",
            "Epoch 3 Step 135 of 219, loss = 0.8845566282980144\n",
            "Epoch 3 Step 136 of 219, loss = 0.6434727711603045\n",
            "Epoch 3 Step 137 of 219, loss = 1.1773280669003725\n",
            "Epoch 3 Step 138 of 219, loss = 0.8132508797571063\n",
            "Epoch 3 Step 139 of 219, loss = 0.5534144449193263\n",
            "Epoch 3 Step 140 of 219, loss = 0.5755358976311982\n",
            "Epoch 3 Step 141 of 219, loss = 0.5979667380452156\n",
            "Epoch 3 Step 142 of 219, loss = 0.6435618372634053\n",
            "Epoch 3 Step 143 of 219, loss = 0.6567074441118166\n",
            "Epoch 3 Step 144 of 219, loss = 0.6440134062431753\n",
            "Epoch 3 Step 145 of 219, loss = 0.6049071706365794\n",
            "Epoch 3 Step 146 of 219, loss = 0.6439522244036198\n",
            "Epoch 3 Step 147 of 219, loss = 0.639318052562885\n",
            "Epoch 3 Step 148 of 219, loss = 0.641148665570654\n",
            "Epoch 3 Step 149 of 219, loss = 0.6819190010428429\n",
            "Epoch 3 Step 150 of 219, loss = 0.7748845871537924\n",
            "Epoch 3 Step 151 of 219, loss = 0.5667938115075231\n",
            "Epoch 3 Step 152 of 219, loss = 0.5100273378193378\n",
            "Epoch 3 Step 153 of 219, loss = 0.6629797804635018\n",
            "Epoch 3 Step 154 of 219, loss = 0.527918441221118\n",
            "Epoch 3 Step 155 of 219, loss = 0.5060760381165892\n",
            "Epoch 3 Step 156 of 219, loss = 0.45768594555556774\n",
            "Epoch 3 Step 157 of 219, loss = 0.3925990324933082\n",
            "Epoch 3 Step 158 of 219, loss = 0.7423438467085361\n",
            "Epoch 3 Step 159 of 219, loss = 0.7269883276894689\n",
            "Epoch 3 Step 160 of 219, loss = 0.6069037606939673\n",
            "Epoch 3 Step 161 of 219, loss = 0.45255901385098696\n",
            "Epoch 3 Step 162 of 219, loss = 0.4160199062898755\n",
            "Epoch 3 Step 163 of 219, loss = 0.5070326616987586\n",
            "Epoch 3 Step 164 of 219, loss = 0.4747492973692715\n",
            "Epoch 3 Step 165 of 219, loss = 0.8249623109586537\n",
            "Epoch 3 Step 166 of 219, loss = 0.3240333069115877\n",
            "Epoch 3 Step 167 of 219, loss = 1.1887120492756367\n",
            "Epoch 3 Step 168 of 219, loss = 0.6163500645197928\n",
            "Epoch 3 Step 169 of 219, loss = 0.3070024042390287\n",
            "Epoch 3 Step 170 of 219, loss = 0.5210870527662337\n",
            "Epoch 3 Step 171 of 219, loss = 0.5769404792226851\n",
            "Epoch 3 Step 172 of 219, loss = 0.655305992346257\n",
            "Epoch 3 Step 173 of 219, loss = 0.46951612643897533\n",
            "Epoch 3 Step 174 of 219, loss = 0.7327542407438159\n",
            "Epoch 3 Step 175 of 219, loss = 0.8886717250570655\n",
            "Epoch 3 Step 176 of 219, loss = 0.5124851185828447\n",
            "Epoch 3 Step 177 of 219, loss = 0.650441606529057\n",
            "Epoch 3 Step 178 of 219, loss = 0.49262214126065373\n",
            "Epoch 3 Step 179 of 219, loss = 0.41559405811131\n",
            "Epoch 3 Step 180 of 219, loss = 0.5663672483060509\n",
            "Epoch 3 Step 181 of 219, loss = 0.6070718415430747\n",
            "Epoch 3 Step 182 of 219, loss = 0.43026470812037587\n",
            "Epoch 3 Step 183 of 219, loss = 0.41209269501268864\n",
            "Epoch 3 Step 184 of 219, loss = 0.4399613821879029\n",
            "Epoch 3 Step 185 of 219, loss = 0.48103558109141886\n",
            "Epoch 3 Step 186 of 219, loss = 0.5052362000569701\n",
            "Epoch 3 Step 187 of 219, loss = 0.46285501308739185\n",
            "Epoch 3 Step 188 of 219, loss = 0.9286112673580647\n",
            "Epoch 3 Step 189 of 219, loss = 0.3220824517775327\n",
            "Epoch 3 Step 190 of 219, loss = 0.858815505169332\n",
            "Epoch 3 Step 191 of 219, loss = 0.9100368609651923\n",
            "Epoch 3 Step 192 of 219, loss = 0.4961085021495819\n",
            "Epoch 3 Step 193 of 219, loss = 0.33976035425439477\n",
            "Epoch 3 Step 194 of 219, loss = 0.3847846989519894\n",
            "Epoch 3 Step 195 of 219, loss = 0.5851631425321102\n",
            "Epoch 3 Step 196 of 219, loss = 0.46072516590356827\n",
            "Epoch 3 Step 197 of 219, loss = 0.5632450836710632\n",
            "Epoch 3 Step 198 of 219, loss = 0.5100812362506986\n",
            "Epoch 3 Step 199 of 219, loss = 0.6444267705082893\n",
            "Epoch 3 Step 200 of 219, loss = 0.6542947608977556\n",
            "Epoch 3 Step 201 of 219, loss = 0.7731248470954597\n",
            "Epoch 3 Step 202 of 219, loss = 0.5928150848485529\n",
            "Epoch 3 Step 203 of 219, loss = 0.5447067962959409\n",
            "Epoch 3 Step 204 of 219, loss = 0.531064280308783\n",
            "Epoch 3 Step 205 of 219, loss = 0.563808174803853\n",
            "Epoch 3 Step 206 of 219, loss = 0.4845251335646026\n",
            "Epoch 3 Step 207 of 219, loss = 0.721436494961381\n",
            "Epoch 3 Step 208 of 219, loss = 0.5937103843316436\n",
            "Epoch 3 Step 209 of 219, loss = 0.5866023882408626\n",
            "Epoch 3 Step 210 of 219, loss = 0.4243654287420213\n",
            "Epoch 3 Step 211 of 219, loss = 0.7148941019549966\n",
            "Epoch 3 Step 212 of 219, loss = 0.7942554261535406\n",
            "Epoch 3 Step 213 of 219, loss = 0.4950820729136467\n",
            "Epoch 3 Step 214 of 219, loss = 0.5510754743590951\n",
            "Epoch 3 Step 215 of 219, loss = 0.6044786935672164\n",
            "Epoch 3 Step 216 of 219, loss = 0.6243519737618044\n",
            "Epoch 3 Step 217 of 219, loss = 0.5255351723171771\n",
            "Epoch 3 Step 218 of 219, loss = 0.8980572695533434\n",
            "Epoch 3 average train_loss: 0.557099 test_loss: 0.671430 test_score 0.58%\n",
            "Epoch 4 Step 0 of 219, loss = 0.8256035041995347\n",
            "Epoch 4 Step 1 of 219, loss = 0.4174443408846855\n",
            "Epoch 4 Step 2 of 219, loss = 0.6628076797351241\n",
            "Epoch 4 Step 3 of 219, loss = 0.6463473179319408\n",
            "Epoch 4 Step 4 of 219, loss = 0.3720569582656026\n",
            "Epoch 4 Step 5 of 219, loss = 0.19700721045956016\n",
            "Epoch 4 Step 6 of 219, loss = 0.21029788674786687\n",
            "Epoch 4 Step 7 of 219, loss = 0.3925088890828192\n",
            "Epoch 4 Step 8 of 219, loss = 0.3012513577705249\n",
            "Epoch 4 Step 9 of 219, loss = 0.2220170060172677\n",
            "Epoch 4 Step 10 of 219, loss = 0.3278347635641694\n",
            "Epoch 4 Step 11 of 219, loss = 0.09979847847716883\n",
            "Epoch 4 Step 12 of 219, loss = 0.20408524747472256\n",
            "Epoch 4 Step 13 of 219, loss = 0.7961764181964099\n",
            "Epoch 4 Step 14 of 219, loss = 0.39513102907221764\n",
            "Epoch 4 Step 15 of 219, loss = 0.14174707198981196\n",
            "Epoch 4 Step 16 of 219, loss = 0.4958008556277491\n",
            "Epoch 4 Step 17 of 219, loss = 0.8712131129577756\n",
            "Epoch 4 Step 18 of 219, loss = 0.5044117723591626\n",
            "Epoch 4 Step 19 of 219, loss = 0.6986557003110647\n",
            "Epoch 4 Step 20 of 219, loss = 0.4242066550068557\n",
            "Epoch 4 Step 21 of 219, loss = 0.5035101533867419\n",
            "Epoch 4 Step 22 of 219, loss = 0.6824106173589826\n",
            "Epoch 4 Step 23 of 219, loss = 0.5344505645334721\n",
            "Epoch 4 Step 24 of 219, loss = 0.48142153955996037\n",
            "Epoch 4 Step 25 of 219, loss = 0.4046989278867841\n",
            "Epoch 4 Step 26 of 219, loss = 0.5396318631246686\n",
            "Epoch 4 Step 27 of 219, loss = 0.6020487016066909\n",
            "Epoch 4 Step 28 of 219, loss = 0.17004189628642052\n",
            "Epoch 4 Step 29 of 219, loss = 0.5602568173781037\n",
            "Epoch 4 Step 30 of 219, loss = 0.3280085828155279\n",
            "Epoch 4 Step 31 of 219, loss = 0.7879057691898197\n",
            "Epoch 4 Step 32 of 219, loss = 0.35552570573054254\n",
            "Epoch 4 Step 33 of 219, loss = 1.423155305092223\n",
            "Epoch 4 Step 34 of 219, loss = 0.765692827757448\n",
            "Epoch 4 Step 35 of 219, loss = 0.23366907984018326\n",
            "Epoch 4 Step 36 of 219, loss = 0.4018096763174981\n",
            "Epoch 4 Step 37 of 219, loss = 0.38238375075161457\n",
            "Epoch 4 Step 38 of 219, loss = 0.1413522750372067\n",
            "Epoch 4 Step 39 of 219, loss = 0.382287657703273\n",
            "Epoch 4 Step 40 of 219, loss = 0.31991272373124957\n",
            "Epoch 4 Step 41 of 219, loss = 0.2573404680006206\n",
            "Epoch 4 Step 42 of 219, loss = 0.22657962702214718\n",
            "Epoch 4 Step 43 of 219, loss = 0.06411232327809557\n",
            "Epoch 4 Step 44 of 219, loss = 0.03746717167086899\n",
            "Epoch 4 Step 45 of 219, loss = 0.48526933533139527\n",
            "Epoch 4 Step 46 of 219, loss = 0.1614816952351248\n",
            "Epoch 4 Step 47 of 219, loss = 3.0283270019863266\n",
            "Epoch 4 Step 48 of 219, loss = 0.025904439185978845\n",
            "Epoch 4 Step 49 of 219, loss = 2.419246316567296\n",
            "Epoch 4 Step 50 of 219, loss = 0.02174141872819746\n",
            "Epoch 4 Step 51 of 219, loss = 0.0066154069936601445\n",
            "Epoch 4 Step 52 of 219, loss = 0.004909017072350252\n",
            "Epoch 4 Step 53 of 219, loss = 0.3190555931942072\n",
            "Epoch 4 Step 54 of 219, loss = 0.8340467482630629\n",
            "Epoch 4 Step 55 of 219, loss = 0.007483401685021818\n",
            "Epoch 4 Step 56 of 219, loss = 0.26424468102050014\n",
            "Epoch 4 Step 57 of 219, loss = 0.010879014182137325\n",
            "Epoch 4 Step 58 of 219, loss = 0.22269082046113908\n",
            "Epoch 4 Step 59 of 219, loss = 0.35197707590123173\n",
            "Epoch 4 Step 60 of 219, loss = 0.7000431333144661\n",
            "Epoch 4 Step 61 of 219, loss = 0.5291341451520566\n",
            "Epoch 4 Step 62 of 219, loss = 0.40966986121202353\n",
            "Epoch 4 Step 63 of 219, loss = 0.23770258207514416\n",
            "Epoch 4 Step 64 of 219, loss = 0.014261124713812023\n",
            "Epoch 4 Step 65 of 219, loss = 0.018117164523573592\n",
            "Epoch 4 Step 66 of 219, loss = 0.14517253743542824\n",
            "Epoch 4 Step 67 of 219, loss = 0.8821651913458481\n",
            "Epoch 4 Step 68 of 219, loss = 1.1943007349473191\n",
            "Epoch 4 Step 69 of 219, loss = 0.11666819453239441\n",
            "Epoch 4 Step 70 of 219, loss = 0.08144649486348499\n",
            "Epoch 4 Step 71 of 219, loss = 0.5892295379308052\n",
            "Epoch 4 Step 72 of 219, loss = 0.07067444443237036\n",
            "Epoch 4 Step 73 of 219, loss = 0.19869064464000985\n",
            "Epoch 4 Step 74 of 219, loss = 0.13705055019818246\n",
            "Epoch 4 Step 75 of 219, loss = 0.10420611596782692\n",
            "Epoch 4 Step 76 of 219, loss = 0.2167620966793038\n",
            "Epoch 4 Step 77 of 219, loss = 0.19657731676124968\n",
            "Epoch 4 Step 78 of 219, loss = 0.11540335405152291\n",
            "Epoch 4 Step 79 of 219, loss = 0.3345211143605411\n",
            "Epoch 4 Step 80 of 219, loss = 0.10437662919866852\n",
            "Epoch 4 Step 81 of 219, loss = 0.017072690126951784\n",
            "Epoch 4 Step 82 of 219, loss = 0.0507912506145658\n",
            "Epoch 4 Step 83 of 219, loss = 0.6508608074509539\n",
            "Epoch 4 Step 84 of 219, loss = 0.06922782058245502\n",
            "Epoch 4 Step 85 of 219, loss = 0.4200163522618823\n",
            "Epoch 4 Step 86 of 219, loss = 0.2159175787382992\n",
            "Epoch 4 Step 87 of 219, loss = 1.5911892659205478\n",
            "Epoch 4 Step 88 of 219, loss = 0.39861834250041284\n",
            "Epoch 4 Step 89 of 219, loss = 1.313517417642288\n",
            "Epoch 4 Step 90 of 219, loss = 0.0586921809008345\n",
            "Epoch 4 Step 91 of 219, loss = 1.3353807857492939\n",
            "Epoch 4 Step 92 of 219, loss = 0.06389539607334882\n",
            "Epoch 4 Step 93 of 219, loss = 0.4693367494037375\n",
            "Epoch 4 Step 94 of 219, loss = 0.15551070752553642\n",
            "Epoch 4 Step 95 of 219, loss = 0.3114153230562806\n",
            "Epoch 4 Step 96 of 219, loss = 0.4106255683582276\n",
            "Epoch 4 Step 97 of 219, loss = 0.07969256001524627\n",
            "Epoch 4 Step 98 of 219, loss = 0.32543344981968403\n",
            "Epoch 4 Step 99 of 219, loss = 1.228271790634608\n",
            "Epoch 4 Step 100 of 219, loss = 0.1373944969382137\n",
            "Epoch 4 Step 101 of 219, loss = 0.23125546233495697\n",
            "Epoch 4 Step 102 of 219, loss = 0.9935409904574044\n",
            "Epoch 4 Step 103 of 219, loss = 0.02951384277548641\n",
            "Epoch 4 Step 104 of 219, loss = 0.0960156705987174\n",
            "Epoch 4 Step 105 of 219, loss = 0.3298245000478346\n",
            "Epoch 4 Step 106 of 219, loss = 0.2689355559996329\n",
            "Epoch 4 Step 107 of 219, loss = 0.15943263907684013\n",
            "Epoch 4 Step 108 of 219, loss = 0.10572682923520915\n",
            "Epoch 4 Step 109 of 219, loss = 0.13416834201780148\n",
            "Epoch 4 Step 110 of 219, loss = 0.7078520474024117\n",
            "Epoch 4 Step 111 of 219, loss = 0.5424946237471886\n",
            "Epoch 4 Step 112 of 219, loss = 0.6719193472526968\n",
            "Epoch 4 Step 113 of 219, loss = 0.6420890190638602\n",
            "Epoch 4 Step 114 of 219, loss = 0.3392596491612494\n",
            "Epoch 4 Step 115 of 219, loss = 0.5703991965856403\n",
            "Epoch 4 Step 116 of 219, loss = 0.5667000550311059\n",
            "Epoch 4 Step 117 of 219, loss = 0.15459040825953707\n",
            "Epoch 4 Step 118 of 219, loss = 0.049137511450680904\n",
            "Epoch 4 Step 119 of 219, loss = 0.4384256171179004\n",
            "Epoch 4 Step 120 of 219, loss = 0.25546070231939666\n",
            "Epoch 4 Step 121 of 219, loss = 0.10164943616837263\n",
            "Epoch 4 Step 122 of 219, loss = 0.20804790697002318\n",
            "Epoch 4 Step 123 of 219, loss = 0.013371633322094567\n",
            "Epoch 4 Step 124 of 219, loss = 1.0320739856688306\n",
            "Epoch 4 Step 125 of 219, loss = 0.04068642092170194\n",
            "Epoch 4 Step 126 of 219, loss = 0.8649878961150534\n",
            "Epoch 4 Step 127 of 219, loss = 0.00630830941372551\n",
            "Epoch 4 Step 128 of 219, loss = 0.10255728763877414\n",
            "Epoch 4 Step 129 of 219, loss = 0.9558990366349462\n",
            "Epoch 4 Step 130 of 219, loss = 0.5616631652374053\n",
            "Epoch 4 Step 131 of 219, loss = 0.008691486720636021\n",
            "Epoch 4 Step 132 of 219, loss = 0.20078484277473763\n",
            "Epoch 4 Step 133 of 219, loss = 0.9670708152698353\n",
            "Epoch 4 Step 134 of 219, loss = 0.5276090382249095\n",
            "Epoch 4 Step 135 of 219, loss = 0.6433775972691365\n",
            "Epoch 4 Step 136 of 219, loss = 0.29779825307196006\n",
            "Epoch 4 Step 137 of 219, loss = 0.6658750728238374\n",
            "Epoch 4 Step 138 of 219, loss = 0.3619329114444554\n",
            "Epoch 4 Step 139 of 219, loss = 0.8671484780497849\n",
            "Epoch 4 Step 140 of 219, loss = 0.5071361288428307\n",
            "Epoch 4 Step 141 of 219, loss = 0.8033854346722364\n",
            "Epoch 4 Step 142 of 219, loss = 0.48277521692216396\n",
            "Epoch 4 Step 143 of 219, loss = 0.4071380743989721\n",
            "Epoch 4 Step 144 of 219, loss = 0.43081722711212933\n",
            "Epoch 4 Step 145 of 219, loss = 0.35889099072664976\n",
            "Epoch 4 Step 146 of 219, loss = 0.3944670744240284\n",
            "Epoch 4 Step 147 of 219, loss = 0.5556004990357906\n",
            "Epoch 4 Step 148 of 219, loss = 0.3468474471010268\n",
            "Epoch 4 Step 149 of 219, loss = 0.5184144619852304\n",
            "Epoch 4 Step 150 of 219, loss = 0.8286824128590524\n",
            "Epoch 4 Step 151 of 219, loss = 0.292810907587409\n",
            "Epoch 4 Step 152 of 219, loss = 0.2543806442990899\n",
            "Epoch 4 Step 153 of 219, loss = 0.5060611828230321\n",
            "Epoch 4 Step 154 of 219, loss = 0.2243542312644422\n",
            "Epoch 4 Step 155 of 219, loss = 0.34388218680396676\n",
            "Epoch 4 Step 156 of 219, loss = 0.3082620375789702\n",
            "Epoch 4 Step 157 of 219, loss = 0.1324737723916769\n",
            "Epoch 4 Step 158 of 219, loss = 0.4579409318976104\n",
            "Epoch 4 Step 159 of 219, loss = 0.6039348861668259\n",
            "Epoch 4 Step 160 of 219, loss = 0.42858054186217487\n",
            "Epoch 4 Step 161 of 219, loss = 0.4973872983828187\n",
            "Epoch 4 Step 162 of 219, loss = 0.08670564880594611\n",
            "Epoch 4 Step 163 of 219, loss = 0.34493821463547647\n",
            "Epoch 4 Step 164 of 219, loss = 0.10401161701884121\n",
            "Epoch 4 Step 165 of 219, loss = 0.8107386328047141\n",
            "Epoch 4 Step 166 of 219, loss = 0.26418104593176395\n",
            "Epoch 4 Step 167 of 219, loss = 1.4491380179533735\n",
            "Epoch 4 Step 168 of 219, loss = 0.6479941533179954\n",
            "Epoch 4 Step 169 of 219, loss = 0.13965932809514925\n",
            "Epoch 4 Step 170 of 219, loss = 0.578450657587382\n",
            "Epoch 4 Step 171 of 219, loss = 0.6793162766261958\n",
            "Epoch 4 Step 172 of 219, loss = 0.876395654049702\n",
            "Epoch 4 Step 173 of 219, loss = 0.21817167848348618\n",
            "Epoch 4 Step 174 of 219, loss = 0.24833815079182386\n",
            "Epoch 4 Step 175 of 219, loss = 0.7109997244551778\n",
            "Epoch 4 Step 176 of 219, loss = 0.41958920925389975\n",
            "Epoch 4 Step 177 of 219, loss = 0.6638864722335711\n",
            "Epoch 4 Step 178 of 219, loss = 0.43178338510915637\n",
            "Epoch 4 Step 179 of 219, loss = 0.14716530148871243\n",
            "Epoch 4 Step 180 of 219, loss = 0.6586861288524233\n",
            "Epoch 4 Step 181 of 219, loss = 0.46132826048415154\n",
            "Epoch 4 Step 182 of 219, loss = 0.3349722158163786\n",
            "Epoch 4 Step 183 of 219, loss = 0.2855529240332544\n",
            "Epoch 4 Step 184 of 219, loss = 0.4236411324236542\n",
            "Epoch 4 Step 185 of 219, loss = 0.3782851742580533\n",
            "Epoch 4 Step 186 of 219, loss = 0.41161252185702324\n",
            "Epoch 4 Step 187 of 219, loss = 0.39152129413560033\n",
            "Epoch 4 Step 188 of 219, loss = 0.7824760312214494\n",
            "Epoch 4 Step 189 of 219, loss = 0.14660995220765471\n",
            "Epoch 4 Step 190 of 219, loss = 0.7514533910434693\n",
            "Epoch 4 Step 191 of 219, loss = 0.863018658477813\n",
            "Epoch 4 Step 192 of 219, loss = 0.36562753841280937\n",
            "Epoch 4 Step 193 of 219, loss = 0.3259158981963992\n",
            "Epoch 4 Step 194 of 219, loss = 0.300220618955791\n",
            "Epoch 4 Step 195 of 219, loss = 0.44568241853266954\n",
            "Epoch 4 Step 196 of 219, loss = 0.34984856517985463\n",
            "Epoch 4 Step 197 of 219, loss = 0.45355098182335496\n",
            "Epoch 4 Step 198 of 219, loss = 0.25149984401650727\n",
            "Epoch 4 Step 199 of 219, loss = 0.41157475067302585\n",
            "Epoch 4 Step 200 of 219, loss = 0.5176990570034832\n",
            "Epoch 4 Step 201 of 219, loss = 0.6593593652360141\n",
            "Epoch 4 Step 202 of 219, loss = 0.26525380788370967\n",
            "Epoch 4 Step 203 of 219, loss = 0.44300617498811334\n",
            "Epoch 4 Step 204 of 219, loss = 0.2809806279838085\n",
            "Epoch 4 Step 205 of 219, loss = 0.21528570377267897\n",
            "Epoch 4 Step 206 of 219, loss = 0.36882091488223523\n",
            "Epoch 4 Step 207 of 219, loss = 0.6945534239057451\n",
            "Epoch 4 Step 208 of 219, loss = 0.6062594875693321\n",
            "Epoch 4 Step 209 of 219, loss = 0.7267948819207959\n",
            "Epoch 4 Step 210 of 219, loss = 0.6736509392503649\n",
            "Epoch 4 Step 211 of 219, loss = 0.6005578299518675\n",
            "Epoch 4 Step 212 of 219, loss = 0.896793941501528\n",
            "Epoch 4 Step 213 of 219, loss = 0.2904855259694159\n",
            "Epoch 4 Step 214 of 219, loss = 0.5362010021926835\n",
            "Epoch 4 Step 215 of 219, loss = 0.2923214298207313\n",
            "Epoch 4 Step 216 of 219, loss = 0.3514313622727059\n",
            "Epoch 4 Step 217 of 219, loss = 0.39968286268413067\n",
            "Epoch 4 Step 218 of 219, loss = 0.6084992024116218\n",
            "Epoch 4 average train_loss: 0.438793 test_loss: 1.054061 test_score 0.55%\n",
            "Epoch 5 Step 0 of 219, loss = 0.352995942812413\n",
            "Epoch 5 Step 1 of 219, loss = 0.06729032122530043\n",
            "Epoch 5 Step 2 of 219, loss = 0.43831736815627664\n",
            "Epoch 5 Step 3 of 219, loss = 0.4497473464580253\n",
            "Epoch 5 Step 4 of 219, loss = 0.1660065577016212\n",
            "Epoch 5 Step 5 of 219, loss = 0.026564183965092525\n",
            "Epoch 5 Step 6 of 219, loss = 0.07707720555481501\n",
            "Epoch 5 Step 7 of 219, loss = 0.0698639846232254\n",
            "Epoch 5 Step 8 of 219, loss = 0.15026561086415313\n",
            "Epoch 5 Step 9 of 219, loss = 0.04797129952930845\n",
            "Epoch 5 Step 10 of 219, loss = 0.2522032145934645\n",
            "Epoch 5 Step 11 of 219, loss = 0.017938392295036465\n",
            "Epoch 5 Step 12 of 219, loss = 0.0372468379791826\n",
            "Epoch 5 Step 13 of 219, loss = 0.3033334006322548\n",
            "Epoch 5 Step 14 of 219, loss = 0.24384478641150054\n",
            "Epoch 5 Step 15 of 219, loss = 0.032440512484754436\n",
            "Epoch 5 Step 16 of 219, loss = 0.5088187350775115\n",
            "Epoch 5 Step 17 of 219, loss = 0.13473433710169047\n",
            "Epoch 5 Step 18 of 219, loss = 1.1984553867951035\n",
            "Epoch 5 Step 19 of 219, loss = 0.4828893356025219\n",
            "Epoch 5 Step 20 of 219, loss = 0.18397766139241867\n",
            "Epoch 5 Step 21 of 219, loss = 0.3109461762942374\n",
            "Epoch 5 Step 22 of 219, loss = 0.490648090839386\n",
            "Epoch 5 Step 23 of 219, loss = 0.4572792153339833\n",
            "Epoch 5 Step 24 of 219, loss = 0.4216885823989287\n",
            "Epoch 5 Step 25 of 219, loss = 0.9545141653361497\n",
            "Epoch 5 Step 26 of 219, loss = 0.26234265232051257\n",
            "Epoch 5 Step 27 of 219, loss = 0.6902956647099927\n",
            "Epoch 5 Step 28 of 219, loss = 0.12753788291593082\n",
            "Epoch 5 Step 29 of 219, loss = 0.11748440569499508\n",
            "Epoch 5 Step 30 of 219, loss = 0.09219962306087837\n",
            "Epoch 5 Step 31 of 219, loss = 0.9022747490089387\n",
            "Epoch 5 Step 32 of 219, loss = 0.02596364272176288\n",
            "Epoch 5 Step 33 of 219, loss = 1.0352627693064278\n",
            "Epoch 5 Step 34 of 219, loss = 0.5271257227286696\n",
            "Epoch 5 Step 35 of 219, loss = 0.277483903511893\n",
            "Epoch 5 Step 36 of 219, loss = 0.16388694994384423\n",
            "Epoch 5 Step 37 of 219, loss = 0.4380359867209336\n",
            "Epoch 5 Step 38 of 219, loss = 0.009289671972510405\n",
            "Epoch 5 Step 39 of 219, loss = 0.49685211954056285\n",
            "Epoch 5 Step 40 of 219, loss = 0.04122141483821906\n",
            "Epoch 5 Step 41 of 219, loss = 0.12597565846226644\n",
            "Epoch 5 Step 42 of 219, loss = 0.041907186605385505\n",
            "Epoch 5 Step 43 of 219, loss = 0.004749839099531528\n",
            "Epoch 5 Step 44 of 219, loss = 0.007209363524452783\n",
            "Epoch 5 Step 45 of 219, loss = 0.29322377838980174\n",
            "Epoch 5 Step 46 of 219, loss = 0.039139691383752506\n",
            "Epoch 5 Step 47 of 219, loss = 2.58975936081697\n",
            "Epoch 5 Step 48 of 219, loss = 0.007264989137183875\n",
            "Epoch 5 Step 49 of 219, loss = 2.6258762595971348\n",
            "Epoch 5 Step 50 of 219, loss = 0.007781288135447539\n",
            "Epoch 5 Step 51 of 219, loss = 0.002192175557865994\n",
            "Epoch 5 Step 52 of 219, loss = 0.00341695046517998\n",
            "Epoch 5 Step 53 of 219, loss = 0.11051861085434211\n",
            "Epoch 5 Step 54 of 219, loss = 0.15674905409832718\n",
            "Epoch 5 Step 55 of 219, loss = 0.009204475827573333\n",
            "Epoch 5 Step 56 of 219, loss = 0.05804507298307726\n",
            "Epoch 5 Step 57 of 219, loss = 0.0105317559937248\n",
            "Epoch 5 Step 58 of 219, loss = 0.22449252004298614\n",
            "Epoch 5 Step 59 of 219, loss = 0.1769030552895856\n",
            "Epoch 5 Step 60 of 219, loss = 0.19992415244632866\n",
            "Epoch 5 Step 61 of 219, loss = 0.09563466015970334\n",
            "Epoch 5 Step 62 of 219, loss = 0.6291676168111735\n",
            "Epoch 5 Step 63 of 219, loss = 0.028475629223976284\n",
            "Epoch 5 Step 64 of 219, loss = 0.0041808787354966626\n",
            "Epoch 5 Step 65 of 219, loss = 0.0070271285803755745\n",
            "Epoch 5 Step 66 of 219, loss = 0.019516980049957056\n",
            "Epoch 5 Step 67 of 219, loss = 0.242250400071498\n",
            "Epoch 5 Step 68 of 219, loss = 0.9325794565520482\n",
            "Epoch 5 Step 69 of 219, loss = 0.011745182826416567\n",
            "Epoch 5 Step 70 of 219, loss = 0.047996442364819814\n",
            "Epoch 5 Step 71 of 219, loss = 0.5236704158305656\n",
            "Epoch 5 Step 72 of 219, loss = 0.026712158069130965\n",
            "Epoch 5 Step 73 of 219, loss = 0.19671047461451963\n",
            "Epoch 5 Step 74 of 219, loss = 0.15081560335238464\n",
            "Epoch 5 Step 75 of 219, loss = 0.0913836469699163\n",
            "Epoch 5 Step 76 of 219, loss = 0.025313676364021376\n",
            "Epoch 5 Step 77 of 219, loss = 0.03763290774077177\n",
            "Epoch 5 Step 78 of 219, loss = 0.15234028483973816\n",
            "Epoch 5 Step 79 of 219, loss = 0.011618051954428665\n",
            "Epoch 5 Step 80 of 219, loss = 0.01289888428436825\n",
            "Epoch 5 Step 81 of 219, loss = 0.0023612802651769016\n",
            "Epoch 5 Step 82 of 219, loss = 0.003771988303924445\n",
            "Epoch 5 Step 83 of 219, loss = 0.13273509177815868\n",
            "Epoch 5 Step 84 of 219, loss = 0.003357450379553484\n",
            "Epoch 5 Step 85 of 219, loss = 0.24348519966588356\n",
            "Epoch 5 Step 86 of 219, loss = 0.005978082563160569\n",
            "Epoch 5 Step 87 of 219, loss = 2.9809729323715146\n",
            "Epoch 5 Step 88 of 219, loss = 0.40839481242073816\n",
            "Epoch 5 Step 89 of 219, loss = 2.663185400866496\n",
            "Epoch 5 Step 90 of 219, loss = 0.2795217883758596\n",
            "Epoch 5 Step 91 of 219, loss = 2.1395601034419087\n",
            "Epoch 5 Step 92 of 219, loss = 0.008792234151769662\n",
            "Epoch 5 Step 93 of 219, loss = 0.07782361883437261\n",
            "Epoch 5 Step 94 of 219, loss = 0.025630983869632473\n",
            "Epoch 5 Step 95 of 219, loss = 0.27808019896110636\n",
            "Epoch 5 Step 96 of 219, loss = 0.03571257964358665\n",
            "Epoch 5 Step 97 of 219, loss = 0.026049618929391727\n",
            "Epoch 5 Step 98 of 219, loss = 0.5514679905463709\n",
            "Epoch 5 Step 99 of 219, loss = 1.1140577340265736\n",
            "Epoch 5 Step 100 of 219, loss = 0.02604027499910444\n",
            "Epoch 5 Step 101 of 219, loss = 0.23796989195398055\n",
            "Epoch 5 Step 102 of 219, loss = 0.37049493519589305\n",
            "Epoch 5 Step 103 of 219, loss = 0.00805112873786129\n",
            "Epoch 5 Step 104 of 219, loss = 0.0074302608409198\n",
            "Epoch 5 Step 105 of 219, loss = 0.006628360930335475\n",
            "Epoch 5 Step 106 of 219, loss = 0.6624047316072392\n",
            "Epoch 5 Step 107 of 219, loss = 0.06441869095579023\n",
            "Epoch 5 Step 108 of 219, loss = 0.021619671111693606\n",
            "Epoch 5 Step 109 of 219, loss = 0.5376131007869844\n",
            "Epoch 5 Step 110 of 219, loss = 1.0278468557226006\n",
            "Epoch 5 Step 111 of 219, loss = 0.4715214698226191\n",
            "Epoch 5 Step 112 of 219, loss = 0.25963564741141454\n",
            "Epoch 5 Step 113 of 219, loss = 0.12780277454294264\n",
            "Epoch 5 Step 114 of 219, loss = 0.08450647391146049\n",
            "Epoch 5 Step 115 of 219, loss = 0.5133688342757523\n",
            "Epoch 5 Step 116 of 219, loss = 0.6643324479227886\n",
            "Epoch 5 Step 117 of 219, loss = 0.427915932261385\n",
            "Epoch 5 Step 118 of 219, loss = 0.029947432543849573\n",
            "Epoch 5 Step 119 of 219, loss = 0.07149249565554783\n",
            "Epoch 5 Step 120 of 219, loss = 0.057669115500175394\n",
            "Epoch 5 Step 121 of 219, loss = 0.017083186612580903\n",
            "Epoch 5 Step 122 of 219, loss = 0.019006220405572094\n",
            "Epoch 5 Step 123 of 219, loss = 0.0025696815273477114\n",
            "Epoch 5 Step 124 of 219, loss = 0.9217552198551857\n",
            "Epoch 5 Step 125 of 219, loss = 0.009109244732826483\n",
            "Epoch 5 Step 126 of 219, loss = 0.6958870626858698\n",
            "Epoch 5 Step 127 of 219, loss = 0.0005717185604225961\n",
            "Epoch 5 Step 128 of 219, loss = 0.05406832603694056\n",
            "Epoch 5 Step 129 of 219, loss = 1.3728207034491788\n",
            "Epoch 5 Step 130 of 219, loss = 0.8859204103018783\n",
            "Epoch 5 Step 131 of 219, loss = 0.029023808565398213\n",
            "Epoch 5 Step 132 of 219, loss = 0.05464171657877159\n",
            "Epoch 5 Step 133 of 219, loss = 0.5973306098603643\n",
            "Epoch 5 Step 134 of 219, loss = 0.3419458331045462\n",
            "Epoch 5 Step 135 of 219, loss = 0.07802922442351701\n",
            "Epoch 5 Step 136 of 219, loss = 0.2588994631660171\n",
            "Epoch 5 Step 137 of 219, loss = 0.6753623997210525\n",
            "Epoch 5 Step 138 of 219, loss = 0.2960978606133722\n",
            "Epoch 5 Step 139 of 219, loss = 0.022644906348432414\n",
            "Epoch 5 Step 140 of 219, loss = 0.6817067940282868\n",
            "Epoch 5 Step 141 of 219, loss = 0.6845559734501876\n",
            "Epoch 5 Step 142 of 219, loss = 0.13989879592554644\n",
            "Epoch 5 Step 143 of 219, loss = 0.12438547550118528\n",
            "Epoch 5 Step 144 of 219, loss = 0.3012016558495816\n",
            "Epoch 5 Step 145 of 219, loss = 0.09772798547055572\n",
            "Epoch 5 Step 146 of 219, loss = 0.4061180936405435\n",
            "Epoch 5 Step 147 of 219, loss = 0.3437646225793287\n",
            "Epoch 5 Step 148 of 219, loss = 0.12727780971908942\n",
            "Epoch 5 Step 149 of 219, loss = 0.6447022937936708\n",
            "Epoch 5 Step 150 of 219, loss = 0.9384208056144416\n",
            "Epoch 5 Step 151 of 219, loss = 0.1628257260308601\n",
            "Epoch 5 Step 152 of 219, loss = 0.06826798338443041\n",
            "Epoch 5 Step 153 of 219, loss = 0.6105855056084692\n",
            "Epoch 5 Step 154 of 219, loss = 0.20350816991413012\n",
            "Epoch 5 Step 155 of 219, loss = 0.14802820759359747\n",
            "Epoch 5 Step 156 of 219, loss = 0.12404203592450358\n",
            "Epoch 5 Step 157 of 219, loss = 0.07534269453026354\n",
            "Epoch 5 Step 158 of 219, loss = 0.2939217898529023\n",
            "Epoch 5 Step 159 of 219, loss = 0.38711413461714983\n",
            "Epoch 5 Step 160 of 219, loss = 0.20143277116585523\n",
            "Epoch 5 Step 161 of 219, loss = 0.16141409170813859\n",
            "Epoch 5 Step 162 of 219, loss = 0.17107563398894854\n",
            "Epoch 5 Step 163 of 219, loss = 0.6478752281691413\n",
            "Epoch 5 Step 164 of 219, loss = 0.2777415738091804\n",
            "Epoch 5 Step 165 of 219, loss = 0.30873139563482255\n",
            "Epoch 5 Step 166 of 219, loss = 0.041505205299472436\n",
            "Epoch 5 Step 167 of 219, loss = 0.5766533869027626\n",
            "Epoch 5 Step 168 of 219, loss = 0.7508784894598648\n",
            "Epoch 5 Step 169 of 219, loss = 0.07082270833780058\n",
            "Epoch 5 Step 170 of 219, loss = 0.06057805029558949\n",
            "Epoch 5 Step 171 of 219, loss = 0.9176634363248013\n",
            "Epoch 5 Step 172 of 219, loss = 0.6486003374448046\n",
            "Epoch 5 Step 173 of 219, loss = 0.024912144377594814\n",
            "Epoch 5 Step 174 of 219, loss = 0.0409111678309273\n",
            "Epoch 5 Step 175 of 219, loss = 0.8625889295071829\n",
            "Epoch 5 Step 176 of 219, loss = 0.4657450790691655\n",
            "Epoch 5 Step 177 of 219, loss = 0.3056414144812152\n",
            "Epoch 5 Step 178 of 219, loss = 0.6827376695291605\n",
            "Epoch 5 Step 179 of 219, loss = 0.02540436874551233\n",
            "Epoch 5 Step 180 of 219, loss = 0.533992897340795\n",
            "Epoch 5 Step 181 of 219, loss = 0.462382302939659\n",
            "Epoch 5 Step 182 of 219, loss = 0.015434627013746649\n",
            "Epoch 5 Step 183 of 219, loss = 0.2720694261515746\n",
            "Epoch 5 Step 184 of 219, loss = 0.7536858356324956\n",
            "Epoch 5 Step 185 of 219, loss = 0.4904244041244965\n",
            "Epoch 5 Step 186 of 219, loss = 0.832057905732654\n",
            "Epoch 5 Step 187 of 219, loss = 0.13307991008332465\n",
            "Epoch 5 Step 188 of 219, loss = 1.5524326360027771\n",
            "Epoch 5 Step 189 of 219, loss = 0.028333137001027353\n",
            "Epoch 5 Step 190 of 219, loss = 0.2511869575828314\n",
            "Epoch 5 Step 191 of 219, loss = 0.6635637632571161\n",
            "Epoch 5 Step 192 of 219, loss = 0.12871262372937053\n",
            "Epoch 5 Step 193 of 219, loss = 0.18351583142066374\n",
            "Epoch 5 Step 194 of 219, loss = 0.1529776219977066\n",
            "Epoch 5 Step 195 of 219, loss = 0.6134457192383707\n",
            "Epoch 5 Step 196 of 219, loss = 0.09090701263630763\n",
            "Epoch 5 Step 197 of 219, loss = 0.3315038183936849\n",
            "Epoch 5 Step 198 of 219, loss = 0.1601746758678928\n",
            "Epoch 5 Step 199 of 219, loss = 0.3322541599627584\n",
            "Epoch 5 Step 200 of 219, loss = 0.4023984911618754\n",
            "Epoch 5 Step 201 of 219, loss = 0.27047716960078105\n",
            "Epoch 5 Step 202 of 219, loss = 0.13652298133820295\n",
            "Epoch 5 Step 203 of 219, loss = 0.17935899319127202\n",
            "Epoch 5 Step 204 of 219, loss = 0.16604890857706778\n",
            "Epoch 5 Step 205 of 219, loss = 0.09245530515909195\n",
            "Epoch 5 Step 206 of 219, loss = 0.12001339686685242\n",
            "Epoch 5 Step 207 of 219, loss = 0.19251294992864132\n",
            "Epoch 5 Step 208 of 219, loss = 0.14141953544458374\n",
            "Epoch 5 Step 209 of 219, loss = 0.2235674076655414\n",
            "Epoch 5 Step 210 of 219, loss = 0.4765689902123995\n",
            "Epoch 5 Step 211 of 219, loss = 0.053528685501078144\n",
            "Epoch 5 Step 212 of 219, loss = 0.6046799480100162\n",
            "Epoch 5 Step 213 of 219, loss = 0.06700451869983226\n",
            "Epoch 5 Step 214 of 219, loss = 0.43497454348835163\n",
            "Epoch 5 Step 215 of 219, loss = 0.4299409512314014\n",
            "Epoch 5 Step 216 of 219, loss = 0.10553898441139609\n",
            "Epoch 5 Step 217 of 219, loss = 0.5916204377426766\n",
            "Epoch 5 Step 218 of 219, loss = 0.6740547781810164\n",
            "Epoch 5 average train_loss: 0.334111 test_loss: 1.916100 test_score 0.53%\n",
            "Epoch 6 Step 0 of 219, loss = 0.10004302000743337\n",
            "Epoch 6 Step 1 of 219, loss = 0.007697745488258079\n",
            "Epoch 6 Step 2 of 219, loss = 0.10489391104783863\n",
            "Epoch 6 Step 3 of 219, loss = 0.18770512100309134\n",
            "Epoch 6 Step 4 of 219, loss = 0.023262446127773728\n",
            "Epoch 6 Step 5 of 219, loss = 0.004936574288876727\n",
            "Epoch 6 Step 6 of 219, loss = 0.006874014419736341\n",
            "Epoch 6 Step 7 of 219, loss = 0.009106526806135662\n",
            "Epoch 6 Step 8 of 219, loss = 0.01670102257776307\n",
            "Epoch 6 Step 9 of 219, loss = 0.005191521115193609\n",
            "Epoch 6 Step 10 of 219, loss = 0.0496614604453498\n",
            "Epoch 6 Step 11 of 219, loss = 0.0019074621486652177\n",
            "Epoch 6 Step 12 of 219, loss = 0.012786374838469783\n",
            "Epoch 6 Step 13 of 219, loss = 0.4414699049812043\n",
            "Epoch 6 Step 14 of 219, loss = 0.2528568444904522\n",
            "Epoch 6 Step 15 of 219, loss = 0.00156479253564612\n",
            "Epoch 6 Step 16 of 219, loss = 0.09662578752613626\n",
            "Epoch 6 Step 17 of 219, loss = 0.9077120102665504\n",
            "Epoch 6 Step 18 of 219, loss = 1.88691409965395\n",
            "Epoch 6 Step 19 of 219, loss = 0.07234553292801138\n",
            "Epoch 6 Step 20 of 219, loss = 0.020276839357393328\n",
            "Epoch 6 Step 21 of 219, loss = 0.17261660275107715\n",
            "Epoch 6 Step 22 of 219, loss = 0.2726167269574944\n",
            "Epoch 6 Step 23 of 219, loss = 0.48711794189875945\n",
            "Epoch 6 Step 24 of 219, loss = 0.02270575865986757\n",
            "Epoch 6 Step 25 of 219, loss = 0.6707878302477184\n",
            "Epoch 6 Step 26 of 219, loss = 0.03838851084583439\n",
            "Epoch 6 Step 27 of 219, loss = 0.2671868493380316\n",
            "Epoch 6 Step 28 of 219, loss = 0.004361775092547759\n",
            "Epoch 6 Step 29 of 219, loss = 0.6603894856889383\n",
            "Epoch 6 Step 30 of 219, loss = 0.012197037191072013\n",
            "Epoch 6 Step 31 of 219, loss = 1.1002140402124496\n",
            "Epoch 6 Step 32 of 219, loss = 0.0037902978583588265\n",
            "Epoch 6 Step 33 of 219, loss = 2.531734298710944\n",
            "Epoch 6 Step 34 of 219, loss = 1.266154711447598\n",
            "Epoch 6 Step 35 of 219, loss = 0.008225139194109943\n",
            "Epoch 6 Step 36 of 219, loss = 0.06314148300589295\n",
            "Epoch 6 Step 37 of 219, loss = 0.023505032346292865\n",
            "Epoch 6 Step 38 of 219, loss = 0.002733672239628504\n",
            "Epoch 6 Step 39 of 219, loss = 0.0120768372216844\n",
            "Epoch 6 Step 40 of 219, loss = 0.3561246675963048\n",
            "Epoch 6 Step 41 of 219, loss = 0.02770602182499715\n",
            "Epoch 6 Step 42 of 219, loss = 0.021584811413049465\n",
            "Epoch 6 Step 43 of 219, loss = 0.0018688535237743054\n",
            "Epoch 6 Step 44 of 219, loss = 0.0037748575377918314\n",
            "Epoch 6 Step 45 of 219, loss = 0.6525899892403686\n",
            "Epoch 6 Step 46 of 219, loss = 0.12584237803821452\n",
            "Epoch 6 Step 47 of 219, loss = 2.057888435483619\n",
            "Epoch 6 Step 48 of 219, loss = 0.0012276149282115512\n",
            "Epoch 6 Step 49 of 219, loss = 2.58586746266883\n",
            "Epoch 6 Step 50 of 219, loss = 0.003054534401599085\n",
            "Epoch 6 Step 51 of 219, loss = 0.002878926181438146\n",
            "Epoch 6 Step 52 of 219, loss = 0.003953313425881788\n",
            "Epoch 6 Step 53 of 219, loss = 0.009463846943617682\n",
            "Epoch 6 Step 54 of 219, loss = 0.2999710309522925\n",
            "Epoch 6 Step 55 of 219, loss = 0.0017099806500482373\n",
            "Epoch 6 Step 56 of 219, loss = 0.00437296488598804\n",
            "Epoch 6 Step 57 of 219, loss = 0.0034060854704875965\n",
            "Epoch 6 Step 58 of 219, loss = 0.007797433943778742\n",
            "Epoch 6 Step 59 of 219, loss = 0.005829566114698537\n",
            "Epoch 6 Step 60 of 219, loss = 0.3176320911516086\n",
            "Epoch 6 Step 61 of 219, loss = 0.6931524484934926\n",
            "Epoch 6 Step 62 of 219, loss = 0.46506795235472964\n",
            "Epoch 6 Step 63 of 219, loss = 0.002379596455284627\n",
            "Epoch 6 Step 64 of 219, loss = 0.0017636832089920063\n",
            "Epoch 6 Step 65 of 219, loss = 0.0019150832704326604\n",
            "Epoch 6 Step 66 of 219, loss = 0.002694143558983342\n",
            "Epoch 6 Step 67 of 219, loss = 0.5547229882176907\n",
            "Epoch 6 Step 68 of 219, loss = 0.9410000230363949\n",
            "Epoch 6 Step 69 of 219, loss = 0.11604274790443014\n",
            "Epoch 6 Step 70 of 219, loss = 0.0028847311332356185\n",
            "Epoch 6 Step 71 of 219, loss = 0.7277552735540667\n",
            "Epoch 6 Step 72 of 219, loss = 0.0030600984828197397\n",
            "Epoch 6 Step 73 of 219, loss = 0.01933597817333066\n",
            "Epoch 6 Step 74 of 219, loss = 0.30752366232627537\n",
            "Epoch 6 Step 75 of 219, loss = 0.004138178424909711\n",
            "Epoch 6 Step 76 of 219, loss = 0.004064645785547327\n",
            "Epoch 6 Step 77 of 219, loss = 0.006167955907585565\n",
            "Epoch 6 Step 78 of 219, loss = 0.6110875007325376\n",
            "Epoch 6 Step 79 of 219, loss = 0.357116919803957\n",
            "Epoch 6 Step 80 of 219, loss = 0.539663796495006\n",
            "Epoch 6 Step 81 of 219, loss = 0.39385561148810666\n",
            "Epoch 6 Step 82 of 219, loss = 0.5105731796138571\n",
            "Epoch 6 Step 83 of 219, loss = 0.060707592114340514\n",
            "Epoch 6 Step 84 of 219, loss = 0.004296725059248274\n",
            "Epoch 6 Step 85 of 219, loss = 0.006761511496733874\n",
            "Epoch 6 Step 86 of 219, loss = 0.005739191456086701\n",
            "Epoch 6 Step 87 of 219, loss = 1.217016984039219\n",
            "Epoch 6 Step 88 of 219, loss = 0.0907953297792119\n",
            "Epoch 6 Step 89 of 219, loss = 1.5941364765567414\n",
            "Epoch 6 Step 90 of 219, loss = 0.011006882865331136\n",
            "Epoch 6 Step 91 of 219, loss = 0.22927468133275397\n",
            "Epoch 6 Step 92 of 219, loss = 0.07986915409128414\n",
            "Epoch 6 Step 93 of 219, loss = 0.7423400504048914\n",
            "Epoch 6 Step 94 of 219, loss = 0.42848562249855604\n",
            "Epoch 6 Step 95 of 219, loss = 0.010339830099837855\n",
            "Epoch 6 Step 96 of 219, loss = 0.41497728366812225\n",
            "Epoch 6 Step 97 of 219, loss = 0.006201163887453731\n",
            "Epoch 6 Step 98 of 219, loss = 0.20546104512322927\n",
            "Epoch 6 Step 99 of 219, loss = 0.5899907882703701\n",
            "Epoch 6 Step 100 of 219, loss = 0.15070260892389342\n",
            "Epoch 6 Step 101 of 219, loss = 0.01734233998286072\n",
            "Epoch 6 Step 102 of 219, loss = 0.6792308075528126\n",
            "Epoch 6 Step 103 of 219, loss = 0.00446520114201121\n",
            "Epoch 6 Step 104 of 219, loss = 0.01025040345666639\n",
            "Epoch 6 Step 105 of 219, loss = 0.006872779795230599\n",
            "Epoch 6 Step 106 of 219, loss = 0.03583592319773743\n",
            "Epoch 6 Step 107 of 219, loss = 0.00186797358765034\n",
            "Epoch 6 Step 108 of 219, loss = 0.03019933666655561\n",
            "Epoch 6 Step 109 of 219, loss = 0.8614988649096631\n",
            "Epoch 6 Step 110 of 219, loss = 1.313914172395016\n",
            "Epoch 6 Step 111 of 219, loss = 0.0424962876313657\n",
            "Epoch 6 Step 112 of 219, loss = 0.1131361835905409\n",
            "Epoch 6 Step 113 of 219, loss = 0.6080340468579379\n",
            "Epoch 6 Step 114 of 219, loss = 0.4670195103244623\n",
            "Epoch 6 Step 115 of 219, loss = 0.339091435627779\n",
            "Epoch 6 Step 116 of 219, loss = 0.21025665888737421\n",
            "Epoch 6 Step 117 of 219, loss = 0.06110249824450875\n",
            "Epoch 6 Step 118 of 219, loss = 0.08980305171280634\n",
            "Epoch 6 Step 119 of 219, loss = 0.7126189737173263\n",
            "Epoch 6 Step 120 of 219, loss = 0.14513999721384607\n",
            "Epoch 6 Step 121 of 219, loss = 0.06056957315195177\n",
            "Epoch 6 Step 122 of 219, loss = 0.11480073486745823\n",
            "Epoch 6 Step 123 of 219, loss = 0.009120845592406113\n",
            "Epoch 6 Step 124 of 219, loss = 0.10267487035889644\n",
            "Epoch 6 Step 125 of 219, loss = 0.005653213884215802\n",
            "Epoch 6 Step 126 of 219, loss = 0.44060010344037437\n",
            "Epoch 6 Step 127 of 219, loss = 0.0009308699909524876\n",
            "Epoch 6 Step 128 of 219, loss = 0.0020164033430773998\n",
            "Epoch 6 Step 129 of 219, loss = 0.7742676779362228\n",
            "Epoch 6 Step 130 of 219, loss = 0.5374386216099083\n",
            "Epoch 6 Step 131 of 219, loss = 0.10713130393378378\n",
            "Epoch 6 Step 132 of 219, loss = 0.011814559624326648\n",
            "Epoch 6 Step 133 of 219, loss = 0.09870914217026439\n",
            "Epoch 6 Step 134 of 219, loss = 0.8864719649809558\n",
            "Epoch 6 Step 135 of 219, loss = 0.3582024566203472\n",
            "Epoch 6 Step 136 of 219, loss = 0.05583922769437777\n",
            "Epoch 6 Step 137 of 219, loss = 0.8740923469013069\n",
            "Epoch 6 Step 138 of 219, loss = 1.233315654535545\n",
            "Epoch 6 Step 139 of 219, loss = 0.3596515511380858\n",
            "Epoch 6 Step 140 of 219, loss = 0.5544182806916069\n",
            "Epoch 6 Step 141 of 219, loss = 0.09081719441746827\n",
            "Epoch 6 Step 142 of 219, loss = 0.37765410673455335\n",
            "Epoch 6 Step 143 of 219, loss = 0.09739737657218939\n",
            "Epoch 6 Step 144 of 219, loss = 0.520053438769537\n",
            "Epoch 6 Step 145 of 219, loss = 0.058447440755116986\n",
            "Epoch 6 Step 146 of 219, loss = 0.36202497853082605\n",
            "Epoch 6 Step 147 of 219, loss = 0.2306166844209656\n",
            "Epoch 6 Step 148 of 219, loss = 0.4690778356452938\n",
            "Epoch 6 Step 149 of 219, loss = 0.11071019666269422\n",
            "Epoch 6 Step 150 of 219, loss = 0.3932071215240285\n",
            "Epoch 6 Step 151 of 219, loss = 0.35356718362891115\n",
            "Epoch 6 Step 152 of 219, loss = 0.05230617035704199\n",
            "Epoch 6 Step 153 of 219, loss = 0.05646296188933775\n",
            "Epoch 6 Step 154 of 219, loss = 0.031136323930695653\n",
            "Epoch 6 Step 155 of 219, loss = 0.0942078476655297\n",
            "Epoch 6 Step 156 of 219, loss = 0.019497183879138902\n",
            "Epoch 6 Step 157 of 219, loss = 0.027128632238600403\n",
            "Epoch 6 Step 158 of 219, loss = 0.18260686215944588\n",
            "Epoch 6 Step 159 of 219, loss = 0.6611807506887999\n",
            "Epoch 6 Step 160 of 219, loss = 0.8922946104357834\n",
            "Epoch 6 Step 161 of 219, loss = 0.7730852328022593\n",
            "Epoch 6 Step 162 of 219, loss = 0.5667485089452384\n",
            "Epoch 6 Step 163 of 219, loss = 0.27519483386640786\n",
            "Epoch 6 Step 164 of 219, loss = 0.5300670586984779\n",
            "Epoch 6 Step 165 of 219, loss = 0.1363236282413709\n",
            "Epoch 6 Step 166 of 219, loss = 0.012971787218702957\n",
            "Epoch 6 Step 167 of 219, loss = 0.05477795493789017\n",
            "Epoch 6 Step 168 of 219, loss = 0.968515376382129\n",
            "Epoch 6 Step 169 of 219, loss = 0.2780823132925434\n",
            "Epoch 6 Step 170 of 219, loss = 0.008855053776642308\n",
            "Epoch 6 Step 171 of 219, loss = 0.4394811991878669\n",
            "Epoch 6 Step 172 of 219, loss = 0.016304018485243432\n",
            "Epoch 6 Step 173 of 219, loss = 0.3314971227373462\n",
            "Epoch 6 Step 174 of 219, loss = 0.005549280394916423\n",
            "Epoch 6 Step 175 of 219, loss = 0.8712664254780975\n",
            "Epoch 6 Step 176 of 219, loss = 0.02062046912033111\n",
            "Epoch 6 Step 177 of 219, loss = 0.024432799793430604\n",
            "Epoch 6 Step 178 of 219, loss = 0.20964307675967575\n",
            "Epoch 6 Step 179 of 219, loss = 0.0033670907469058875\n",
            "Epoch 6 Step 180 of 219, loss = 0.0289670408492384\n",
            "Epoch 6 Step 181 of 219, loss = 0.3142742649943102\n",
            "Epoch 6 Step 182 of 219, loss = 0.004034899357066024\n",
            "Epoch 6 Step 183 of 219, loss = 0.25890206452459097\n",
            "Epoch 6 Step 184 of 219, loss = 0.5939557828860416\n",
            "Epoch 6 Step 185 of 219, loss = 0.97936549163569\n",
            "Epoch 6 Step 186 of 219, loss = 0.14436928460781928\n",
            "Epoch 6 Step 187 of 219, loss = 0.19176019589212956\n",
            "Epoch 6 Step 188 of 219, loss = 2.349316965228354\n",
            "Epoch 6 Step 189 of 219, loss = 0.017412466240784852\n",
            "Epoch 6 Step 190 of 219, loss = 0.4122035967557167\n",
            "Epoch 6 Step 191 of 219, loss = 1.4999705634254497\n",
            "Epoch 6 Step 192 of 219, loss = 0.26814975224260706\n",
            "Epoch 6 Step 193 of 219, loss = 0.21990668849321082\n",
            "Epoch 6 Step 194 of 219, loss = 0.00695689617714379\n",
            "Epoch 6 Step 195 of 219, loss = 0.2988743954774691\n",
            "Epoch 6 Step 196 of 219, loss = 0.03157466206175741\n",
            "Epoch 6 Step 197 of 219, loss = 0.39427907578647137\n",
            "Epoch 6 Step 198 of 219, loss = 0.009984000789700076\n",
            "Epoch 6 Step 199 of 219, loss = 0.019776385586737888\n",
            "Epoch 6 Step 200 of 219, loss = 0.46534129287465475\n",
            "Epoch 6 Step 201 of 219, loss = 0.03297239658422768\n",
            "Epoch 6 Step 202 of 219, loss = 0.014464681502431631\n",
            "Epoch 6 Step 203 of 219, loss = 0.2811520205141278\n",
            "Epoch 6 Step 204 of 219, loss = 0.0540589210941107\n",
            "Epoch 6 Step 205 of 219, loss = 0.002101071699144086\n",
            "Epoch 6 Step 206 of 219, loss = 0.007669761929719243\n",
            "Epoch 6 Step 207 of 219, loss = 0.6711341307818657\n",
            "Epoch 6 Step 208 of 219, loss = 0.41518974947030074\n",
            "Epoch 6 Step 209 of 219, loss = 0.007048386500173365\n",
            "Epoch 6 Step 210 of 219, loss = 0.013883149847970344\n",
            "Epoch 6 Step 211 of 219, loss = 0.2689693643296778\n",
            "Epoch 6 Step 212 of 219, loss = 0.5695297373858921\n",
            "Epoch 6 Step 213 of 219, loss = 0.005186195521673653\n",
            "Epoch 6 Step 214 of 219, loss = 0.010347486415412277\n",
            "Epoch 6 Step 215 of 219, loss = 0.7253668606863357\n",
            "Epoch 6 Step 216 of 219, loss = 0.69394716629904\n",
            "Epoch 6 Step 217 of 219, loss = 0.7606646343192551\n",
            "Epoch 6 Step 218 of 219, loss = 0.6991107224021107\n",
            "Epoch 6 average train_loss: 0.306513 test_loss: 2.576356 test_score 0.52%\n",
            "Epoch 7 Step 0 of 219, loss = 0.24431591385655338\n",
            "Epoch 7 Step 1 of 219, loss = 0.0017181848124891985\n",
            "Epoch 7 Step 2 of 219, loss = 0.4855936162348371\n",
            "Epoch 7 Step 3 of 219, loss = 0.3639331476588268\n",
            "Epoch 7 Step 4 of 219, loss = 0.19307794367705355\n",
            "Epoch 7 Step 5 of 219, loss = 0.011725583142833784\n",
            "Epoch 7 Step 6 of 219, loss = 0.05975923768710345\n",
            "Epoch 7 Step 7 of 219, loss = 0.031616414424206596\n",
            "Epoch 7 Step 8 of 219, loss = 0.017497907603683416\n",
            "Epoch 7 Step 9 of 219, loss = 0.02820077774231322\n",
            "Epoch 7 Step 10 of 219, loss = 0.27112202168791555\n",
            "Epoch 7 Step 11 of 219, loss = 0.0036863993209408363\n",
            "Epoch 7 Step 12 of 219, loss = 0.002479688222592813\n",
            "Epoch 7 Step 13 of 219, loss = 0.006717628661135677\n",
            "Epoch 7 Step 14 of 219, loss = 0.5976163678642479\n",
            "Epoch 7 Step 15 of 219, loss = 0.0010044151476904517\n",
            "Epoch 7 Step 16 of 219, loss = 0.010655386195139727\n",
            "Epoch 7 Step 17 of 219, loss = 1.1526370519204647\n",
            "Epoch 7 Step 18 of 219, loss = 0.47328218855727755\n",
            "Epoch 7 Step 19 of 219, loss = 0.17566048761000275\n",
            "Epoch 7 Step 20 of 219, loss = 0.005384641197451856\n",
            "Epoch 7 Step 21 of 219, loss = 0.4136758963613829\n",
            "Epoch 7 Step 22 of 219, loss = 0.5976219335425412\n",
            "Epoch 7 Step 23 of 219, loss = 0.037662436356185935\n",
            "Epoch 7 Step 24 of 219, loss = 0.216692293082815\n",
            "Epoch 7 Step 25 of 219, loss = 0.3409567177805002\n",
            "Epoch 7 Step 26 of 219, loss = 0.7516389281845477\n",
            "Epoch 7 Step 27 of 219, loss = 0.845849039433233\n",
            "Epoch 7 Step 28 of 219, loss = 0.006700879272102611\n",
            "Epoch 7 Step 29 of 219, loss = 0.014704519497172441\n",
            "Epoch 7 Step 30 of 219, loss = 0.004254550935002044\n",
            "Epoch 7 Step 31 of 219, loss = 0.02642483096860815\n",
            "Epoch 7 Step 32 of 219, loss = 0.0025778358030947857\n",
            "Epoch 7 Step 33 of 219, loss = 0.8378243424958782\n",
            "Epoch 7 Step 34 of 219, loss = 0.045322882433538325\n",
            "Epoch 7 Step 35 of 219, loss = 0.006793319293137756\n",
            "Epoch 7 Step 36 of 219, loss = 0.08009930621119565\n",
            "Epoch 7 Step 37 of 219, loss = 0.05953788726401399\n",
            "Epoch 7 Step 38 of 219, loss = 0.000762776853662217\n",
            "Epoch 7 Step 39 of 219, loss = 0.10910485043950757\n",
            "Epoch 7 Step 40 of 219, loss = 0.0010739972221927019\n",
            "Epoch 7 Step 41 of 219, loss = 0.5438871517708321\n",
            "Epoch 7 Step 42 of 219, loss = 0.002906056139181601\n",
            "Epoch 7 Step 43 of 219, loss = 0.0009695994149296894\n",
            "Epoch 7 Step 44 of 219, loss = 0.0008766308219492203\n",
            "Epoch 7 Step 45 of 219, loss = 0.0254532391900284\n",
            "Epoch 7 Step 46 of 219, loss = 0.0011377623154658068\n",
            "Epoch 7 Step 47 of 219, loss = 1.914891072854516\n",
            "Epoch 7 Step 48 of 219, loss = 0.0005231595268924139\n",
            "Epoch 7 Step 49 of 219, loss = 1.9718476934176579\n",
            "Epoch 7 Step 50 of 219, loss = 0.0022834742812847253\n",
            "Epoch 7 Step 51 of 219, loss = 0.000951140164033859\n",
            "Epoch 7 Step 52 of 219, loss = 0.0030492919831885956\n",
            "Epoch 7 Step 53 of 219, loss = 0.004432084388099611\n",
            "Epoch 7 Step 54 of 219, loss = 0.01223457426749519\n",
            "Epoch 7 Step 55 of 219, loss = 0.0013755695254076272\n",
            "Epoch 7 Step 56 of 219, loss = 0.005048008135418058\n",
            "Epoch 7 Step 57 of 219, loss = 0.00165121471582097\n",
            "Epoch 7 Step 58 of 219, loss = 0.006092046398407547\n",
            "Epoch 7 Step 59 of 219, loss = 0.004964524881870602\n",
            "Epoch 7 Step 60 of 219, loss = 0.01438424991829379\n",
            "Epoch 7 Step 61 of 219, loss = 0.04092413615217083\n",
            "Epoch 7 Step 62 of 219, loss = 0.06992017398624739\n",
            "Epoch 7 Step 63 of 219, loss = 0.0016740018509153742\n",
            "Epoch 7 Step 64 of 219, loss = 0.000800432975665899\n",
            "Epoch 7 Step 65 of 219, loss = 0.0010797215509228408\n",
            "Epoch 7 Step 66 of 219, loss = 0.0008548071609766339\n",
            "Epoch 7 Step 67 of 219, loss = 0.0054404420998253045\n",
            "Epoch 7 Step 68 of 219, loss = 0.29346434930812393\n",
            "Epoch 7 Step 69 of 219, loss = 0.0004381006388030073\n",
            "Epoch 7 Step 70 of 219, loss = 0.0023110023321351036\n",
            "Epoch 7 Step 71 of 219, loss = 0.006699970150293666\n",
            "Epoch 7 Step 72 of 219, loss = 0.0030918923494027695\n",
            "Epoch 7 Step 73 of 219, loss = 0.0024878471667761914\n",
            "Epoch 7 Step 74 of 219, loss = 0.0030651908891741186\n",
            "Epoch 7 Step 75 of 219, loss = 0.0009954064107660088\n",
            "Epoch 7 Step 76 of 219, loss = 0.0008814597140371916\n",
            "Epoch 7 Step 77 of 219, loss = 0.0011445839027146576\n",
            "Epoch 7 Step 78 of 219, loss = 0.30251820303055865\n",
            "Epoch 7 Step 79 of 219, loss = 0.563539814568685\n",
            "Epoch 7 Step 80 of 219, loss = 0.0008714307414265932\n",
            "Epoch 7 Step 81 of 219, loss = 0.0005968489303995739\n",
            "Epoch 7 Step 82 of 219, loss = 0.0004575983930408256\n",
            "Epoch 7 Step 83 of 219, loss = 0.2595542772614863\n",
            "Epoch 7 Step 84 of 219, loss = 0.000833356908060523\n",
            "Epoch 7 Step 85 of 219, loss = 0.0024631748128740583\n",
            "Epoch 7 Step 86 of 219, loss = 0.0004055696999785141\n",
            "Epoch 7 Step 87 of 219, loss = 0.9461676315436307\n",
            "Epoch 7 Step 88 of 219, loss = 0.0037824656492375652\n",
            "Epoch 7 Step 89 of 219, loss = 0.9550374527061649\n",
            "Epoch 7 Step 90 of 219, loss = 0.2906445510352569\n",
            "Epoch 7 Step 91 of 219, loss = 0.7699972074997277\n",
            "Epoch 7 Step 92 of 219, loss = 0.0005416217463789508\n",
            "Epoch 7 Step 93 of 219, loss = 0.00204682930780109\n",
            "Epoch 7 Step 94 of 219, loss = 0.0011239369159738999\n",
            "Epoch 7 Step 95 of 219, loss = 0.009351213973786798\n",
            "Epoch 7 Step 96 of 219, loss = 0.0025079525585169904\n",
            "Epoch 7 Step 97 of 219, loss = 0.0004765040721395053\n",
            "Epoch 7 Step 98 of 219, loss = 0.0060056828460801626\n",
            "Epoch 7 Step 99 of 219, loss = 0.34270797262342967\n",
            "Epoch 7 Step 100 of 219, loss = 0.8047895099271045\n",
            "Epoch 7 Step 101 of 219, loss = 0.0012145313648943556\n",
            "Epoch 7 Step 102 of 219, loss = 0.009282959307256533\n",
            "Epoch 7 Step 103 of 219, loss = 0.00029828930792064057\n",
            "Epoch 7 Step 104 of 219, loss = 0.00036141286000201944\n",
            "Epoch 7 Step 105 of 219, loss = 0.00036277027129472117\n",
            "Epoch 7 Step 106 of 219, loss = 0.0007386480692730402\n",
            "Epoch 7 Step 107 of 219, loss = 0.001805976601644943\n",
            "Epoch 7 Step 108 of 219, loss = 0.005199774343054742\n",
            "Epoch 7 Step 109 of 219, loss = 0.0007365770252363291\n",
            "Epoch 7 Step 110 of 219, loss = 1.6019749127208343\n",
            "Epoch 7 Step 111 of 219, loss = 0.002964960294775665\n",
            "Epoch 7 Step 112 of 219, loss = 0.4848157153342072\n",
            "Epoch 7 Step 113 of 219, loss = 0.0011656773658614838\n",
            "Epoch 7 Step 114 of 219, loss = 0.0014383624084075564\n",
            "Epoch 7 Step 115 of 219, loss = 0.0006663670974376146\n",
            "Epoch 7 Step 116 of 219, loss = 0.0008486187293783587\n",
            "Epoch 7 Step 117 of 219, loss = 0.4635270216913341\n",
            "Epoch 7 Step 118 of 219, loss = 0.0010168937014896073\n",
            "Epoch 7 Step 119 of 219, loss = 0.4447507670320192\n",
            "Epoch 7 Step 120 of 219, loss = 0.34014762311380764\n",
            "Epoch 7 Step 121 of 219, loss = 0.009912477579518963\n",
            "Epoch 7 Step 122 of 219, loss = 0.0058101305755826615\n",
            "Epoch 7 Step 123 of 219, loss = 0.003246371893965261\n",
            "Epoch 7 Step 124 of 219, loss = 0.0059915763240496744\n",
            "Epoch 7 Step 125 of 219, loss = 0.27789705699979095\n",
            "Epoch 7 Step 126 of 219, loss = 0.09117011142279807\n",
            "Epoch 7 Step 127 of 219, loss = 0.00029722397584919236\n",
            "Epoch 7 Step 128 of 219, loss = 0.007303837683139136\n",
            "Epoch 7 Step 129 of 219, loss = 0.024667032605066197\n",
            "Epoch 7 Step 130 of 219, loss = 0.0019492553165036952\n",
            "Epoch 7 Step 131 of 219, loss = 0.0003698989489748783\n",
            "Epoch 7 Step 132 of 219, loss = 0.003471835750133323\n",
            "Epoch 7 Step 133 of 219, loss = 0.003133336625069205\n",
            "Epoch 7 Step 134 of 219, loss = 0.34155578170293666\n",
            "Epoch 7 Step 135 of 219, loss = 0.019857650705489505\n",
            "Epoch 7 Step 136 of 219, loss = 0.028281621517635358\n",
            "Epoch 7 Step 137 of 219, loss = 1.2169922740013135\n",
            "Epoch 7 Step 138 of 219, loss = 0.0045749368337055785\n",
            "Epoch 7 Step 139 of 219, loss = 0.004631271517610003\n",
            "Epoch 7 Step 140 of 219, loss = 0.656332074509919\n",
            "Epoch 7 Step 141 of 219, loss = 0.008248931580510543\n",
            "Epoch 7 Step 142 of 219, loss = 0.7756058634913643\n",
            "Epoch 7 Step 143 of 219, loss = 0.01441125930432463\n",
            "Epoch 7 Step 144 of 219, loss = 0.0003856438170259935\n",
            "Epoch 7 Step 145 of 219, loss = 0.006475496949406079\n",
            "Epoch 7 Step 146 of 219, loss = 0.9816365788374242\n",
            "Epoch 7 Step 147 of 219, loss = 0.4030295815955469\n",
            "Epoch 7 Step 148 of 219, loss = 0.7932410857752075\n",
            "Epoch 7 Step 149 of 219, loss = 0.3990786422682504\n",
            "Epoch 7 Step 150 of 219, loss = 0.0013688661447304185\n",
            "Epoch 7 Step 151 of 219, loss = 0.004871979483141331\n",
            "Epoch 7 Step 152 of 219, loss = 0.0016183953393920092\n",
            "Epoch 7 Step 153 of 219, loss = 0.0025921867277247657\n",
            "Epoch 7 Step 154 of 219, loss = 0.12014296862798801\n",
            "Epoch 7 Step 155 of 219, loss = 0.06738654335640604\n",
            "Epoch 7 Step 156 of 219, loss = 0.0008879642291503842\n",
            "Epoch 7 Step 157 of 219, loss = 0.4274867649019143\n",
            "Epoch 7 Step 158 of 219, loss = 0.008580247266763763\n",
            "Epoch 7 Step 159 of 219, loss = 0.11555432152772482\n",
            "Epoch 7 Step 160 of 219, loss = 0.6832481712090157\n",
            "Epoch 7 Step 161 of 219, loss = 1.1849165636713224\n",
            "Epoch 7 Step 162 of 219, loss = 0.09232837768468016\n",
            "Epoch 7 Step 163 of 219, loss = 0.01124937910844892\n",
            "Epoch 7 Step 164 of 219, loss = 0.6830699437405201\n",
            "Epoch 7 Step 165 of 219, loss = 0.02688446792672039\n",
            "Epoch 7 Step 166 of 219, loss = 0.6228792806723504\n",
            "Epoch 7 Step 167 of 219, loss = 1.0278748621026352\n",
            "Epoch 7 Step 168 of 219, loss = 0.45344302418334337\n",
            "Epoch 7 Step 169 of 219, loss = 0.0008495603483424929\n",
            "Epoch 7 Step 170 of 219, loss = 0.004587541112414328\n",
            "Epoch 7 Step 171 of 219, loss = 1.1801700022042496\n",
            "Epoch 7 Step 172 of 219, loss = 0.04309735362403444\n",
            "Epoch 7 Step 173 of 219, loss = 0.028872401535409153\n",
            "Epoch 7 Step 174 of 219, loss = 0.06200587000603264\n",
            "Epoch 7 Step 175 of 219, loss = 0.4011760430475988\n",
            "Epoch 7 Step 176 of 219, loss = 0.008984077874629293\n",
            "Epoch 7 Step 177 of 219, loss = 0.2020150661792286\n",
            "Epoch 7 Step 178 of 219, loss = 0.02669534756205394\n",
            "Epoch 7 Step 179 of 219, loss = 0.0020139376620136318\n",
            "Epoch 7 Step 180 of 219, loss = 0.11527042587113101\n",
            "Epoch 7 Step 181 of 219, loss = 0.38607771795796\n",
            "Epoch 7 Step 182 of 219, loss = 0.0019131221606585314\n",
            "Epoch 7 Step 183 of 219, loss = 0.002193202068156097\n",
            "Epoch 7 Step 184 of 219, loss = 1.0326017587212846\n",
            "Epoch 7 Step 185 of 219, loss = 0.006107857980168774\n",
            "Epoch 7 Step 186 of 219, loss = 0.0015955088347254787\n",
            "Epoch 7 Step 187 of 219, loss = 0.004730864003249735\n",
            "Epoch 7 Step 188 of 219, loss = 1.473774153570048\n",
            "Epoch 7 Step 189 of 219, loss = 0.04774478910439939\n",
            "Epoch 7 Step 190 of 219, loss = 0.004122003685097297\n",
            "Epoch 7 Step 191 of 219, loss = 0.5955132967919781\n",
            "Epoch 7 Step 192 of 219, loss = 0.05016297535075864\n",
            "Epoch 7 Step 193 of 219, loss = 0.0037832895604879013\n",
            "Epoch 7 Step 194 of 219, loss = 0.005437656367575983\n",
            "Epoch 7 Step 195 of 219, loss = 0.3850269820795802\n",
            "Epoch 7 Step 196 of 219, loss = 0.19084342250425834\n",
            "Epoch 7 Step 197 of 219, loss = 0.5859767190559069\n",
            "Epoch 7 Step 198 of 219, loss = 0.0036016432895848993\n",
            "Epoch 7 Step 199 of 219, loss = 0.0033885319562614313\n",
            "Epoch 7 Step 200 of 219, loss = 0.040216279165179\n",
            "Epoch 7 Step 201 of 219, loss = 0.5422250262945454\n",
            "Epoch 7 Step 202 of 219, loss = 0.009171790754407994\n",
            "Epoch 7 Step 203 of 219, loss = 0.14210313212970505\n",
            "Epoch 7 Step 204 of 219, loss = 0.01962651875510346\n",
            "Epoch 7 Step 205 of 219, loss = 0.0014340527040985762\n",
            "Epoch 7 Step 206 of 219, loss = 0.003163177274473128\n",
            "Epoch 7 Step 207 of 219, loss = 0.36035877659378457\n",
            "Epoch 7 Step 208 of 219, loss = 0.002222358892140619\n",
            "Epoch 7 Step 209 of 219, loss = 0.0027486164726724382\n",
            "Epoch 7 Step 210 of 219, loss = 0.011648955374766956\n",
            "Epoch 7 Step 211 of 219, loss = 0.003593093263589253\n",
            "Epoch 7 Step 212 of 219, loss = 0.6943597377685364\n",
            "Epoch 7 Step 213 of 219, loss = 0.006003136113577057\n",
            "Epoch 7 Step 214 of 219, loss = 0.6179220900412474\n",
            "Epoch 7 Step 215 of 219, loss = 0.002296706942615856\n",
            "Epoch 7 Step 216 of 219, loss = 0.0030269062699517235\n",
            "Epoch 7 Step 217 of 219, loss = 0.026152981059567537\n",
            "Epoch 7 Step 218 of 219, loss = 0.35709849210494815\n",
            "Epoch 7 average train_loss: 0.198414 test_loss: 2.530225 test_score 0.54%\n",
            "Epoch 8 Step 0 of 219, loss = 0.0009310030172855477\n",
            "Epoch 8 Step 1 of 219, loss = 0.0006082302406866802\n",
            "Epoch 8 Step 2 of 219, loss = 0.0016437678586953552\n",
            "Epoch 8 Step 3 of 219, loss = 0.5633998739549497\n",
            "Epoch 8 Step 4 of 219, loss = 0.40330965998327883\n",
            "Epoch 8 Step 5 of 219, loss = 0.00046053476216911804\n",
            "Epoch 8 Step 6 of 219, loss = 0.0005591048488895467\n",
            "Epoch 8 Step 7 of 219, loss = 0.0005093009062875353\n",
            "Epoch 8 Step 8 of 219, loss = 0.25122614952033473\n",
            "Epoch 8 Step 9 of 219, loss = 0.2496863826927438\n",
            "Epoch 8 Step 10 of 219, loss = 0.0025287382186434115\n",
            "Epoch 8 Step 11 of 219, loss = 0.00024711847845537704\n",
            "Epoch 8 Step 12 of 219, loss = 0.0014561952071971973\n",
            "Epoch 8 Step 13 of 219, loss = 0.4111473612449572\n",
            "Epoch 8 Step 14 of 219, loss = 0.06082770202328902\n",
            "Epoch 8 Step 15 of 219, loss = 0.0006529862394017982\n",
            "Epoch 8 Step 16 of 219, loss = 0.022024146347462192\n",
            "Epoch 8 Step 17 of 219, loss = 0.8405581795341277\n",
            "Epoch 8 Step 18 of 219, loss = 0.786498590343399\n",
            "Epoch 8 Step 19 of 219, loss = 0.002393486262008082\n",
            "Epoch 8 Step 20 of 219, loss = 0.0002839124474576238\n",
            "Epoch 8 Step 21 of 219, loss = 0.003574471824322245\n",
            "Epoch 8 Step 22 of 219, loss = 0.4366263744668686\n",
            "Epoch 8 Step 23 of 219, loss = 3.119072259403765\n",
            "Epoch 8 Step 24 of 219, loss = 0.45862716258125147\n",
            "Epoch 8 Step 25 of 219, loss = 1.2848911978821889\n",
            "Epoch 8 Step 26 of 219, loss = 0.0009060215418230655\n",
            "Epoch 8 Step 27 of 219, loss = 0.002142413166438928\n",
            "Epoch 8 Step 28 of 219, loss = 0.0029757854695162678\n",
            "Epoch 8 Step 29 of 219, loss = 0.010270802460581763\n",
            "Epoch 8 Step 30 of 219, loss = 0.2571607855588809\n",
            "Epoch 8 Step 31 of 219, loss = 0.001444165263819741\n",
            "Epoch 8 Step 32 of 219, loss = 0.00047924556201905943\n",
            "Epoch 8 Step 33 of 219, loss = 0.4200408914621221\n",
            "Epoch 8 Step 34 of 219, loss = 0.6972713696759456\n",
            "Epoch 8 Step 35 of 219, loss = 0.0011070462396673975\n",
            "Epoch 8 Step 36 of 219, loss = 0.0028092701504647266\n",
            "Epoch 8 Step 37 of 219, loss = 0.0020405656418915896\n",
            "Epoch 8 Step 38 of 219, loss = 0.0005038511781094712\n",
            "Epoch 8 Step 39 of 219, loss = 0.015363054084446048\n",
            "Epoch 8 Step 40 of 219, loss = 0.002422720586764626\n",
            "Epoch 8 Step 41 of 219, loss = 0.0013025834323343588\n",
            "Epoch 8 Step 42 of 219, loss = 0.002087848934024805\n",
            "Epoch 8 Step 43 of 219, loss = 0.0023776032012392534\n",
            "Epoch 8 Step 44 of 219, loss = 0.0005688716355507495\n",
            "Epoch 8 Step 45 of 219, loss = 0.0017011794216159615\n",
            "Epoch 8 Step 46 of 219, loss = 0.2136056610079322\n",
            "Epoch 8 Step 47 of 219, loss = 0.9484628023146797\n",
            "Epoch 8 Step 48 of 219, loss = 0.0006223680629773298\n",
            "Epoch 8 Step 49 of 219, loss = 1.1589913459320087\n",
            "Epoch 8 Step 50 of 219, loss = 0.004512981739253519\n",
            "Epoch 8 Step 51 of 219, loss = 0.0014468834724539192\n",
            "Epoch 8 Step 52 of 219, loss = 0.0025851520331343636\n",
            "Epoch 8 Step 53 of 219, loss = 0.0018246582330903038\n",
            "Epoch 8 Step 54 of 219, loss = 0.011603594806729234\n",
            "Epoch 8 Step 55 of 219, loss = 0.0010375970387030975\n",
            "Epoch 8 Step 56 of 219, loss = 0.0012733960102195852\n",
            "Epoch 8 Step 57 of 219, loss = 0.016380842539547302\n",
            "Epoch 8 Step 58 of 219, loss = 0.0014066612093301956\n",
            "Epoch 8 Step 59 of 219, loss = 0.0014011619264238107\n",
            "Epoch 8 Step 60 of 219, loss = 0.002711293580432539\n",
            "Epoch 8 Step 61 of 219, loss = 0.0037959042310831137\n",
            "Epoch 8 Step 62 of 219, loss = 0.0171918338019168\n",
            "Epoch 8 Step 63 of 219, loss = 0.0010899446203893604\n",
            "Epoch 8 Step 64 of 219, loss = 7.516445828059659e-05\n",
            "Epoch 8 Step 65 of 219, loss = 9.972576299333014e-05\n",
            "Epoch 8 Step 66 of 219, loss = 0.00014832400393061107\n",
            "Epoch 8 Step 67 of 219, loss = 0.0034731431634327237\n",
            "Epoch 8 Step 68 of 219, loss = 0.020463410903175827\n",
            "Epoch 8 Step 69 of 219, loss = 0.00023061507829424954\n",
            "Epoch 8 Step 70 of 219, loss = 0.0001551257278151752\n",
            "Epoch 8 Step 71 of 219, loss = 0.0005608177809790504\n",
            "Epoch 8 Step 72 of 219, loss = 0.000124519205883189\n",
            "Epoch 8 Step 73 of 219, loss = 0.00027552200572245056\n",
            "Epoch 8 Step 74 of 219, loss = 0.0178618875193024\n",
            "Epoch 8 Step 75 of 219, loss = 0.0006823910107414122\n",
            "Epoch 8 Step 76 of 219, loss = 0.000403362860197376\n",
            "Epoch 8 Step 77 of 219, loss = 0.0009339732287116931\n",
            "Epoch 8 Step 78 of 219, loss = 0.004321209061345144\n",
            "Epoch 8 Step 79 of 219, loss = 0.0006495746083601261\n",
            "Epoch 8 Step 80 of 219, loss = 0.0006305982042249525\n",
            "Epoch 8 Step 81 of 219, loss = 0.0010540670746195246\n",
            "Epoch 8 Step 82 of 219, loss = 0.0002222877583335503\n",
            "Epoch 8 Step 83 of 219, loss = 0.5666703107604008\n",
            "Epoch 8 Step 84 of 219, loss = 0.01014370432267242\n",
            "Epoch 8 Step 85 of 219, loss = 0.0005404245184763568\n",
            "Epoch 8 Step 86 of 219, loss = 0.00019065759499881096\n",
            "Epoch 8 Step 87 of 219, loss = 0.764692199073238\n",
            "Epoch 8 Step 88 of 219, loss = 0.002491713246854488\n",
            "Epoch 8 Step 89 of 219, loss = 0.8772686702841384\n",
            "Epoch 8 Step 90 of 219, loss = 0.00015829431515612669\n",
            "Epoch 8 Step 91 of 219, loss = 0.10927343364596709\n",
            "Epoch 8 Step 92 of 219, loss = 0.0006888405239351414\n",
            "Epoch 8 Step 93 of 219, loss = 0.6757370236794031\n",
            "Epoch 8 Step 94 of 219, loss = 0.05364980442209344\n",
            "Epoch 8 Step 95 of 219, loss = 0.001122003466662136\n",
            "Epoch 8 Step 96 of 219, loss = 0.14395063216943527\n",
            "Epoch 8 Step 97 of 219, loss = 0.0020336350662546465\n",
            "Epoch 8 Step 98 of 219, loss = 0.07758552934865293\n",
            "Epoch 8 Step 99 of 219, loss = 0.051735643934080144\n",
            "Epoch 8 Step 100 of 219, loss = 0.06706763326110377\n",
            "Epoch 8 Step 101 of 219, loss = 0.0026183698141721834\n",
            "Epoch 8 Step 102 of 219, loss = 0.027595671941071487\n",
            "Epoch 8 Step 103 of 219, loss = 0.017296084984309346\n",
            "Epoch 8 Step 104 of 219, loss = 0.0010125020626219339\n",
            "Epoch 8 Step 105 of 219, loss = 0.0013962466991870315\n",
            "Epoch 8 Step 106 of 219, loss = 0.7138352010824747\n",
            "Epoch 8 Step 107 of 219, loss = 0.002949454831195908\n",
            "Epoch 8 Step 108 of 219, loss = 0.009779874586456572\n",
            "Epoch 8 Step 109 of 219, loss = 0.023276797611515576\n",
            "Epoch 8 Step 110 of 219, loss = 0.373051249330274\n",
            "Epoch 8 Step 111 of 219, loss = 0.011139346142044815\n",
            "Epoch 8 Step 112 of 219, loss = 0.10777178032799384\n",
            "Epoch 8 Step 113 of 219, loss = 0.0008602558837083052\n",
            "Epoch 8 Step 114 of 219, loss = 0.03875371602725863\n",
            "Epoch 8 Step 115 of 219, loss = 0.00048789569655127707\n",
            "Epoch 8 Step 116 of 219, loss = 0.07804001281147066\n",
            "Epoch 8 Step 117 of 219, loss = 0.0004768888895796408\n",
            "Epoch 8 Step 118 of 219, loss = 9.764134301804006e-05\n",
            "Epoch 8 Step 119 of 219, loss = 0.007073148269682861\n",
            "Epoch 8 Step 120 of 219, loss = 0.0001651902509820502\n",
            "Epoch 8 Step 121 of 219, loss = 0.0001030785961120273\n",
            "Epoch 8 Step 122 of 219, loss = 0.00014565708670488675\n",
            "Epoch 8 Step 123 of 219, loss = 9.004878870655375e-05\n",
            "Epoch 8 Step 124 of 219, loss = 0.24926979330643917\n",
            "Epoch 8 Step 125 of 219, loss = 0.0005248934114661097\n",
            "Epoch 8 Step 126 of 219, loss = 0.7237438327974814\n",
            "Epoch 8 Step 127 of 219, loss = 0.00018005651588737237\n",
            "Epoch 8 Step 128 of 219, loss = 0.0001455550295759167\n",
            "Epoch 8 Step 129 of 219, loss = 0.598188334605311\n",
            "Epoch 8 Step 130 of 219, loss = 0.0007496973075831193\n",
            "Epoch 8 Step 131 of 219, loss = 0.00048606693280817126\n",
            "Epoch 8 Step 132 of 219, loss = 0.0004577382369461702\n",
            "Epoch 8 Step 133 of 219, loss = 0.0010759076512840693\n",
            "Epoch 8 Step 134 of 219, loss = 0.0009451989935769234\n",
            "Epoch 8 Step 135 of 219, loss = 0.055902878066035555\n",
            "Epoch 8 Step 136 of 219, loss = 0.003146664417727152\n",
            "Epoch 8 Step 137 of 219, loss = 0.6143836757171357\n",
            "Epoch 8 Step 138 of 219, loss = 0.0006508704245788977\n",
            "Epoch 8 Step 139 of 219, loss = 0.00035439136649983993\n",
            "Epoch 8 Step 140 of 219, loss = 1.2459042759182921\n",
            "Epoch 8 Step 141 of 219, loss = 0.43111183461587643\n",
            "Epoch 8 Step 142 of 219, loss = 0.006293958610513073\n",
            "Epoch 8 Step 143 of 219, loss = 0.002126412388861354\n",
            "Epoch 8 Step 144 of 219, loss = 0.00035558732770368806\n",
            "Epoch 8 Step 145 of 219, loss = 0.0019503280050230387\n",
            "Epoch 8 Step 146 of 219, loss = 0.008845902370694603\n",
            "Epoch 8 Step 147 of 219, loss = 0.019757804277105606\n",
            "Epoch 8 Step 148 of 219, loss = 0.01165834866651494\n",
            "Epoch 8 Step 149 of 219, loss = 0.0008605486091255443\n",
            "Epoch 8 Step 150 of 219, loss = 0.08599569592661283\n",
            "Epoch 8 Step 151 of 219, loss = 0.0009092603695535217\n",
            "Epoch 8 Step 152 of 219, loss = 0.0007108406912266219\n",
            "Epoch 8 Step 153 of 219, loss = 0.7091287851631023\n",
            "Epoch 8 Step 154 of 219, loss = 0.20662461056372194\n",
            "Epoch 8 Step 155 of 219, loss = 0.006300656625626289\n",
            "Epoch 8 Step 156 of 219, loss = 0.0011782236952058156\n",
            "Epoch 8 Step 157 of 219, loss = 0.000912325223453081\n",
            "Epoch 8 Step 158 of 219, loss = 0.002347324745642254\n",
            "Epoch 8 Step 159 of 219, loss = 0.9003584341432997\n",
            "Epoch 8 Step 160 of 219, loss = 0.01858461045776494\n",
            "Epoch 8 Step 161 of 219, loss = 0.0007081840831233421\n",
            "Epoch 8 Step 162 of 219, loss = 0.03058972087683287\n",
            "Epoch 8 Step 163 of 219, loss = 0.0011865607061736227\n",
            "Epoch 8 Step 164 of 219, loss = 0.25973652544507786\n",
            "Epoch 8 Step 165 of 219, loss = 0.017595256923414127\n",
            "Epoch 8 Step 166 of 219, loss = 0.0012742568242174457\n",
            "Epoch 8 Step 167 of 219, loss = 0.005180127136554802\n",
            "Epoch 8 Step 168 of 219, loss = 0.0015804718796061934\n",
            "Epoch 8 Step 169 of 219, loss = 0.018491790234747896\n",
            "Epoch 8 Step 170 of 219, loss = 0.0029099125576976803\n",
            "Epoch 8 Step 171 of 219, loss = 0.5388194141414715\n",
            "Epoch 8 Step 172 of 219, loss = 0.0007114634308891254\n",
            "Epoch 8 Step 173 of 219, loss = 0.003126132067791332\n",
            "Epoch 8 Step 174 of 219, loss = 0.6754559187247651\n",
            "Epoch 8 Step 175 of 219, loss = 0.2310272284103121\n",
            "Epoch 8 Step 176 of 219, loss = 0.005854965909747989\n",
            "Epoch 8 Step 177 of 219, loss = 0.0015886063174548326\n",
            "Epoch 8 Step 178 of 219, loss = 0.43941072303641704\n",
            "Epoch 8 Step 179 of 219, loss = 0.0014038533472557901\n",
            "Epoch 8 Step 180 of 219, loss = 0.004652542539588467\n",
            "Epoch 8 Step 181 of 219, loss = 0.0031977503413145314\n",
            "Epoch 8 Step 182 of 219, loss = 0.0014826712731519365\n",
            "Epoch 8 Step 183 of 219, loss = 0.0033343620029882004\n",
            "Epoch 8 Step 184 of 219, loss = 0.0008846446917232242\n",
            "Epoch 8 Step 185 of 219, loss = 0.0010157154029002413\n",
            "Epoch 8 Step 186 of 219, loss = 0.0007681838069402147\n",
            "Epoch 8 Step 187 of 219, loss = 0.004019345607957803\n",
            "Epoch 8 Step 188 of 219, loss = 1.2089159540792025\n",
            "Epoch 8 Step 189 of 219, loss = 0.03752016407361225\n",
            "Epoch 8 Step 190 of 219, loss = 0.4970154313434705\n",
            "Epoch 8 Step 191 of 219, loss = 0.0890024441141577\n",
            "Epoch 8 Step 192 of 219, loss = 0.020774748743860982\n",
            "Epoch 8 Step 193 of 219, loss = 0.00048090248674270697\n",
            "Epoch 8 Step 194 of 219, loss = 0.00046742031236135517\n",
            "Epoch 8 Step 195 of 219, loss = 0.0006563798119714193\n",
            "Epoch 8 Step 196 of 219, loss = 0.0009347792502012453\n",
            "Epoch 8 Step 197 of 219, loss = 0.12077740826225636\n",
            "Epoch 8 Step 198 of 219, loss = 0.0002892661464102275\n",
            "Epoch 8 Step 199 of 219, loss = 0.0017220963293311797\n",
            "Epoch 8 Step 200 of 219, loss = 0.0013476062167683267\n",
            "Epoch 8 Step 201 of 219, loss = 0.001848734684244846\n",
            "Epoch 8 Step 202 of 219, loss = 0.0006391507581611222\n",
            "Epoch 8 Step 203 of 219, loss = 0.0007861067556405033\n",
            "Epoch 8 Step 204 of 219, loss = 0.0011752039970360784\n",
            "Epoch 8 Step 205 of 219, loss = 0.0005054719495092286\n",
            "Epoch 8 Step 206 of 219, loss = 0.0008310809146223619\n",
            "Epoch 8 Step 207 of 219, loss = 0.11418712598833736\n",
            "Epoch 8 Step 208 of 219, loss = 0.000827443811658668\n",
            "Epoch 8 Step 209 of 219, loss = 0.001588641701346205\n",
            "Epoch 8 Step 210 of 219, loss = 0.001051524045124097\n",
            "Epoch 8 Step 211 of 219, loss = 0.0026956031870213337\n",
            "Epoch 8 Step 212 of 219, loss = 0.0014043322180441464\n",
            "Epoch 8 Step 213 of 219, loss = 0.0005737562005379004\n",
            "Epoch 8 Step 214 of 219, loss = 0.00039089323126972886\n",
            "Epoch 8 Step 215 of 219, loss = 0.026365350194737402\n",
            "Epoch 8 Step 216 of 219, loss = 0.437527023135317\n",
            "Epoch 8 Step 217 of 219, loss = 0.0007759677478134108\n",
            "Epoch 8 Step 218 of 219, loss = 0.0018882717389108923\n",
            "Epoch 8 average train_loss: 0.129393 test_loss: 2.924324 test_score 0.54%\n",
            "Epoch 9 Step 0 of 219, loss = 0.00040329108401238045\n",
            "Epoch 9 Step 1 of 219, loss = 9.618118360776862e-05\n",
            "Epoch 9 Step 2 of 219, loss = 0.000658488588101136\n",
            "Epoch 9 Step 3 of 219, loss = 0.2651962065560838\n",
            "Epoch 9 Step 4 of 219, loss = 0.00017097171394198085\n",
            "Epoch 9 Step 5 of 219, loss = 0.00010173280003300533\n",
            "Epoch 9 Step 6 of 219, loss = 8.644455101602944e-05\n",
            "Epoch 9 Step 7 of 219, loss = 0.0001907269902403641\n",
            "Epoch 9 Step 8 of 219, loss = 7.652081126252597e-05\n",
            "Epoch 9 Step 9 of 219, loss = 8.046191817356885e-05\n",
            "Epoch 9 Step 10 of 219, loss = 0.00014164332299060334\n",
            "Epoch 9 Step 11 of 219, loss = 0.00020093088573958084\n",
            "Epoch 9 Step 12 of 219, loss = 0.00010520720672957395\n",
            "Epoch 9 Step 13 of 219, loss = 0.00020092280180961097\n",
            "Epoch 9 Step 14 of 219, loss = 0.5072045853082727\n",
            "Epoch 9 Step 15 of 219, loss = 6.34894369113681e-05\n",
            "Epoch 9 Step 16 of 219, loss = 0.6538568618120735\n",
            "Epoch 9 Step 17 of 219, loss = 0.00024098907942970982\n",
            "Epoch 9 Step 18 of 219, loss = 0.6989961044107531\n",
            "Epoch 9 Step 19 of 219, loss = 0.0005359657545795926\n",
            "Epoch 9 Step 20 of 219, loss = 0.0002648426058158293\n",
            "Epoch 9 Step 21 of 219, loss = 0.0011091748215221742\n",
            "Epoch 9 Step 22 of 219, loss = 0.00722952897012874\n",
            "Epoch 9 Step 23 of 219, loss = 0.8762707761597994\n",
            "Epoch 9 Step 24 of 219, loss = 0.0031526965827879394\n",
            "Epoch 9 Step 25 of 219, loss = 0.5099563344045919\n",
            "Epoch 9 Step 26 of 219, loss = 0.9597369094685746\n",
            "Epoch 9 Step 27 of 219, loss = 0.0008970719272838323\n",
            "Epoch 9 Step 28 of 219, loss = 5.162297009064787e-05\n",
            "Epoch 9 Step 29 of 219, loss = 0.3196252182395938\n",
            "Epoch 9 Step 30 of 219, loss = 0.017434102276411068\n",
            "Epoch 9 Step 31 of 219, loss = 0.5113276659794792\n",
            "Epoch 9 Step 32 of 219, loss = 0.000115601970719581\n",
            "Epoch 9 Step 33 of 219, loss = 0.039954574757757655\n",
            "Epoch 9 Step 34 of 219, loss = 0.7244692266876882\n",
            "Epoch 9 Step 35 of 219, loss = 0.0009276726436837635\n",
            "Epoch 9 Step 36 of 219, loss = 0.0012017311782983597\n",
            "Epoch 9 Step 37 of 219, loss = 0.0007391316235043632\n",
            "Epoch 9 Step 38 of 219, loss = 0.00024225177116932173\n",
            "Epoch 9 Step 39 of 219, loss = 0.7580833649931265\n",
            "Epoch 9 Step 40 of 219, loss = 1.3100281725564855\n",
            "Epoch 9 Step 41 of 219, loss = 0.0009588657658241573\n",
            "Epoch 9 Step 42 of 219, loss = 0.0008846167190768028\n",
            "Epoch 9 Step 43 of 219, loss = 0.00016510013642800914\n",
            "Epoch 9 Step 44 of 219, loss = 0.0006222522702046263\n",
            "Epoch 9 Step 45 of 219, loss = 0.0003663508626914336\n",
            "Epoch 9 Step 46 of 219, loss = 0.0003221974234293157\n",
            "Epoch 9 Step 47 of 219, loss = 0.6289240837738816\n",
            "Epoch 9 Step 48 of 219, loss = 0.00018112829684469034\n",
            "Epoch 9 Step 49 of 219, loss = 0.6986407136578237\n",
            "Epoch 9 Step 50 of 219, loss = 0.0002793151930973181\n",
            "Epoch 9 Step 51 of 219, loss = 0.000920013652603302\n",
            "Epoch 9 Step 52 of 219, loss = 0.3510846553090232\n",
            "Epoch 9 Step 53 of 219, loss = 0.0008169301436282694\n",
            "Epoch 9 Step 54 of 219, loss = 0.0015066826954353019\n",
            "Epoch 9 Step 55 of 219, loss = 0.0009676867375674192\n",
            "Epoch 9 Step 56 of 219, loss = 0.0007835939263713954\n",
            "Epoch 9 Step 57 of 219, loss = 0.0022122159857644874\n",
            "Epoch 9 Step 58 of 219, loss = 0.0037213342757240753\n",
            "Epoch 9 Step 59 of 219, loss = 0.2208806670573722\n",
            "Epoch 9 Step 60 of 219, loss = 0.004015023542706331\n",
            "Epoch 9 Step 61 of 219, loss = 0.0004281628905573598\n",
            "Epoch 9 Step 62 of 219, loss = 0.0021477845141362195\n",
            "Epoch 9 Step 63 of 219, loss = 0.011764789603603276\n",
            "Epoch 9 Step 64 of 219, loss = 9.485509895057476e-05\n",
            "Epoch 9 Step 65 of 219, loss = 0.0001668248005444184\n",
            "Epoch 9 Step 66 of 219, loss = 0.00023416218584770832\n",
            "Epoch 9 Step 67 of 219, loss = 0.7050670664709742\n",
            "Epoch 9 Step 68 of 219, loss = 0.02190375451169757\n",
            "Epoch 9 Step 69 of 219, loss = 0.0003187550371421821\n",
            "Epoch 9 Step 70 of 219, loss = 0.00022748022638552357\n",
            "Epoch 9 Step 71 of 219, loss = 0.035226833170327154\n",
            "Epoch 9 Step 72 of 219, loss = 0.0002686018738131679\n",
            "Epoch 9 Step 73 of 219, loss = 0.0008983699177633753\n",
            "Epoch 9 Step 74 of 219, loss = 0.0018503509263609885\n",
            "Epoch 9 Step 75 of 219, loss = 0.0005388697541093279\n",
            "Epoch 9 Step 76 of 219, loss = 0.003610770272644004\n",
            "Epoch 9 Step 77 of 219, loss = 0.0014579264970961958\n",
            "Epoch 9 Step 78 of 219, loss = 0.0005410381090769079\n",
            "Epoch 9 Step 79 of 219, loss = 0.0006650132158938504\n",
            "Epoch 9 Step 80 of 219, loss = 0.0007057726543280296\n",
            "Epoch 9 Step 81 of 219, loss = 0.000569545858525089\n",
            "Epoch 9 Step 82 of 219, loss = 0.00028098840971324535\n",
            "Epoch 9 Step 83 of 219, loss = 0.0014051937387193902\n",
            "Epoch 9 Step 84 of 219, loss = 0.0007725740924797719\n",
            "Epoch 9 Step 85 of 219, loss = 0.0024237151446868666\n",
            "Epoch 9 Step 86 of 219, loss = 0.0004101275874290877\n",
            "Epoch 9 Step 87 of 219, loss = 0.0033498103312012972\n",
            "Epoch 9 Step 88 of 219, loss = 0.0018132827208319213\n",
            "Epoch 9 Step 89 of 219, loss = 0.34451024405370845\n",
            "Epoch 9 Step 90 of 219, loss = 0.0018905987512880529\n",
            "Epoch 9 Step 91 of 219, loss = 0.021133889543136775\n",
            "Epoch 9 Step 92 of 219, loss = 0.00010474762473222654\n",
            "Epoch 9 Step 93 of 219, loss = 0.00023439723122464784\n",
            "Epoch 9 Step 94 of 219, loss = 0.00019220497051719576\n",
            "Epoch 9 Step 95 of 219, loss = 0.9062213195951472\n",
            "Epoch 9 Step 96 of 219, loss = 0.0003086802964844537\n",
            "Epoch 9 Step 97 of 219, loss = 7.979178917594254e-05\n",
            "Epoch 9 Step 98 of 219, loss = 1.0971286223846164\n",
            "Epoch 9 Step 99 of 219, loss = 0.00041932411295420025\n",
            "Epoch 9 Step 100 of 219, loss = 0.0005529568902602477\n",
            "Epoch 9 Step 101 of 219, loss = 0.3472342928484977\n",
            "Epoch 9 Step 102 of 219, loss = 0.0005209061187088082\n",
            "Epoch 9 Step 103 of 219, loss = 0.00023335379205491336\n",
            "Epoch 9 Step 104 of 219, loss = 0.00020725830404444423\n",
            "Epoch 9 Step 105 of 219, loss = 0.0001796558403839299\n",
            "Epoch 9 Step 106 of 219, loss = 0.6640745169379443\n",
            "Epoch 9 Step 107 of 219, loss = 0.00037413174527500814\n",
            "Epoch 9 Step 108 of 219, loss = 0.005318066610016103\n",
            "Epoch 9 Step 109 of 219, loss = 0.16592467955746315\n",
            "Epoch 9 Step 110 of 219, loss = 0.6066850732086095\n",
            "Epoch 9 Step 111 of 219, loss = 0.05495245653673919\n",
            "Epoch 9 Step 112 of 219, loss = 0.00043736494058066455\n",
            "Epoch 9 Step 113 of 219, loss = 0.0012189207413939585\n",
            "Epoch 9 Step 114 of 219, loss = 0.022404335425335375\n",
            "Epoch 9 Step 115 of 219, loss = 0.01708546365955499\n",
            "Epoch 9 Step 116 of 219, loss = 0.0029973134315923744\n",
            "Epoch 9 Step 117 of 219, loss = 0.0009435036358809157\n",
            "Epoch 9 Step 118 of 219, loss = 0.0001782524554982956\n",
            "Epoch 9 Step 119 of 219, loss = 8.707664306939478e-05\n",
            "Epoch 9 Step 120 of 219, loss = 9.966009747586213e-05\n",
            "Epoch 9 Step 121 of 219, loss = 8.115377386275213e-05\n",
            "Epoch 9 Step 122 of 219, loss = 7.97604132003471e-05\n",
            "Epoch 9 Step 123 of 219, loss = 6.811736193412798e-05\n",
            "Epoch 9 Step 124 of 219, loss = 0.00027710851350093435\n",
            "Epoch 9 Step 125 of 219, loss = 0.8081632283490308\n",
            "Epoch 9 Step 126 of 219, loss = 0.000595203500097341\n",
            "Epoch 9 Step 127 of 219, loss = 3.325132161080546e-05\n",
            "Epoch 9 Step 128 of 219, loss = 0.00013090655420455732\n",
            "Epoch 9 Step 129 of 219, loss = 0.00042814593098228215\n",
            "Epoch 9 Step 130 of 219, loss = 0.0008354094824767344\n",
            "Epoch 9 Step 131 of 219, loss = 2.6680062603645638e-05\n",
            "Epoch 9 Step 132 of 219, loss = 0.00023716293389952625\n",
            "Epoch 9 Step 133 of 219, loss = 0.017391603240639597\n",
            "Epoch 9 Step 134 of 219, loss = 0.004634012508233809\n",
            "Epoch 9 Step 135 of 219, loss = 0.018347999918148616\n",
            "Epoch 9 Step 136 of 219, loss = 0.0228738521516334\n",
            "Epoch 9 Step 137 of 219, loss = 0.5792405606914599\n",
            "Epoch 9 Step 138 of 219, loss = 0.00044045427966921125\n",
            "Epoch 9 Step 139 of 219, loss = 0.00015424226296545385\n",
            "Epoch 9 Step 140 of 219, loss = 0.5637391437651331\n",
            "Epoch 9 Step 141 of 219, loss = 0.0016002029434503129\n",
            "Epoch 9 Step 142 of 219, loss = 0.000529390497604254\n",
            "Epoch 9 Step 143 of 219, loss = 0.46973487341944065\n",
            "Epoch 9 Step 144 of 219, loss = 0.0002746266035842382\n",
            "Epoch 9 Step 145 of 219, loss = 0.0009657146224526514\n",
            "Epoch 9 Step 146 of 219, loss = 0.01207098286602104\n",
            "Epoch 9 Step 147 of 219, loss = 0.011159064804360241\n",
            "Epoch 9 Step 148 of 219, loss = 0.0082326783531812\n",
            "Epoch 9 Step 149 of 219, loss = 0.0009601192430181982\n",
            "Epoch 9 Step 150 of 219, loss = 0.47246688123891545\n",
            "Epoch 9 Step 151 of 219, loss = 0.00018160735521632887\n",
            "Epoch 9 Step 152 of 219, loss = 0.00012487167236940877\n",
            "Epoch 9 Step 153 of 219, loss = 0.0006634850395812464\n",
            "Epoch 9 Step 154 of 219, loss = 0.00014047457830201893\n",
            "Epoch 9 Step 155 of 219, loss = 0.00023995007859412\n",
            "Epoch 9 Step 156 of 219, loss = 0.00019838276102746022\n",
            "Epoch 9 Step 157 of 219, loss = 0.0003609133427744382\n",
            "Epoch 9 Step 158 of 219, loss = 0.00023972550752660027\n",
            "Epoch 9 Step 159 of 219, loss = 0.6612042404440217\n",
            "Epoch 9 Step 160 of 219, loss = 0.011629901950982457\n",
            "Epoch 9 Step 161 of 219, loss = 0.0001676618819601572\n",
            "Epoch 9 Step 162 of 219, loss = 0.00040538284974900307\n",
            "Epoch 9 Step 163 of 219, loss = 0.00027536174638953526\n",
            "Epoch 9 Step 164 of 219, loss = 0.0004646349307222408\n",
            "Epoch 9 Step 165 of 219, loss = 0.003823849799573509\n",
            "Epoch 9 Step 166 of 219, loss = 0.005085731197596033\n",
            "Epoch 9 Step 167 of 219, loss = 0.0006703714705054153\n",
            "Epoch 9 Step 168 of 219, loss = 0.21789803615070014\n",
            "Epoch 9 Step 169 of 219, loss = 0.0001339050884325843\n",
            "Epoch 9 Step 170 of 219, loss = 0.0002893687365030928\n",
            "Epoch 9 Step 171 of 219, loss = 0.7870881861529142\n",
            "Epoch 9 Step 172 of 219, loss = 1.0655193387383406\n",
            "Epoch 9 Step 173 of 219, loss = 0.00014252972437134304\n",
            "Epoch 9 Step 174 of 219, loss = 0.00037970457424307824\n",
            "Epoch 9 Step 175 of 219, loss = 0.769768402975842\n",
            "Epoch 9 Step 176 of 219, loss = 0.00046064071943874296\n",
            "Epoch 9 Step 177 of 219, loss = 0.0018084639664266433\n",
            "Epoch 9 Step 178 of 219, loss = 0.0003672191892292176\n",
            "Epoch 9 Step 179 of 219, loss = 0.00013348812990443548\n",
            "Epoch 9 Step 180 of 219, loss = 0.00028678719581876067\n",
            "Epoch 9 Step 181 of 219, loss = 0.0010143913401634563\n",
            "Epoch 9 Step 182 of 219, loss = 0.7202245112484889\n",
            "Epoch 9 Step 183 of 219, loss = 0.0001991563344745373\n",
            "Epoch 9 Step 184 of 219, loss = 0.0006792621170461643\n",
            "Epoch 9 Step 185 of 219, loss = 0.0003072215943120682\n",
            "Epoch 9 Step 186 of 219, loss = 0.00023870793643254729\n",
            "Epoch 9 Step 187 of 219, loss = 0.0006079186418901372\n",
            "Epoch 9 Step 188 of 219, loss = 0.778158822561636\n",
            "Epoch 9 Step 189 of 219, loss = 0.00013826138183503645\n",
            "Epoch 9 Step 190 of 219, loss = 0.001008238295753472\n",
            "Epoch 9 Step 191 of 219, loss = 0.37244898281733185\n",
            "Epoch 9 Step 192 of 219, loss = 0.005784858436072682\n",
            "Epoch 9 Step 193 of 219, loss = 0.00032394417121395236\n",
            "Epoch 9 Step 194 of 219, loss = 0.0009575437345574755\n",
            "Epoch 9 Step 195 of 219, loss = 0.024233018806626205\n",
            "Epoch 9 Step 196 of 219, loss = 0.00023163303649198497\n",
            "Epoch 9 Step 197 of 219, loss = 0.8882773492998695\n",
            "Epoch 9 Step 198 of 219, loss = 0.0004387097203562007\n",
            "Epoch 9 Step 199 of 219, loss = 0.0007612067618083529\n",
            "Epoch 9 Step 200 of 219, loss = 0.006945940598370726\n",
            "Epoch 9 Step 201 of 219, loss = 0.004191324881503533\n",
            "Epoch 9 Step 202 of 219, loss = 0.0002342857956136868\n",
            "Epoch 9 Step 203 of 219, loss = 0.0005646191918913246\n",
            "Epoch 9 Step 204 of 219, loss = 0.0004945664395563654\n",
            "Epoch 9 Step 205 of 219, loss = 0.00017248284439119743\n",
            "Epoch 9 Step 206 of 219, loss = 0.2887407870698553\n",
            "Epoch 9 Step 207 of 219, loss = 0.5621136313727675\n",
            "Epoch 9 Step 208 of 219, loss = 0.7436436284444881\n",
            "Epoch 9 Step 209 of 219, loss = 0.0012134239520946721\n",
            "Epoch 9 Step 210 of 219, loss = 0.16076651118009977\n",
            "Epoch 9 Step 211 of 219, loss = 0.0011811700903763267\n",
            "Epoch 9 Step 212 of 219, loss = 0.7792674413544773\n",
            "Epoch 9 Step 213 of 219, loss = 0.0006161411522498383\n",
            "Epoch 9 Step 214 of 219, loss = 0.06773545006763015\n",
            "Epoch 9 Step 215 of 219, loss = 0.00018921359128398763\n",
            "Epoch 9 Step 216 of 219, loss = 0.0008353628059012408\n",
            "Epoch 9 Step 217 of 219, loss = 0.0005272940707072848\n",
            "Epoch 9 Step 218 of 219, loss = 0.0009380530773341889\n",
            "Epoch 9 average train_loss: 0.123722 test_loss: 2.917165 test_score 0.54%\n",
            "Epoch 10 Step 0 of 219, loss = 0.0003824892456236739\n",
            "Epoch 10 Step 1 of 219, loss = 0.00030666109660160146\n",
            "Epoch 10 Step 2 of 219, loss = 0.006392706169663143\n",
            "Epoch 10 Step 3 of 219, loss = 0.1424703320478784\n",
            "Epoch 10 Step 4 of 219, loss = 0.00012287187757920037\n",
            "Epoch 10 Step 5 of 219, loss = 5.6346733003920235e-05\n",
            "Epoch 10 Step 6 of 219, loss = 4.6646599344057904e-05\n",
            "Epoch 10 Step 7 of 219, loss = 9.297807037000894e-05\n",
            "Epoch 10 Step 8 of 219, loss = 5.43127669061505e-05\n",
            "Epoch 10 Step 9 of 219, loss = 7.272047275819205e-05\n",
            "Epoch 10 Step 10 of 219, loss = 1.4479567818279975\n",
            "Epoch 10 Step 11 of 219, loss = 6.295354074836723e-05\n",
            "Epoch 10 Step 12 of 219, loss = 6.998729099905177e-05\n",
            "Epoch 10 Step 13 of 219, loss = 0.4761310944018078\n",
            "Epoch 10 Step 14 of 219, loss = 0.885486965850987\n",
            "Epoch 10 Step 15 of 219, loss = 0.00012367180858063875\n",
            "Epoch 10 Step 16 of 219, loss = 1.2058002416871432\n",
            "Epoch 10 Step 17 of 219, loss = 0.0002194193157265545\n",
            "Epoch 10 Step 18 of 219, loss = 3.8149380779309467\n",
            "Epoch 10 Step 19 of 219, loss = 1.797180919701077\n",
            "Epoch 10 Step 20 of 219, loss = 0.6748628729746997\n",
            "Epoch 10 Step 21 of 219, loss = 0.32688785912728235\n",
            "Epoch 10 Step 22 of 219, loss = 0.14546729261394375\n",
            "Epoch 10 Step 23 of 219, loss = 0.8057057653968513\n",
            "Epoch 10 Step 24 of 219, loss = 0.010332681630643492\n",
            "Epoch 10 Step 25 of 219, loss = 0.06602788742702614\n",
            "Epoch 10 Step 26 of 219, loss = 0.0002450092365506862\n",
            "Epoch 10 Step 27 of 219, loss = 0.44925714989585686\n",
            "Epoch 10 Step 28 of 219, loss = 1.6011163211260282e-05\n",
            "Epoch 10 Step 29 of 219, loss = 0.4104145177195164\n",
            "Epoch 10 Step 30 of 219, loss = 0.0007142063537344256\n",
            "Epoch 10 Step 31 of 219, loss = 0.28232398426717964\n",
            "Epoch 10 Step 32 of 219, loss = 0.0001484124550188426\n",
            "Epoch 10 Step 33 of 219, loss = 1.6514441994954723\n",
            "Epoch 10 Step 34 of 219, loss = 1.1371047992299452\n",
            "Epoch 10 Step 35 of 219, loss = 4.218391728727511e-05\n",
            "Epoch 10 Step 36 of 219, loss = 0.0008830223291056427\n",
            "Epoch 10 Step 37 of 219, loss = 0.00028462556423392016\n",
            "Epoch 10 Step 38 of 219, loss = 3.241686431465496e-05\n",
            "Epoch 10 Step 39 of 219, loss = 0.001732105101950765\n",
            "Epoch 10 Step 40 of 219, loss = 0.00016056484105320123\n",
            "Epoch 10 Step 41 of 219, loss = 0.000244842344159224\n",
            "Epoch 10 Step 42 of 219, loss = 0.25241983666501255\n",
            "Epoch 10 Step 43 of 219, loss = 0.389429016013878\n",
            "Epoch 10 Step 44 of 219, loss = 0.00039465498082336126\n",
            "Epoch 10 Step 45 of 219, loss = 0.4494599202557765\n",
            "Epoch 10 Step 46 of 219, loss = 0.0017468219580223376\n",
            "Epoch 10 Step 47 of 219, loss = 0.0043860103005499695\n",
            "Epoch 10 Step 48 of 219, loss = 0.009562892470455608\n",
            "Epoch 10 Step 49 of 219, loss = 0.5401555830555935\n",
            "Epoch 10 Step 50 of 219, loss = 0.015220206250944557\n",
            "Epoch 10 Step 51 of 219, loss = 2.1412703063106164e-05\n",
            "Epoch 10 Step 52 of 219, loss = 4.0045684215783695e-05\n",
            "Epoch 10 Step 53 of 219, loss = 0.0004826194106044568\n",
            "Epoch 10 Step 54 of 219, loss = 0.0025718586148286704\n",
            "Epoch 10 Step 55 of 219, loss = 1.587704434768966e-05\n",
            "Epoch 10 Step 56 of 219, loss = 0.0005170125775748602\n",
            "Epoch 10 Step 57 of 219, loss = 0.0007134152183425613\n",
            "Epoch 10 Step 58 of 219, loss = 0.000513366556560868\n",
            "Epoch 10 Step 59 of 219, loss = 0.058867788279314937\n",
            "Epoch 10 Step 60 of 219, loss = 0.0005210300321323302\n",
            "Epoch 10 Step 61 of 219, loss = 0.0002913077840958067\n",
            "Epoch 10 Step 62 of 219, loss = 0.0020910589760205767\n",
            "Epoch 10 Step 63 of 219, loss = 0.0017274139135565747\n",
            "Epoch 10 Step 64 of 219, loss = 1.725537484276174e-05\n",
            "Epoch 10 Step 65 of 219, loss = 2.297729599831655e-05\n",
            "Epoch 10 Step 66 of 219, loss = 6.644071680739216e-05\n",
            "Epoch 10 Step 67 of 219, loss = 0.004994592528703379\n",
            "Epoch 10 Step 68 of 219, loss = 0.05294404655160179\n",
            "Epoch 10 Step 69 of 219, loss = 7.148621693886525e-05\n",
            "Epoch 10 Step 70 of 219, loss = 5.135974964787238e-05\n",
            "Epoch 10 Step 71 of 219, loss = 0.005168134402538271\n",
            "Epoch 10 Step 72 of 219, loss = 4.715917140174497e-05\n",
            "Epoch 10 Step 73 of 219, loss = 0.0003307086443555818\n",
            "Epoch 10 Step 74 of 219, loss = 0.00020024302000365424\n",
            "Epoch 10 Step 75 of 219, loss = 0.00046177884223652654\n",
            "Epoch 10 Step 76 of 219, loss = 0.000687958723688098\n",
            "Epoch 10 Step 77 of 219, loss = 0.014362562605583662\n",
            "Epoch 10 Step 78 of 219, loss = 7.142878365584693e-05\n",
            "Epoch 10 Step 79 of 219, loss = 4.1513244696034235e-05\n",
            "Epoch 10 Step 80 of 219, loss = 5.356598171601945e-05\n",
            "Epoch 10 Step 81 of 219, loss = 4.1096258428297006e-05\n",
            "Epoch 10 Step 82 of 219, loss = 9.25431990594916e-05\n",
            "Epoch 10 Step 83 of 219, loss = 0.002089883290977923\n",
            "Epoch 10 Step 84 of 219, loss = 0.00012972678086953238\n",
            "Epoch 10 Step 85 of 219, loss = 0.00020635370856325608\n",
            "Epoch 10 Step 86 of 219, loss = 6.695165893688682e-05\n",
            "Epoch 10 Step 87 of 219, loss = 0.0004700517995388509\n",
            "Epoch 10 Step 88 of 219, loss = 8.750656820666336e-05\n",
            "Epoch 10 Step 89 of 219, loss = 0.15068462703118257\n",
            "Epoch 10 Step 90 of 219, loss = 0.006459081264324595\n",
            "Epoch 10 Step 91 of 219, loss = 0.45894244156454533\n",
            "Epoch 10 Step 92 of 219, loss = 4.754787596539245e-05\n",
            "Epoch 10 Step 93 of 219, loss = 0.0006170612815026288\n",
            "Epoch 10 Step 94 of 219, loss = 0.0004999787201995787\n",
            "Epoch 10 Step 95 of 219, loss = 0.0024336947467418213\n",
            "Epoch 10 Step 96 of 219, loss = 0.0008818913828463337\n",
            "Epoch 10 Step 97 of 219, loss = 3.35566855937941e-05\n",
            "Epoch 10 Step 98 of 219, loss = 0.0008795811634172424\n",
            "Epoch 10 Step 99 of 219, loss = 0.00042187187807485316\n",
            "Epoch 10 Step 100 of 219, loss = 0.00015945924292282143\n",
            "Epoch 10 Step 101 of 219, loss = 0.00024867624506441643\n",
            "Epoch 10 Step 102 of 219, loss = 0.0008123056070985513\n",
            "Epoch 10 Step 103 of 219, loss = 3.924840905256133e-05\n",
            "Epoch 10 Step 104 of 219, loss = 0.08859356546429353\n",
            "Epoch 10 Step 105 of 219, loss = 2.4065048023658164e-05\n",
            "Epoch 10 Step 106 of 219, loss = 0.16113931237373436\n",
            "Epoch 10 Step 107 of 219, loss = 7.277794657056802e-05\n",
            "Epoch 10 Step 108 of 219, loss = 0.21105863211721498\n",
            "Epoch 10 Step 109 of 219, loss = 0.2042057954287202\n",
            "Epoch 10 Step 110 of 219, loss = 0.6163452754833543\n",
            "Epoch 10 Step 111 of 219, loss = 0.01199009986441979\n",
            "Epoch 10 Step 112 of 219, loss = 0.736142842787558\n",
            "Epoch 10 Step 113 of 219, loss = 0.09415984916950038\n",
            "Epoch 10 Step 114 of 219, loss = 0.0014769752198162678\n",
            "Epoch 10 Step 115 of 219, loss = 0.0024063600104682337\n",
            "Epoch 10 Step 116 of 219, loss = 0.00034291143606424157\n",
            "Epoch 10 Step 117 of 219, loss = 0.000135631585635565\n",
            "Epoch 10 Step 118 of 219, loss = 0.0004925003223661406\n",
            "Epoch 10 Step 119 of 219, loss = 0.00016670382819938823\n",
            "Epoch 10 Step 120 of 219, loss = 0.002949784881366213\n",
            "Epoch 10 Step 121 of 219, loss = 0.0018788657514505758\n",
            "Epoch 10 Step 122 of 219, loss = 0.0005470621240988294\n",
            "Epoch 10 Step 123 of 219, loss = 7.63783406227958e-05\n",
            "Epoch 10 Step 124 of 219, loss = 6.827246443208423e-05\n",
            "Epoch 10 Step 125 of 219, loss = 0.001384552556487506\n",
            "Epoch 10 Step 126 of 219, loss = 0.00020308872387886368\n",
            "Epoch 10 Step 127 of 219, loss = 2.4146971554728225e-05\n",
            "Epoch 10 Step 128 of 219, loss = 0.7735437414798412\n",
            "Epoch 10 Step 129 of 219, loss = 0.0002011235606005357\n",
            "Epoch 10 Step 130 of 219, loss = 9.665531956670748e-05\n",
            "Epoch 10 Step 131 of 219, loss = 7.063247284122554e-05\n",
            "Epoch 10 Step 132 of 219, loss = 0.0013074403112796062\n",
            "Epoch 10 Step 133 of 219, loss = 0.0019510196416376857\n",
            "Epoch 10 Step 134 of 219, loss = 0.0002746945826288538\n",
            "Epoch 10 Step 135 of 219, loss = 0.0006360061693726493\n",
            "Epoch 10 Step 136 of 219, loss = 8.295523406331995e-05\n",
            "Epoch 10 Step 137 of 219, loss = 0.5574511264337616\n",
            "Epoch 10 Step 138 of 219, loss = 1.1492579238379221\n",
            "Epoch 10 Step 139 of 219, loss = 0.0009837931940523958\n",
            "Epoch 10 Step 140 of 219, loss = 0.020002232023273336\n",
            "Epoch 10 Step 141 of 219, loss = 0.00014382359580622506\n",
            "Epoch 10 Step 142 of 219, loss = 0.001344645721019333\n",
            "Epoch 10 Step 143 of 219, loss = 0.0003948501555441908\n",
            "Epoch 10 Step 144 of 219, loss = 0.0004666616317194894\n",
            "Epoch 10 Step 145 of 219, loss = 0.00019250230849365835\n",
            "Epoch 10 Step 146 of 219, loss = 0.00018708207710460556\n",
            "Epoch 10 Step 147 of 219, loss = 0.0007007104987906132\n",
            "Epoch 10 Step 148 of 219, loss = 8.513211867011705e-05\n",
            "Epoch 10 Step 149 of 219, loss = 0.00016232223015322234\n",
            "Epoch 10 Step 150 of 219, loss = 0.0020839377286847593\n",
            "Epoch 10 Step 151 of 219, loss = 0.00020321709257586917\n",
            "Epoch 10 Step 152 of 219, loss = 0.0005741474552678483\n",
            "Epoch 10 Step 153 of 219, loss = 0.020393070645241096\n",
            "Epoch 10 Step 154 of 219, loss = 8.467795623801067e-05\n",
            "Epoch 10 Step 155 of 219, loss = 0.18541195539887667\n",
            "Epoch 10 Step 156 of 219, loss = 7.366768079464237e-05\n",
            "Epoch 10 Step 157 of 219, loss = 0.00010860537418011518\n",
            "Epoch 10 Step 158 of 219, loss = 0.24070303353130384\n",
            "Epoch 10 Step 159 of 219, loss = 0.6769744706168694\n",
            "Epoch 10 Step 160 of 219, loss = 0.31715032148918\n",
            "Epoch 10 Step 161 of 219, loss = 0.00027706258947546303\n",
            "Epoch 10 Step 162 of 219, loss = 0.6979248871996333\n",
            "Epoch 10 Step 163 of 219, loss = 0.0002881175369111588\n",
            "Epoch 10 Step 164 of 219, loss = 0.0010172725342272315\n",
            "Epoch 10 Step 165 of 219, loss = 0.00013559752142100479\n",
            "Epoch 10 Step 166 of 219, loss = 0.0005784613335890754\n",
            "Epoch 10 Step 167 of 219, loss = 0.00024463548106723465\n",
            "Epoch 10 Step 168 of 219, loss = 0.04276993664791462\n",
            "Epoch 10 Step 169 of 219, loss = 0.00018643727980816038\n",
            "Epoch 10 Step 170 of 219, loss = 0.007815939827366947\n",
            "Epoch 10 Step 171 of 219, loss = 0.11028196360484799\n",
            "Epoch 10 Step 172 of 219, loss = 0.0007591909442226097\n",
            "Epoch 10 Step 173 of 219, loss = 0.0007691354506960124\n",
            "Epoch 10 Step 174 of 219, loss = 0.8175359969961846\n",
            "Epoch 10 Step 175 of 219, loss = 0.32447877450692886\n",
            "Epoch 10 Step 176 of 219, loss = 0.001247993549441162\n",
            "Epoch 10 Step 177 of 219, loss = 0.6283656701243672\n",
            "Epoch 10 Step 178 of 219, loss = 0.0011057488918595482\n",
            "Epoch 10 Step 179 of 219, loss = 0.4112665287166237\n",
            "Epoch 10 Step 180 of 219, loss = 0.0008231152146436216\n",
            "Epoch 10 Step 181 of 219, loss = 0.55505362623785\n",
            "Epoch 10 Step 182 of 219, loss = 0.0008058967798660888\n",
            "Epoch 10 Step 183 of 219, loss = 0.00015808122475391428\n",
            "Epoch 10 Step 184 of 219, loss = 0.004077286647543588\n",
            "Epoch 10 Step 185 of 219, loss = 0.00027506036803970346\n",
            "Epoch 10 Step 186 of 219, loss = 0.19244879869143006\n",
            "Epoch 10 Step 187 of 219, loss = 0.00025553634509378753\n",
            "Epoch 10 Step 188 of 219, loss = 0.0004378881312732119\n",
            "Epoch 10 Step 189 of 219, loss = 0.0007460323030272775\n",
            "Epoch 10 Step 190 of 219, loss = 0.00020309085766712087\n",
            "Epoch 10 Step 191 of 219, loss = 1.155190694290468\n",
            "Epoch 10 Step 192 of 219, loss = 0.501262604470412\n",
            "Epoch 10 Step 193 of 219, loss = 0.00014852055073788506\n",
            "Epoch 10 Step 194 of 219, loss = 0.00048132001836620475\n",
            "Epoch 10 Step 195 of 219, loss = 0.0003042995425630579\n",
            "Epoch 10 Step 196 of 219, loss = 0.0003458647754541744\n",
            "Epoch 10 Step 197 of 219, loss = 0.003392053114112059\n",
            "Epoch 10 Step 198 of 219, loss = 0.00011154824346704117\n",
            "Epoch 10 Step 199 of 219, loss = 0.0007806146786606405\n",
            "Epoch 10 Step 200 of 219, loss = 0.0017358299903662555\n",
            "Epoch 10 Step 201 of 219, loss = 0.00019549063199519878\n",
            "Epoch 10 Step 202 of 219, loss = 0.4125212856413327\n",
            "Epoch 10 Step 203 of 219, loss = 0.01565018368546589\n",
            "Epoch 10 Step 204 of 219, loss = 0.00033789842530040914\n",
            "Epoch 10 Step 205 of 219, loss = 0.0003262632401401788\n",
            "Epoch 10 Step 206 of 219, loss = 0.00013661390323704836\n",
            "Epoch 10 Step 207 of 219, loss = 0.0017577234966665856\n",
            "Epoch 10 Step 208 of 219, loss = 0.00022619151536673598\n",
            "Epoch 10 Step 209 of 219, loss = 0.0002614749776057579\n",
            "Epoch 10 Step 210 of 219, loss = 0.0002534378357950118\n",
            "Epoch 10 Step 211 of 219, loss = 0.00040344000012737524\n",
            "Epoch 10 Step 212 of 219, loss = 0.5596310399059803\n",
            "Epoch 10 Step 213 of 219, loss = 0.17914014747509555\n",
            "Epoch 10 Step 214 of 219, loss = 0.0002966824326904316\n",
            "Epoch 10 Step 215 of 219, loss = 0.00028468285631788603\n",
            "Epoch 10 Step 216 of 219, loss = 0.0004249428352522955\n",
            "Epoch 10 Step 217 of 219, loss = 0.0009908233764690522\n",
            "Epoch 10 Step 218 of 219, loss = 0.00030840067483950406\n",
            "Epoch 10 average train_loss: 0.141132 test_loss: 2.901446 test_score 0.57%\n",
            "Epoch 11 Step 0 of 219, loss = 0.0002291961524178987\n",
            "Epoch 11 Step 1 of 219, loss = 4.031393876857692e-05\n",
            "Epoch 11 Step 2 of 219, loss = 0.0009015030912564725\n",
            "Epoch 11 Step 3 of 219, loss = 0.9874109974904286\n",
            "Epoch 11 Step 4 of 219, loss = 5.43575263236562e-05\n",
            "Epoch 11 Step 5 of 219, loss = 6.691797580060665e-05\n",
            "Epoch 11 Step 6 of 219, loss = 8.741076078422338e-05\n",
            "Epoch 11 Step 7 of 219, loss = 0.00010940920498114792\n",
            "Epoch 11 Step 8 of 219, loss = 0.0003431843337011742\n",
            "Epoch 11 Step 9 of 219, loss = 0.008865125668990004\n",
            "Epoch 11 Step 10 of 219, loss = 2.2033479352363656\n",
            "Epoch 11 Step 11 of 219, loss = 0.00111250388249573\n",
            "Epoch 11 Step 12 of 219, loss = 0.0007213819410480937\n",
            "Epoch 11 Step 13 of 219, loss = 0.0003063496250206299\n",
            "Epoch 11 Step 14 of 219, loss = 1.0155270120694126\n",
            "Epoch 11 Step 15 of 219, loss = 3.7736405602117884e-05\n",
            "Epoch 11 Step 16 of 219, loss = 0.025028382051516473\n",
            "Epoch 11 Step 17 of 219, loss = 0.4197979959988629\n",
            "Epoch 11 Step 18 of 219, loss = 0.8538225230195167\n",
            "Epoch 11 Step 19 of 219, loss = 0.0005911319451570307\n",
            "Epoch 11 Step 20 of 219, loss = 4.7621449311918695e-05\n",
            "Epoch 11 Step 21 of 219, loss = 0.1290742343446709\n",
            "Epoch 11 Step 22 of 219, loss = 0.010738835052961804\n",
            "Epoch 11 Step 23 of 219, loss = 0.022982310500538006\n",
            "Epoch 11 Step 24 of 219, loss = 0.0002245702767140756\n",
            "Epoch 11 Step 25 of 219, loss = 0.7125788814807947\n",
            "Epoch 11 Step 26 of 219, loss = 0.00018713573552986418\n",
            "Epoch 11 Step 27 of 219, loss = 0.14917943852697135\n",
            "Epoch 11 Step 28 of 219, loss = 3.442807411602189e-05\n",
            "Epoch 11 Step 29 of 219, loss = 0.0002908636082565863\n",
            "Epoch 11 Step 30 of 219, loss = 5.829065969464864e-05\n",
            "Epoch 11 Step 31 of 219, loss = 0.0030029958852537675\n",
            "Epoch 11 Step 32 of 219, loss = 4.34200751442404e-05\n",
            "Epoch 11 Step 33 of 219, loss = 0.34086940118891107\n",
            "Epoch 11 Step 34 of 219, loss = 0.0003153066229515389\n",
            "Epoch 11 Step 35 of 219, loss = 5.7209910778510675e-05\n",
            "Epoch 11 Step 36 of 219, loss = 5.49904060562767e-05\n",
            "Epoch 11 Step 37 of 219, loss = 6.441281715297009e-05\n",
            "Epoch 11 Step 38 of 219, loss = 3.604516609811981e-05\n",
            "Epoch 11 Step 39 of 219, loss = 7.405336725696543e-05\n",
            "Epoch 11 Step 40 of 219, loss = 8.220272343351098e-05\n",
            "Epoch 11 Step 41 of 219, loss = 9.397532107868756e-05\n",
            "Epoch 11 Step 42 of 219, loss = 8.700246496573527e-05\n",
            "Epoch 11 Step 43 of 219, loss = 5.391019720946133e-05\n",
            "Epoch 11 Step 44 of 219, loss = 0.000126999110534598\n",
            "Epoch 11 Step 45 of 219, loss = 8.519848466903568e-05\n",
            "Epoch 11 Step 46 of 219, loss = 0.00031420041466390103\n",
            "Epoch 11 Step 47 of 219, loss = 9.243300667094445e-05\n",
            "Epoch 11 Step 48 of 219, loss = 0.0001974692904695985\n",
            "Epoch 11 Step 49 of 219, loss = 0.019054076736665593\n",
            "Epoch 11 Step 50 of 219, loss = 0.00020607977296549507\n",
            "Epoch 11 Step 51 of 219, loss = 6.801328379424376e-05\n",
            "Epoch 11 Step 52 of 219, loss = 0.37367446972996277\n",
            "Epoch 11 Step 53 of 219, loss = 7.106024099812203e-05\n",
            "Epoch 11 Step 54 of 219, loss = 0.00015334145518863807\n",
            "Epoch 11 Step 55 of 219, loss = 0.00022927770726255403\n",
            "Epoch 11 Step 56 of 219, loss = 0.00021723786301208747\n",
            "Epoch 11 Step 57 of 219, loss = 0.9185588975238943\n",
            "Epoch 11 Step 58 of 219, loss = 0.00013183304429276177\n",
            "Epoch 11 Step 59 of 219, loss = 8.371621498781678e-05\n",
            "Epoch 11 Step 60 of 219, loss = 0.00027758613020978373\n",
            "Epoch 11 Step 61 of 219, loss = 7.362073472449993e-05\n",
            "Epoch 11 Step 62 of 219, loss = 0.001978694146998805\n",
            "Epoch 11 Step 63 of 219, loss = 7.712838760198792e-05\n",
            "Epoch 11 Step 64 of 219, loss = 2.1479761812770448e-05\n",
            "Epoch 11 Step 65 of 219, loss = 0.0006933299144407101\n",
            "Epoch 11 Step 66 of 219, loss = 3.462123623876323e-05\n",
            "Epoch 11 Step 67 of 219, loss = 0.0006177622576046815\n",
            "Epoch 11 Step 68 of 219, loss = 0.32583446654791715\n",
            "Epoch 11 Step 69 of 219, loss = 5.848074550840465e-05\n",
            "Epoch 11 Step 70 of 219, loss = 0.7815198262669014\n",
            "Epoch 11 Step 71 of 219, loss = 0.0008799226460496357\n",
            "Epoch 11 Step 72 of 219, loss = 2.2664293368279687e-05\n",
            "Epoch 11 Step 73 of 219, loss = 0.00011089980131373522\n",
            "Epoch 11 Step 74 of 219, loss = 2.3886235851477977e-05\n",
            "Epoch 11 Step 75 of 219, loss = 0.00014383614598045824\n",
            "Epoch 11 Step 76 of 219, loss = 0.00031341159240128036\n",
            "Epoch 11 Step 77 of 219, loss = 0.0003936061846161465\n",
            "Epoch 11 Step 78 of 219, loss = 0.00018244772167008705\n",
            "Epoch 11 Step 79 of 219, loss = 5.446402633424441e-05\n",
            "Epoch 11 Step 80 of 219, loss = 6.72706085538266e-05\n",
            "Epoch 11 Step 81 of 219, loss = 2.136800168273112e-05\n",
            "Epoch 11 Step 82 of 219, loss = 0.0002488945372647322\n",
            "Epoch 11 Step 83 of 219, loss = 0.0005345282848452371\n",
            "Epoch 11 Step 84 of 219, loss = 2.758152993465046e-05\n",
            "Epoch 11 Step 85 of 219, loss = 0.5257461540920758\n",
            "Epoch 11 Step 86 of 219, loss = 6.270398787933118e-05\n",
            "Epoch 11 Step 87 of 219, loss = 0.0023029611096490044\n",
            "Epoch 11 Step 88 of 219, loss = 8.828519915482502e-05\n",
            "Epoch 11 Step 89 of 219, loss = 0.455765154577648\n",
            "Epoch 11 Step 90 of 219, loss = 4.2437001638973015e-05\n",
            "Epoch 11 Step 91 of 219, loss = 9.551446873956593e-05\n",
            "Epoch 11 Step 92 of 219, loss = 9.41604208719582e-05\n",
            "Epoch 11 Step 93 of 219, loss = 0.00015850624623681142\n",
            "Epoch 11 Step 94 of 219, loss = 0.00014751873118257208\n",
            "Epoch 11 Step 95 of 219, loss = 0.00010488261494856488\n",
            "Epoch 11 Step 96 of 219, loss = 0.0003864188767011001\n",
            "Epoch 11 Step 97 of 219, loss = 6.728868635264007e-05\n",
            "Epoch 11 Step 98 of 219, loss = 0.00013403882462625916\n",
            "Epoch 11 Step 99 of 219, loss = 0.0003336844769137315\n",
            "Epoch 11 Step 100 of 219, loss = 0.00021611283000311232\n",
            "Epoch 11 Step 101 of 219, loss = 0.00019186413487659593\n",
            "Epoch 11 Step 102 of 219, loss = 0.00029287611528161506\n",
            "Epoch 11 Step 103 of 219, loss = 0.00029611006436880416\n",
            "Epoch 11 Step 104 of 219, loss = 0.18640584942863825\n",
            "Epoch 11 Step 105 of 219, loss = 0.22456698089956717\n",
            "Epoch 11 Step 106 of 219, loss = 0.0005995836165766377\n",
            "Epoch 11 Step 107 of 219, loss = 7.929213393254031e-05\n",
            "Epoch 11 Step 108 of 219, loss = 0.44952242146808885\n",
            "Epoch 11 Step 109 of 219, loss = 6.150914191493939e-05\n",
            "Epoch 11 Step 110 of 219, loss = 0.0013536301039493992\n",
            "Epoch 11 Step 111 of 219, loss = 0.0004061530964065696\n",
            "Epoch 11 Step 112 of 219, loss = 0.02599811164429866\n",
            "Epoch 11 Step 113 of 219, loss = 0.08809549599277489\n",
            "Epoch 11 Step 114 of 219, loss = 0.00011223915106484128\n",
            "Epoch 11 Step 115 of 219, loss = 0.00018577323243107458\n",
            "Epoch 11 Step 116 of 219, loss = 0.1527972183828865\n",
            "Epoch 11 Step 117 of 219, loss = 5.2730174303405875e-05\n",
            "Epoch 11 Step 118 of 219, loss = 2.1382839463512937e-05\n",
            "Epoch 11 Step 119 of 219, loss = 2.3379549588753434e-05\n",
            "Epoch 11 Step 120 of 219, loss = 0.00010575945725577185\n",
            "Epoch 11 Step 121 of 219, loss = 2.1449923082172972e-05\n",
            "Epoch 11 Step 122 of 219, loss = 0.32109083538210825\n",
            "Epoch 11 Step 123 of 219, loss = 1.8827409576260834e-05\n",
            "Epoch 11 Step 124 of 219, loss = 0.00041483464869429554\n",
            "Epoch 11 Step 125 of 219, loss = 3.68042225602494e-05\n",
            "Epoch 11 Step 126 of 219, loss = 0.003393982258160122\n",
            "Epoch 11 Step 127 of 219, loss = 1.6972254286429234e-05\n",
            "Epoch 11 Step 128 of 219, loss = 6.196845029649012e-05\n",
            "Epoch 11 Step 129 of 219, loss = 0.0005500946025449593\n",
            "Epoch 11 Step 130 of 219, loss = 8.886433317911724e-05\n",
            "Epoch 11 Step 131 of 219, loss = 1.9982227627224347e-05\n",
            "Epoch 11 Step 132 of 219, loss = 3.7564780313914525e-05\n",
            "Epoch 11 Step 133 of 219, loss = 0.0017456083941169709\n",
            "Epoch 11 Step 134 of 219, loss = 0.00038167754047435665\n",
            "Epoch 11 Step 135 of 219, loss = 0.0009759990533098062\n",
            "Epoch 11 Step 136 of 219, loss = 6.488382047109553e-05\n",
            "Epoch 11 Step 137 of 219, loss = 0.5934263860355031\n",
            "Epoch 11 Step 138 of 219, loss = 1.2734606951220258\n",
            "Epoch 11 Step 139 of 219, loss = 5.6257092751366145e-05\n",
            "Epoch 11 Step 140 of 219, loss = 0.4826216524940037\n",
            "Epoch 11 Step 141 of 219, loss = 0.00017676179368208977\n",
            "Epoch 11 Step 142 of 219, loss = 0.06990697806418211\n",
            "Epoch 11 Step 143 of 219, loss = 0.0001716260993589458\n",
            "Epoch 11 Step 144 of 219, loss = 0.00020342676015161487\n",
            "Epoch 11 Step 145 of 219, loss = 0.0001413092431903351\n",
            "Epoch 11 Step 146 of 219, loss = 0.00032696890934857947\n",
            "Epoch 11 Step 147 of 219, loss = 0.0005248657158745118\n",
            "Epoch 11 Step 148 of 219, loss = 0.006602130671353734\n",
            "Epoch 11 Step 149 of 219, loss = 0.002024318191161001\n",
            "Epoch 11 Step 150 of 219, loss = 0.0006440853558160597\n",
            "Epoch 11 Step 151 of 219, loss = 0.00012895619431674277\n",
            "Epoch 11 Step 152 of 219, loss = 0.0003273166339567979\n",
            "Epoch 11 Step 153 of 219, loss = 0.0004146722075120124\n",
            "Epoch 11 Step 154 of 219, loss = 9.667936762980389e-05\n",
            "Epoch 11 Step 155 of 219, loss = 0.0005381472778935859\n",
            "Epoch 11 Step 156 of 219, loss = 0.00014498848747734883\n",
            "Epoch 11 Step 157 of 219, loss = 0.00010915969176039653\n",
            "Epoch 11 Step 158 of 219, loss = 0.0013685878759588377\n",
            "Epoch 11 Step 159 of 219, loss = 0.20074910442826877\n",
            "Epoch 11 Step 160 of 219, loss = 0.02862811767238327\n",
            "Epoch 11 Step 161 of 219, loss = 0.000401724192556685\n",
            "Epoch 11 Step 162 of 219, loss = 0.006845190344051844\n",
            "Epoch 11 Step 163 of 219, loss = 0.0013898960406777405\n",
            "Epoch 11 Step 164 of 219, loss = 0.00021135762125368274\n",
            "Epoch 11 Step 165 of 219, loss = 6.815467185106172e-05\n",
            "Epoch 11 Step 166 of 219, loss = 0.0002888680683099665\n",
            "Epoch 11 Step 167 of 219, loss = 0.0010276127492261367\n",
            "Epoch 11 Step 168 of 219, loss = 0.15134673381658104\n",
            "Epoch 11 Step 169 of 219, loss = 0.00011853583237098064\n",
            "Epoch 11 Step 170 of 219, loss = 0.00011334020183539906\n",
            "Epoch 11 Step 171 of 219, loss = 0.09690757628823121\n",
            "Epoch 11 Step 172 of 219, loss = 9.59424694997324e-05\n",
            "Epoch 11 Step 173 of 219, loss = 0.0002020791988570636\n",
            "Epoch 11 Step 174 of 219, loss = 0.00022515447687965207\n",
            "Epoch 11 Step 175 of 219, loss = 0.00011404784697788273\n",
            "Epoch 11 Step 176 of 219, loss = 0.028612355740506246\n",
            "Epoch 11 Step 177 of 219, loss = 0.000496454993253792\n",
            "Epoch 11 Step 178 of 219, loss = 0.018240838620727118\n",
            "Epoch 11 Step 179 of 219, loss = 0.00015042326822367613\n",
            "Epoch 11 Step 180 of 219, loss = 0.00016691666337464994\n",
            "Epoch 11 Step 181 of 219, loss = 0.000814041623243611\n",
            "Epoch 11 Step 182 of 219, loss = 6.429505333471752e-05\n",
            "Epoch 11 Step 183 of 219, loss = 0.000170953380006722\n",
            "Epoch 11 Step 184 of 219, loss = 0.0018230457748131812\n",
            "Epoch 11 Step 185 of 219, loss = 0.00016645589676045347\n",
            "Epoch 11 Step 186 of 219, loss = 0.0002456432205804049\n",
            "Epoch 11 Step 187 of 219, loss = 0.00027058209741426253\n",
            "Epoch 11 Step 188 of 219, loss = 0.549101019832051\n",
            "Epoch 11 Step 189 of 219, loss = 0.0012140138353515795\n",
            "Epoch 11 Step 190 of 219, loss = 0.00024383488386092722\n",
            "Epoch 11 Step 191 of 219, loss = 0.006192823524088453\n",
            "Epoch 11 Step 192 of 219, loss = 0.0012239465795573778\n",
            "Epoch 11 Step 193 of 219, loss = 1.2327280324254275\n",
            "Epoch 11 Step 194 of 219, loss = 0.00016679853717960214\n",
            "Epoch 11 Step 195 of 219, loss = 0.0014475259499704407\n",
            "Epoch 11 Step 196 of 219, loss = 0.00024895189676499285\n",
            "Epoch 11 Step 197 of 219, loss = 0.0005679522960235772\n",
            "Epoch 11 Step 198 of 219, loss = 0.00027733337469726393\n",
            "Epoch 11 Step 199 of 219, loss = 0.00011209056765437708\n",
            "Epoch 11 Step 200 of 219, loss = 0.00015231200745802198\n",
            "Epoch 11 Step 201 of 219, loss = 0.0003049827819268103\n",
            "Epoch 11 Step 202 of 219, loss = 0.00016115863490995253\n",
            "Epoch 11 Step 203 of 219, loss = 0.00023922540322018904\n",
            "Epoch 11 Step 204 of 219, loss = 0.00034116049232579826\n",
            "Epoch 11 Step 205 of 219, loss = 0.00012343062167019525\n",
            "Epoch 11 Step 206 of 219, loss = 0.0001245399878371245\n",
            "Epoch 11 Step 207 of 219, loss = 0.00037406618048407836\n",
            "Epoch 11 Step 208 of 219, loss = 0.00015665190062463807\n",
            "Epoch 11 Step 209 of 219, loss = 0.00022633655225945404\n",
            "Epoch 11 Step 210 of 219, loss = 0.6853659573739606\n",
            "Epoch 11 Step 211 of 219, loss = 0.0002501173510154331\n",
            "Epoch 11 Step 212 of 219, loss = 0.0004961225085935439\n",
            "Epoch 11 Step 213 of 219, loss = 0.0004655805587390205\n",
            "Epoch 11 Step 214 of 219, loss = 0.00026906159212103375\n",
            "Epoch 11 Step 215 of 219, loss = 0.0002288110852077807\n",
            "Epoch 11 Step 216 of 219, loss = 0.0004359852632660477\n",
            "Epoch 11 Step 217 of 219, loss = 0.0003742072119621298\n",
            "Epoch 11 Step 218 of 219, loss = 0.00013243054430252715\n",
            "Epoch 11 average train_loss: 0.078626 test_loss: 2.664432 test_score 0.59%\n",
            "Epoch 12 Step 0 of 219, loss = 0.0009305301887252426\n",
            "Epoch 12 Step 1 of 219, loss = 7.477011934042821e-05\n",
            "Epoch 12 Step 2 of 219, loss = 0.00010157199938021222\n",
            "Epoch 12 Step 3 of 219, loss = 0.00035680263908943743\n",
            "Epoch 12 Step 4 of 219, loss = 5.00814390989035e-05\n",
            "Epoch 12 Step 5 of 219, loss = 4.2504480688876356e-05\n",
            "Epoch 12 Step 6 of 219, loss = 4.611794452102913e-05\n",
            "Epoch 12 Step 7 of 219, loss = 0.00014344231090035464\n",
            "Epoch 12 Step 8 of 219, loss = 5.588434487435734e-05\n",
            "Epoch 12 Step 9 of 219, loss = 9.378323090913909e-05\n",
            "Epoch 12 Step 10 of 219, loss = 0.642567766317029\n",
            "Epoch 12 Step 11 of 219, loss = 4.070135616984771e-05\n",
            "Epoch 12 Step 12 of 219, loss = 6.545300908555873e-05\n",
            "Epoch 12 Step 13 of 219, loss = 0.00010514706877984281\n",
            "Epoch 12 Step 14 of 219, loss = 5.008728840039112e-05\n",
            "Epoch 12 Step 15 of 219, loss = 2.6403905110328196e-05\n",
            "Epoch 12 Step 16 of 219, loss = 0.00016221632915858208\n",
            "Epoch 12 Step 17 of 219, loss = 5.67333296430661e-05\n",
            "Epoch 12 Step 18 of 219, loss = 1.6738968396696805\n",
            "Epoch 12 Step 19 of 219, loss = 0.0016056526797001425\n",
            "Epoch 12 Step 20 of 219, loss = 2.9861339498893358e-05\n",
            "Epoch 12 Step 21 of 219, loss = 0.13144070137138897\n",
            "Epoch 12 Step 22 of 219, loss = 0.00032424916128093173\n",
            "Epoch 12 Step 23 of 219, loss = 0.0003022232017428905\n",
            "Epoch 12 Step 24 of 219, loss = 9.324144406264168e-05\n",
            "Epoch 12 Step 25 of 219, loss = 7.456773306557807e-05\n",
            "Epoch 12 Step 26 of 219, loss = 2.9138413083273917e-05\n",
            "Epoch 12 Step 27 of 219, loss = 0.00011691121986245889\n",
            "Epoch 12 Step 28 of 219, loss = 1.9706319676515704e-05\n",
            "Epoch 12 Step 29 of 219, loss = 0.00020537319755931094\n",
            "Epoch 12 Step 30 of 219, loss = 3.4144415565151576e-05\n",
            "Epoch 12 Step 31 of 219, loss = 6.772842212399155e-05\n",
            "Epoch 12 Step 32 of 219, loss = 3.2914652649651543e-05\n",
            "Epoch 12 Step 33 of 219, loss = 0.004352552138243482\n",
            "Epoch 12 Step 34 of 219, loss = 0.008903398845404809\n",
            "Epoch 12 Step 35 of 219, loss = 0.0001758148278270255\n",
            "Epoch 12 Step 36 of 219, loss = 3.3964242305728476e-05\n",
            "Epoch 12 Step 37 of 219, loss = 0.001022858590118858\n",
            "Epoch 12 Step 38 of 219, loss = 9.655902005079042e-06\n",
            "Epoch 12 Step 39 of 219, loss = 0.00010159651037611184\n",
            "Epoch 12 Step 40 of 219, loss = 1.2360428797819623e-05\n",
            "Epoch 12 Step 41 of 219, loss = 4.137514156354882e-05\n",
            "Epoch 12 Step 42 of 219, loss = 4.924173427411915e-05\n",
            "Epoch 12 Step 43 of 219, loss = 8.247752049328483e-06\n",
            "Epoch 12 Step 44 of 219, loss = 1.09671760526453e-05\n",
            "Epoch 12 Step 45 of 219, loss = 4.348578877966247e-05\n",
            "Epoch 12 Step 46 of 219, loss = 2.2023394848247335e-05\n",
            "Epoch 12 Step 47 of 219, loss = 0.00010698497129624229\n",
            "Epoch 12 Step 48 of 219, loss = 1.2248663551872596e-05\n",
            "Epoch 12 Step 49 of 219, loss = 0.37646034808312834\n",
            "Epoch 12 Step 50 of 219, loss = 1.5586450047067046e-05\n",
            "Epoch 12 Step 51 of 219, loss = 1.6219759856994642e-05\n",
            "Epoch 12 Step 52 of 219, loss = 2.368500048532951e-05\n",
            "Epoch 12 Step 53 of 219, loss = 4.458171832766311e-05\n",
            "Epoch 12 Step 54 of 219, loss = 8.950172937716161e-05\n",
            "Epoch 12 Step 55 of 219, loss = 2.8512725123164273e-05\n",
            "Epoch 12 Step 56 of 219, loss = 0.0006287088921510531\n",
            "Epoch 12 Step 57 of 219, loss = 0.0009524312420552405\n",
            "Epoch 12 Step 58 of 219, loss = 0.4124009532984587\n",
            "Epoch 12 Step 59 of 219, loss = 4.369550327965044e-05\n",
            "Epoch 12 Step 60 of 219, loss = 4.1824869327911074e-05\n",
            "Epoch 12 Step 61 of 219, loss = 3.604461264217207e-05\n",
            "Epoch 12 Step 62 of 219, loss = 0.00019177876839648889\n",
            "Epoch 12 Step 63 of 219, loss = 4.439993344362847e-05\n",
            "Epoch 12 Step 64 of 219, loss = 2.152418150558333e-05\n",
            "Epoch 12 Step 65 of 219, loss = 1.6607155856718236e-05\n",
            "Epoch 12 Step 66 of 219, loss = 0.00011805087839888984\n",
            "Epoch 12 Step 67 of 219, loss = 9.911019662922627e-05\n",
            "Epoch 12 Step 68 of 219, loss = 6.587995186180251e-05\n",
            "Epoch 12 Step 69 of 219, loss = 2.5547524586500003e-05\n",
            "Epoch 12 Step 70 of 219, loss = 1.1995339463055643e-05\n",
            "Epoch 12 Step 71 of 219, loss = 0.0002610101019229205\n",
            "Epoch 12 Step 72 of 219, loss = 1.0482890559160296e-05\n",
            "Epoch 12 Step 73 of 219, loss = 2.4272689472581988e-05\n",
            "Epoch 12 Step 74 of 219, loss = 9.797454154636398e-06\n",
            "Epoch 12 Step 75 of 219, loss = 1.6100437946420243e-05\n",
            "Epoch 12 Step 76 of 219, loss = 0.00021409403566963192\n",
            "Epoch 12 Step 77 of 219, loss = 0.0001486348000412363\n",
            "Epoch 12 Step 78 of 219, loss = 4.987386100197e-05\n",
            "Epoch 12 Step 79 of 219, loss = 1.2680770282713638e-05\n",
            "Epoch 12 Step 80 of 219, loss = 0.00020696076046533562\n",
            "Epoch 12 Step 81 of 219, loss = 6.884310494115198e-06\n",
            "Epoch 12 Step 82 of 219, loss = 1.1265163266216405e-05\n",
            "Epoch 12 Step 83 of 219, loss = 7.492205867265511e-05\n",
            "Epoch 12 Step 84 of 219, loss = 1.191337240413759e-05\n",
            "Epoch 12 Step 85 of 219, loss = 0.0035906597575490196\n",
            "Epoch 12 Step 86 of 219, loss = 2.1039740431660903e-05\n",
            "Epoch 12 Step 87 of 219, loss = 0.00020874917959190498\n",
            "Epoch 12 Step 88 of 219, loss = 3.101556779938619e-05\n",
            "Epoch 12 Step 89 of 219, loss = 0.5358452911573579\n",
            "Epoch 12 Step 90 of 219, loss = 1.3500300582336422e-05\n",
            "Epoch 12 Step 91 of 219, loss = 2.1636154173165778e-05\n",
            "Epoch 12 Step 92 of 219, loss = 3.570828565102602e-05\n",
            "Epoch 12 Step 93 of 219, loss = 0.0026611153130318144\n",
            "Epoch 12 Step 94 of 219, loss = 5.852464585132111e-05\n",
            "Epoch 12 Step 95 of 219, loss = 1.0222126235248652e-05\n",
            "Epoch 12 Step 96 of 219, loss = 0.000187104675859473\n",
            "Epoch 12 Step 97 of 219, loss = 7.770919594918269e-06\n",
            "Epoch 12 Step 98 of 219, loss = 0.7064699909110743\n",
            "Epoch 12 Step 99 of 219, loss = 0.00018019154362036716\n",
            "Epoch 12 Step 100 of 219, loss = 8.173271629630108e-05\n",
            "Epoch 12 Step 101 of 219, loss = 6.617028623168153e-05\n",
            "Epoch 12 Step 102 of 219, loss = 1.0031666854976322\n",
            "Epoch 12 Step 103 of 219, loss = 3.363026371516753e-05\n",
            "Epoch 12 Step 104 of 219, loss = 0.00041381388545858044\n",
            "Epoch 12 Step 105 of 219, loss = 1.443158129177391e-05\n",
            "Epoch 12 Step 106 of 219, loss = 2.0965674281114843e-05\n",
            "Epoch 12 Step 107 of 219, loss = 3.3705395424021845e-05\n",
            "Epoch 12 Step 108 of 219, loss = 9.163609416873442e-05\n",
            "Epoch 12 Step 109 of 219, loss = 3.0889216731111446e-05\n",
            "Epoch 12 Step 110 of 219, loss = 0.0013000335347896907\n",
            "Epoch 12 Step 111 of 219, loss = 0.24447234608157942\n",
            "Epoch 12 Step 112 of 219, loss = 9.48806642782074e-05\n",
            "Epoch 12 Step 113 of 219, loss = 6.787861366319703e-05\n",
            "Epoch 12 Step 114 of 219, loss = 0.17862390922010718\n",
            "Epoch 12 Step 115 of 219, loss = 3.155260282028394e-05\n",
            "Epoch 12 Step 116 of 219, loss = 9.929351955406673e-05\n",
            "Epoch 12 Step 117 of 219, loss = 3.3205992565399356e-05\n",
            "Epoch 12 Step 118 of 219, loss = 1.1384403364900209e-05\n",
            "Epoch 12 Step 119 of 219, loss = 1.5534289303786863e-05\n",
            "Epoch 12 Step 120 of 219, loss = 2.3774210035298893e-05\n",
            "Epoch 12 Step 121 of 219, loss = 1.2650984928086473e-05\n",
            "Epoch 12 Step 122 of 219, loss = 2.632238809496812e-05\n",
            "Epoch 12 Step 123 of 219, loss = 1.1086378506774963e-05\n",
            "Epoch 12 Step 124 of 219, loss = 2.502584143826425e-05\n",
            "Epoch 12 Step 125 of 219, loss = 1.8834777108622802e-05\n",
            "Epoch 12 Step 126 of 219, loss = 5.324565557884853e-05\n",
            "Epoch 12 Step 127 of 219, loss = 1.0266827430882586e-05\n",
            "Epoch 12 Step 128 of 219, loss = 1.4751969246162844e-05\n",
            "Epoch 12 Step 129 of 219, loss = 6.382742000710095e-05\n",
            "Epoch 12 Step 130 of 219, loss = 2.7886760932460675e-05\n",
            "Epoch 12 Step 131 of 219, loss = 1.0959726552073334e-05\n",
            "Epoch 12 Step 132 of 219, loss = 1.7948177088555894e-05\n",
            "Epoch 12 Step 133 of 219, loss = 0.00013554016385342038\n",
            "Epoch 12 Step 134 of 219, loss = 0.00016265900035250525\n",
            "Epoch 12 Step 135 of 219, loss = 0.00039349389581389005\n",
            "Epoch 12 Step 136 of 219, loss = 5.037065915303174e-05\n",
            "Epoch 12 Step 137 of 219, loss = 0.7390018601699921\n",
            "Epoch 12 Step 138 of 219, loss = 0.2824681223422374\n",
            "Epoch 12 Step 139 of 219, loss = 2.419899348637955e-05\n",
            "Epoch 12 Step 140 of 219, loss = 0.06844593772274266\n",
            "Epoch 12 Step 141 of 219, loss = 6.420537027906903e-05\n",
            "Epoch 12 Step 142 of 219, loss = 0.00014529361061477175\n",
            "Epoch 12 Step 143 of 219, loss = 0.00010863152093065764\n",
            "Epoch 12 Step 144 of 219, loss = 0.00162992435821252\n",
            "Epoch 12 Step 145 of 219, loss = 7.003707165154083e-05\n",
            "Epoch 12 Step 146 of 219, loss = 9.130914571642279e-05\n",
            "Epoch 12 Step 147 of 219, loss = 0.0002562295600228026\n",
            "Epoch 12 Step 148 of 219, loss = 0.00033144484805802676\n",
            "Epoch 12 Step 149 of 219, loss = 0.00011745656763650913\n",
            "Epoch 12 Step 150 of 219, loss = 0.0015771988452115693\n",
            "Epoch 12 Step 151 of 219, loss = 9.13805254754152e-05\n",
            "Epoch 12 Step 152 of 219, loss = 0.0001360497927862525\n",
            "Epoch 12 Step 153 of 219, loss = 9.940446895484456e-05\n",
            "Epoch 12 Step 154 of 219, loss = 7.198247385531431e-05\n",
            "Epoch 12 Step 155 of 219, loss = 0.00019748211536807503\n",
            "Epoch 12 Step 156 of 219, loss = 6.033218988932276e-05\n",
            "Epoch 12 Step 157 of 219, loss = 5.637669198677031e-05\n",
            "Epoch 12 Step 158 of 219, loss = 0.0001257201995485957\n",
            "Epoch 12 Step 159 of 219, loss = 0.006276011351076249\n",
            "Epoch 12 Step 160 of 219, loss = 0.00019445713132881792\n",
            "Epoch 12 Step 161 of 219, loss = 7.13806585963539e-05\n",
            "Epoch 12 Step 162 of 219, loss = 0.0001015791050349435\n",
            "Epoch 12 Step 163 of 219, loss = 0.000542000430243661\n",
            "Epoch 12 Step 164 of 219, loss = 6.348282232693236e-05\n",
            "Epoch 12 Step 165 of 219, loss = 4.8173758955272206e-05\n",
            "Epoch 12 Step 166 of 219, loss = 8.988547267563263e-05\n",
            "Epoch 12 Step 167 of 219, loss = 7.35838145260459e-05\n",
            "Epoch 12 Step 168 of 219, loss = 6.357949752100467e-05\n",
            "Epoch 12 Step 169 of 219, loss = 8.165822470118655e-05\n",
            "Epoch 12 Step 170 of 219, loss = 5.7978168626959814e-05\n",
            "Epoch 12 Step 171 of 219, loss = 0.00021545014374169114\n",
            "Epoch 12 Step 172 of 219, loss = 8.584785652487881e-05\n",
            "Epoch 12 Step 173 of 219, loss = 0.00017874173786935899\n",
            "Epoch 12 Step 174 of 219, loss = 5.151155619387282e-05\n",
            "Epoch 12 Step 175 of 219, loss = 5.537692393886573e-05\n",
            "Epoch 12 Step 176 of 219, loss = 0.004968178359149533\n",
            "Epoch 12 Step 177 of 219, loss = 0.0005770207501996083\n",
            "Epoch 12 Step 178 of 219, loss = 8.688282787261414e-05\n",
            "Epoch 12 Step 179 of 219, loss = 3.590334466707645e-05\n",
            "Epoch 12 Step 180 of 219, loss = 7.85897490231946e-05\n",
            "Epoch 12 Step 181 of 219, loss = 0.00026187109278907883\n",
            "Epoch 12 Step 182 of 219, loss = 3.283397018094547e-05\n",
            "Epoch 12 Step 183 of 219, loss = 4.3845430070632574e-05\n",
            "Epoch 12 Step 184 of 219, loss = 8.458776540010149e-05\n",
            "Epoch 12 Step 185 of 219, loss = 7.583602450722537e-05\n",
            "Epoch 12 Step 186 of 219, loss = 0.0001264307417159216\n",
            "Epoch 12 Step 187 of 219, loss = 8.681606124127939e-05\n",
            "Epoch 12 Step 188 of 219, loss = 0.00011626804626985177\n",
            "Epoch 12 Step 189 of 219, loss = 0.021740345365174107\n",
            "Epoch 12 Step 190 of 219, loss = 9.569838036327383e-05\n",
            "Epoch 12 Step 191 of 219, loss = 6.005596014801995e-05\n",
            "Epoch 12 Step 192 of 219, loss = 0.00020745140579947474\n",
            "Epoch 12 Step 193 of 219, loss = 1.1213433822133538\n",
            "Epoch 12 Step 194 of 219, loss = 0.002263241189666587\n",
            "Epoch 12 Step 195 of 219, loss = 0.00028296104051150905\n",
            "Epoch 12 Step 196 of 219, loss = 6.84146457388124e-05\n",
            "Epoch 12 Step 197 of 219, loss = 0.00015930445624690037\n",
            "Epoch 12 Step 198 of 219, loss = 3.9494458803801535e-05\n",
            "Epoch 12 Step 199 of 219, loss = 4.380821025051773e-05\n",
            "Epoch 12 Step 200 of 219, loss = 0.011766094637664537\n",
            "Epoch 12 Step 201 of 219, loss = 9.788541160560271e-05\n",
            "Epoch 12 Step 202 of 219, loss = 8.793023334874306e-05\n",
            "Epoch 12 Step 203 of 219, loss = 0.15655134412025973\n",
            "Epoch 12 Step 204 of 219, loss = 0.0004409999839936063\n",
            "Epoch 12 Step 205 of 219, loss = 4.3904855033360946e-05\n",
            "Epoch 12 Step 206 of 219, loss = 4.737630229101342e-05\n",
            "Epoch 12 Step 207 of 219, loss = 6.880273321030472e-05\n",
            "Epoch 12 Step 208 of 219, loss = 3.566516397768282e-05\n",
            "Epoch 12 Step 209 of 219, loss = 5.888606773396532e-05\n",
            "Epoch 12 Step 210 of 219, loss = 0.00011873355708758027\n",
            "Epoch 12 Step 211 of 219, loss = 7.368103786120628e-05\n",
            "Epoch 12 Step 212 of 219, loss = 0.00012575061055031256\n",
            "Epoch 12 Step 213 of 219, loss = 0.4174586761075716\n",
            "Epoch 12 Step 214 of 219, loss = 7.223656143651169e-05\n",
            "Epoch 12 Step 215 of 219, loss = 0.00024288175643505383\n",
            "Epoch 12 Step 216 of 219, loss = 0.00021369684924366084\n",
            "Epoch 12 Step 217 of 219, loss = 0.00010817247550676257\n",
            "Epoch 12 Step 218 of 219, loss = 0.035169892722478835\n",
            "Epoch 12 average train_loss: 0.040268 test_loss: 2.838896 test_score 0.59%\n",
            "Epoch 13 Step 0 of 219, loss = 0.00011376407110219589\n",
            "Epoch 13 Step 1 of 219, loss = 0.020600682886936283\n",
            "Epoch 13 Step 2 of 219, loss = 4.49400515663001e-05\n",
            "Epoch 13 Step 3 of 219, loss = 0.00016098111962037365\n",
            "Epoch 13 Step 4 of 219, loss = 2.5741330546225072e-05\n",
            "Epoch 13 Step 5 of 219, loss = 2.520481569945332e-05\n",
            "Epoch 13 Step 6 of 219, loss = 2.4906863814067037e-05\n",
            "Epoch 13 Step 7 of 219, loss = 2.9861243945106253e-05\n",
            "Epoch 13 Step 8 of 219, loss = 2.113699628125687e-05\n",
            "Epoch 13 Step 9 of 219, loss = 2.7529199257969594e-05\n",
            "Epoch 13 Step 10 of 219, loss = 0.09632909521525335\n",
            "Epoch 13 Step 11 of 219, loss = 3.868213693181133e-05\n",
            "Epoch 13 Step 12 of 219, loss = 0.0004340598337364554\n",
            "Epoch 13 Step 13 of 219, loss = 0.00017790878132473154\n",
            "Epoch 13 Step 14 of 219, loss = 5.3431472906595445e-05\n",
            "Epoch 13 Step 15 of 219, loss = 1.3917556486831018e-05\n",
            "Epoch 13 Step 16 of 219, loss = 3.737082195698349e-05\n",
            "Epoch 13 Step 17 of 219, loss = 5.737220885748684e-05\n",
            "Epoch 13 Step 18 of 219, loss = 0.6085451963826358\n",
            "Epoch 13 Step 19 of 219, loss = 7.373886813866193e-05\n",
            "Epoch 13 Step 20 of 219, loss = 4.055151467241558e-05\n",
            "Epoch 13 Step 21 of 219, loss = 0.0001147102013874246\n",
            "Epoch 13 Step 22 of 219, loss = 7.236088447371003e-05\n",
            "Epoch 13 Step 23 of 219, loss = 1.2047831636295427\n",
            "Epoch 13 Step 24 of 219, loss = 6.470993093898869e-05\n",
            "Epoch 13 Step 25 of 219, loss = 0.0019862163666175547\n",
            "Epoch 13 Step 26 of 219, loss = 0.00032795449487821315\n",
            "Epoch 13 Step 27 of 219, loss = 5.8596271230726416e-05\n",
            "Epoch 13 Step 28 of 219, loss = 0.0002878299665951545\n",
            "Epoch 13 Step 29 of 219, loss = 7.08204864849904e-05\n",
            "Epoch 13 Step 30 of 219, loss = 0.0002194497841969678\n",
            "Epoch 13 Step 31 of 219, loss = 0.00015195999299066898\n",
            "Epoch 13 Step 32 of 219, loss = 4.5521723905039835e-05\n",
            "Epoch 13 Step 33 of 219, loss = 9.067397320450254e-05\n",
            "Epoch 13 Step 34 of 219, loss = 6.311796016689186e-05\n",
            "Epoch 13 Step 35 of 219, loss = 0.0004879375914015327\n",
            "Epoch 13 Step 36 of 219, loss = 3.7706566104134254e-05\n",
            "Epoch 13 Step 37 of 219, loss = 4.8151355258596595e-05\n",
            "Epoch 13 Step 38 of 219, loss = 4.5819834042504226e-05\n",
            "Epoch 13 Step 39 of 219, loss = 0.00012564755786570458\n",
            "Epoch 13 Step 40 of 219, loss = 8.283096906325227e-05\n",
            "Epoch 13 Step 41 of 219, loss = 0.0010933105232879825\n",
            "Epoch 13 Step 42 of 219, loss = 6.30588233434537e-05\n",
            "Epoch 13 Step 43 of 219, loss = 2.512300136459089e-05\n",
            "Epoch 13 Step 44 of 219, loss = 8.137753388837154e-05\n",
            "Epoch 13 Step 45 of 219, loss = 5.9653798302861105e-05\n",
            "Epoch 13 Step 46 of 219, loss = 5.068484097137116e-05\n",
            "Epoch 13 Step 47 of 219, loss = 0.0002315411488780228\n",
            "Epoch 13 Step 48 of 219, loss = 3.8518639826179424e-05\n",
            "Epoch 13 Step 49 of 219, loss = 0.0012188162315851514\n",
            "Epoch 13 Step 50 of 219, loss = 7.349556165081594e-05\n",
            "Epoch 13 Step 51 of 219, loss = 6.114448274274764e-05\n",
            "Epoch 13 Step 52 of 219, loss = 7.435324482685246e-05\n",
            "Epoch 13 Step 53 of 219, loss = 0.00014651333617621276\n",
            "Epoch 13 Step 54 of 219, loss = 0.00017492694200882397\n",
            "Epoch 13 Step 55 of 219, loss = 0.00012166458975571004\n",
            "Epoch 13 Step 56 of 219, loss = 0.0001553458003513697\n",
            "Epoch 13 Step 57 of 219, loss = 0.00014146443265872222\n",
            "Epoch 13 Step 58 of 219, loss = 0.00012598925559359486\n",
            "Epoch 13 Step 59 of 219, loss = 0.015641955932096607\n",
            "Epoch 13 Step 60 of 219, loss = 0.000158567077448879\n",
            "Epoch 13 Step 61 of 219, loss = 9.402557373050513e-05\n",
            "Epoch 13 Step 62 of 219, loss = 4.989499029761646e-05\n",
            "Epoch 13 Step 63 of 219, loss = 0.00015699407765623619\n",
            "Epoch 13 Step 64 of 219, loss = 0.00019821582111489988\n",
            "Epoch 13 Step 65 of 219, loss = 0.00012188120535938651\n",
            "Epoch 13 Step 66 of 219, loss = 0.00014370139660968562\n",
            "Epoch 13 Step 67 of 219, loss = 5.0095343453904206e-05\n",
            "Epoch 13 Step 68 of 219, loss = 9.923899455088758e-05\n",
            "Epoch 13 Step 69 of 219, loss = 8.755758653933299e-05\n",
            "Epoch 13 Step 70 of 219, loss = 1.981830399699902e-05\n",
            "Epoch 13 Step 71 of 219, loss = 4.876084688021365e-05\n",
            "Epoch 13 Step 72 of 219, loss = 2.6262704835744444e-05\n",
            "Epoch 13 Step 73 of 219, loss = 0.00014879793297950528\n",
            "Epoch 13 Step 74 of 219, loss = 0.00013823136822566084\n",
            "Epoch 13 Step 75 of 219, loss = 0.00023749543657913819\n",
            "Epoch 13 Step 76 of 219, loss = 0.5098327214293477\n",
            "Epoch 13 Step 77 of 219, loss = 3.5195546729482885e-05\n",
            "Epoch 13 Step 78 of 219, loss = 2.7238746270086267e-05\n",
            "Epoch 13 Step 79 of 219, loss = 2.0421703482043085e-05\n",
            "Epoch 13 Step 80 of 219, loss = 4.4894532720718416e-05\n",
            "Epoch 13 Step 81 of 219, loss = 7.107871380185316e-05\n",
            "Epoch 13 Step 82 of 219, loss = 1.742672600357764e-05\n",
            "Epoch 13 Step 83 of 219, loss = 4.4903009950303385e-05\n",
            "Epoch 13 Step 84 of 219, loss = 0.00013684841772487744\n",
            "Epoch 13 Step 85 of 219, loss = 7.096405477113876e-05\n",
            "Epoch 13 Step 86 of 219, loss = 9.044476541930635e-05\n",
            "Epoch 13 Step 87 of 219, loss = 6.94940355288054e-05\n",
            "Epoch 13 Step 88 of 219, loss = 2.4862182726792526e-05\n",
            "Epoch 13 Step 89 of 219, loss = 0.000166323955511416\n",
            "Epoch 13 Step 90 of 219, loss = 3.943432363939792e-05\n",
            "Epoch 13 Step 91 of 219, loss = 0.000251746378040707\n",
            "Epoch 13 Step 92 of 219, loss = 2.0101334371247503e-05\n",
            "Epoch 13 Step 93 of 219, loss = 0.6967896469639356\n",
            "Epoch 13 Step 94 of 219, loss = 2.124136341308258e-05\n",
            "Epoch 13 Step 95 of 219, loss = 3.5090227925138606e-05\n",
            "Epoch 13 Step 96 of 219, loss = 7.539647106113989e-05\n",
            "Epoch 13 Step 97 of 219, loss = 1.4595535049011232e-05\n",
            "Epoch 13 Step 98 of 219, loss = 5.988229310105453e-05\n",
            "Epoch 13 Step 99 of 219, loss = 0.00014187037777446676\n",
            "Epoch 13 Step 100 of 219, loss = 5.191317370645265e-05\n",
            "Epoch 13 Step 101 of 219, loss = 5.796924904188927e-05\n",
            "Epoch 13 Step 102 of 219, loss = 0.00011688215010963177\n",
            "Epoch 13 Step 103 of 219, loss = 3.610449456914466e-05\n",
            "Epoch 13 Step 104 of 219, loss = 3.1231778109486186e-05\n",
            "Epoch 13 Step 105 of 219, loss = 1.5713132654582296e-05\n",
            "Epoch 13 Step 106 of 219, loss = 1.986302254408656e-05\n",
            "Epoch 13 Step 107 of 219, loss = 2.8356224845538236e-05\n",
            "Epoch 13 Step 108 of 219, loss = 3.592536930341339e-05\n",
            "Epoch 13 Step 109 of 219, loss = 3.031489734439674e-05\n",
            "Epoch 13 Step 110 of 219, loss = 1.001685921336275\n",
            "Epoch 13 Step 111 of 219, loss = 9.298109836208823e-05\n",
            "Epoch 13 Step 112 of 219, loss = 3.437608040712803e-05\n",
            "Epoch 13 Step 113 of 219, loss = 5.40296035751453e-05\n",
            "Epoch 13 Step 114 of 219, loss = 4.4806087601045874e-05\n",
            "Epoch 13 Step 115 of 219, loss = 8.389225467908545e-05\n",
            "Epoch 13 Step 116 of 219, loss = 6.522584988033486e-05\n",
            "Epoch 13 Step 117 of 219, loss = 4.1788941132381296e-05\n",
            "Epoch 13 Step 118 of 219, loss = 0.00011809780016847071\n",
            "Epoch 13 Step 119 of 219, loss = 0.00019195675082528396\n",
            "Epoch 13 Step 120 of 219, loss = 0.00027230080883100527\n",
            "Epoch 13 Step 121 of 219, loss = 0.000141063679564013\n",
            "Epoch 13 Step 122 of 219, loss = 0.00014315850694401888\n",
            "Epoch 13 Step 123 of 219, loss = 6.117307407293993e-05\n",
            "Epoch 13 Step 124 of 219, loss = 0.00010108127094099473\n",
            "Epoch 13 Step 125 of 219, loss = 0.0003624520791163377\n",
            "Epoch 13 Step 126 of 219, loss = 0.0006980232629985039\n",
            "Epoch 13 Step 127 of 219, loss = 3.6238776488062285e-05\n",
            "Epoch 13 Step 128 of 219, loss = 0.00014449909411951012\n",
            "Epoch 13 Step 129 of 219, loss = 0.00014246833097786293\n",
            "Epoch 13 Step 130 of 219, loss = 0.00012662573777788566\n",
            "Epoch 13 Step 131 of 219, loss = 8.14197377394521e-05\n",
            "Epoch 13 Step 132 of 219, loss = 0.00010627099271687257\n",
            "Epoch 13 Step 133 of 219, loss = 9.641858935083292e-05\n",
            "Epoch 13 Step 134 of 219, loss = 9.480803112182912e-05\n",
            "Epoch 13 Step 135 of 219, loss = 0.0013860743399618514\n",
            "Epoch 13 Step 136 of 219, loss = 5.4513405132183834e-05\n",
            "Epoch 13 Step 137 of 219, loss = 0.6658638432924135\n",
            "Epoch 13 Step 138 of 219, loss = 0.00011615836331202445\n",
            "Epoch 13 Step 139 of 219, loss = 0.0006577368391731397\n",
            "Epoch 13 Step 140 of 219, loss = 1.407853625653047\n",
            "Epoch 13 Step 141 of 219, loss = 0.0003471674818911197\n",
            "Epoch 13 Step 142 of 219, loss = 0.00017169648015169514\n",
            "Epoch 13 Step 143 of 219, loss = 0.009009327170815595\n",
            "Epoch 13 Step 144 of 219, loss = 0.18223122022425287\n",
            "Epoch 13 Step 145 of 219, loss = 8.334513165664248e-05\n",
            "Epoch 13 Step 146 of 219, loss = 0.00020870893342816998\n",
            "Epoch 13 Step 147 of 219, loss = 0.0006356024861133847\n",
            "Epoch 13 Step 148 of 219, loss = 0.00021964931613638328\n",
            "Epoch 13 Step 149 of 219, loss = 0.0001958959751391376\n",
            "Epoch 13 Step 150 of 219, loss = 0.0009796038048079936\n",
            "Epoch 13 Step 151 of 219, loss = 0.00020302419238760194\n",
            "Epoch 13 Step 152 of 219, loss = 0.0003296955295581938\n",
            "Epoch 13 Step 153 of 219, loss = 0.00024933294980655774\n",
            "Epoch 13 Step 154 of 219, loss = 0.0002692030615207841\n",
            "Epoch 13 Step 155 of 219, loss = 0.00042507235843913804\n",
            "Epoch 13 Step 156 of 219, loss = 0.00018836649678632966\n",
            "Epoch 13 Step 157 of 219, loss = 0.00025666787951195147\n",
            "Epoch 13 Step 158 of 219, loss = 0.00027992204934434994\n",
            "Epoch 13 Step 159 of 219, loss = 0.003884384358116222\n",
            "Epoch 13 Step 160 of 219, loss = 0.000480993238852534\n",
            "Epoch 13 Step 161 of 219, loss = 0.00024347939597646473\n",
            "Epoch 13 Step 162 of 219, loss = 0.00028306139984124457\n",
            "Epoch 13 Step 163 of 219, loss = 0.00022816471573605668\n",
            "Epoch 13 Step 164 of 219, loss = 0.00014162198738176812\n",
            "Epoch 13 Step 165 of 219, loss = 0.0003160576960681283\n",
            "Epoch 13 Step 166 of 219, loss = 0.0002220105693595542\n",
            "Epoch 13 Step 167 of 219, loss = 0.0015236832630307617\n",
            "Epoch 13 Step 168 of 219, loss = 0.0002651114876925931\n",
            "Epoch 13 Step 169 of 219, loss = 0.00023653089078834455\n",
            "Epoch 13 Step 170 of 219, loss = 0.00018239323964053256\n",
            "Epoch 13 Step 171 of 219, loss = 0.0005446993443456449\n",
            "Epoch 13 Step 172 of 219, loss = 0.0005008477733099426\n",
            "Epoch 13 Step 173 of 219, loss = 0.0003080802337080968\n",
            "Epoch 13 Step 174 of 219, loss = 0.0001442431336045047\n",
            "Epoch 13 Step 175 of 219, loss = 0.0003204100494258455\n",
            "Epoch 13 Step 176 of 219, loss = 0.0005560976755987213\n",
            "Epoch 13 Step 177 of 219, loss = 0.0003267285010224441\n",
            "Epoch 13 Step 178 of 219, loss = 0.00018693048968998482\n",
            "Epoch 13 Step 179 of 219, loss = 9.876585176016306e-05\n",
            "Epoch 13 Step 180 of 219, loss = 0.00048633378310114495\n",
            "Epoch 13 Step 181 of 219, loss = 0.0006374182451054367\n",
            "Epoch 13 Step 182 of 219, loss = 0.00011307758131806622\n",
            "Epoch 13 Step 183 of 219, loss = 0.00011294178807474964\n",
            "Epoch 13 Step 184 of 219, loss = 0.00021628377970728252\n",
            "Epoch 13 Step 185 of 219, loss = 0.0002492640371656307\n",
            "Epoch 13 Step 186 of 219, loss = 0.0002616044566821074\n",
            "Epoch 13 Step 187 of 219, loss = 0.00023185933503100387\n",
            "Epoch 13 Step 188 of 219, loss = 0.0008936316298786551\n",
            "Epoch 13 Step 189 of 219, loss = 0.00019595946730532887\n",
            "Epoch 13 Step 190 of 219, loss = 0.0015737796805979087\n",
            "Epoch 13 Step 191 of 219, loss = 0.0008487223707334124\n",
            "Epoch 13 Step 192 of 219, loss = 0.0004398340868192463\n",
            "Epoch 13 Step 193 of 219, loss = 0.0001203329786676477\n",
            "Epoch 13 Step 194 of 219, loss = 0.00010648370584931399\n",
            "Epoch 13 Step 195 of 219, loss = 0.0001376825484840083\n",
            "Epoch 13 Step 196 of 219, loss = 0.00015198890832834877\n",
            "Epoch 13 Step 197 of 219, loss = 0.0002658652706486464\n",
            "Epoch 13 Step 198 of 219, loss = 0.00017630411036861915\n",
            "Epoch 13 Step 199 of 219, loss = 0.0002920436502336088\n",
            "Epoch 13 Step 200 of 219, loss = 0.00014592106447253173\n",
            "Epoch 13 Step 201 of 219, loss = 0.00012452928922357387\n",
            "Epoch 13 Step 202 of 219, loss = 0.00016761040365054214\n",
            "Epoch 13 Step 203 of 219, loss = 0.00013973684781376505\n",
            "Epoch 13 Step 204 of 219, loss = 0.000231867175671141\n",
            "Epoch 13 Step 205 of 219, loss = 0.0002785471782544846\n",
            "Epoch 13 Step 206 of 219, loss = 0.00010053767539375258\n",
            "Epoch 13 Step 207 of 219, loss = 0.00022250494623676786\n",
            "Epoch 13 Step 208 of 219, loss = 0.0001195206727970799\n",
            "Epoch 13 Step 209 of 219, loss = 0.00014968315554142464\n",
            "Epoch 13 Step 210 of 219, loss = 0.394923815546008\n",
            "Epoch 13 Step 211 of 219, loss = 0.00011364109923306387\n",
            "Epoch 13 Step 212 of 219, loss = 0.0006848124011185064\n",
            "Epoch 13 Step 213 of 219, loss = 0.0001297911305755406\n",
            "Epoch 13 Step 214 of 219, loss = 0.00017075337973437854\n",
            "Epoch 13 Step 215 of 219, loss = 0.00016830940342060785\n",
            "Epoch 13 Step 216 of 219, loss = 0.17372938225344114\n",
            "Epoch 13 Step 217 of 219, loss = 0.0002883677740328494\n",
            "Epoch 13 Step 218 of 219, loss = 0.000465888482419056\n",
            "Epoch 13 average train_loss: 0.032119 test_loss: 2.712863 test_score 0.58%\n",
            "Epoch 14 Step 0 of 219, loss = 0.00022214259763586597\n",
            "Epoch 14 Step 1 of 219, loss = 2.9585736342596647e-05\n",
            "Epoch 14 Step 2 of 219, loss = 0.008660715929863727\n",
            "Epoch 14 Step 3 of 219, loss = 0.00027753270342145697\n",
            "Epoch 14 Step 4 of 219, loss = 4.6825584490761685e-05\n",
            "Epoch 14 Step 5 of 219, loss = 6.513450614420435e-05\n",
            "Epoch 14 Step 6 of 219, loss = 3.7602065191322254e-05\n",
            "Epoch 14 Step 7 of 219, loss = 0.00017797610166780942\n",
            "Epoch 14 Step 8 of 219, loss = 0.0001150802349911828\n",
            "Epoch 14 Step 9 of 219, loss = 9.62386426692774e-05\n",
            "Epoch 14 Step 10 of 219, loss = 9.488844759175663e-05\n",
            "Epoch 14 Step 11 of 219, loss = 7.74640968188578e-05\n",
            "Epoch 14 Step 12 of 219, loss = 4.145306706959673e-05\n",
            "Epoch 14 Step 13 of 219, loss = 0.00015676080101911793\n",
            "Epoch 14 Step 14 of 219, loss = 0.022085651901306846\n",
            "Epoch 14 Step 15 of 219, loss = 2.954842091185128e-05\n",
            "Epoch 14 Step 16 of 219, loss = 0.711684908661482\n",
            "Epoch 14 Step 17 of 219, loss = 0.27481388623652947\n",
            "Epoch 14 Step 18 of 219, loss = 0.013807007097739188\n",
            "Epoch 14 Step 19 of 219, loss = 0.00045223645724945527\n",
            "Epoch 14 Step 20 of 219, loss = 0.3234125145311282\n",
            "Epoch 14 Step 21 of 219, loss = 0.00029215809666993664\n",
            "Epoch 14 Step 22 of 219, loss = 0.0002048220017059066\n",
            "Epoch 14 Step 23 of 219, loss = 0.7945407103095476\n",
            "Epoch 14 Step 24 of 219, loss = 8.06581764436487e-05\n",
            "Epoch 14 Step 25 of 219, loss = 0.21348149226497526\n",
            "Epoch 14 Step 26 of 219, loss = 4.587883921658431e-05\n",
            "Epoch 14 Step 27 of 219, loss = 7.429829472016536e-05\n",
            "Epoch 14 Step 28 of 219, loss = 1.536294161041951e-05\n",
            "Epoch 14 Step 29 of 219, loss = 4.615435705090931e-05\n",
            "Epoch 14 Step 30 of 219, loss = 8.932200466915674e-05\n",
            "Epoch 14 Step 31 of 219, loss = 4.587142348100315e-05\n",
            "Epoch 14 Step 32 of 219, loss = 1.3507795529221767e-05\n",
            "Epoch 14 Step 33 of 219, loss = 0.00023724787496348654\n",
            "Epoch 14 Step 34 of 219, loss = 9.348070426540289e-05\n",
            "Epoch 14 Step 35 of 219, loss = 1.608564252819633e-05\n",
            "Epoch 14 Step 36 of 219, loss = 3.3317017198442045e-05\n",
            "Epoch 14 Step 37 of 219, loss = 1.7307459671656034e-05\n",
            "Epoch 14 Step 38 of 219, loss = 1.4982986897393857e-05\n",
            "Epoch 14 Step 39 of 219, loss = 3.156748960009281e-05\n",
            "Epoch 14 Step 40 of 219, loss = 2.0928398868136355e-05\n",
            "Epoch 14 Step 41 of 219, loss = 3.272941836485188e-05\n",
            "Epoch 14 Step 42 of 219, loss = 0.00011508035777296755\n",
            "Epoch 14 Step 43 of 219, loss = 1.3299189106419362e-05\n",
            "Epoch 14 Step 44 of 219, loss = 2.6151061604196002e-05\n",
            "Epoch 14 Step 45 of 219, loss = 8.14557458568288e-05\n",
            "Epoch 14 Step 46 of 219, loss = 7.531518579639851e-05\n",
            "Epoch 14 Step 47 of 219, loss = 0.0005708198682441434\n",
            "Epoch 14 Step 48 of 219, loss = 1.9632035389349767e-05\n",
            "Epoch 14 Step 49 of 219, loss = 0.0003305478142010543\n",
            "Epoch 14 Step 50 of 219, loss = 2.338705951387965e-05\n",
            "Epoch 14 Step 51 of 219, loss = 1.736710390787266e-05\n",
            "Epoch 14 Step 52 of 219, loss = 2.0548401664655103e-05\n",
            "Epoch 14 Step 53 of 219, loss = 4.462590248976994e-05\n",
            "Epoch 14 Step 54 of 219, loss = 0.00010830052343635543\n",
            "Epoch 14 Step 55 of 219, loss = 1.724788768342478e-05\n",
            "Epoch 14 Step 56 of 219, loss = 4.415021186332524e-05\n",
            "Epoch 14 Step 57 of 219, loss = 1.3612105078664172e-05\n",
            "Epoch 14 Step 58 of 219, loss = 0.00044118230323419994\n",
            "Epoch 14 Step 59 of 219, loss = 9.403725493939419e-05\n",
            "Epoch 14 Step 60 of 219, loss = 4.714562163599112e-05\n",
            "Epoch 14 Step 61 of 219, loss = 5.201042495173169e-05\n",
            "Epoch 14 Step 62 of 219, loss = 3.481534520233254e-05\n",
            "Epoch 14 Step 63 of 219, loss = 4.292098424230062e-05\n",
            "Epoch 14 Step 64 of 219, loss = 2.124133459346922e-05\n",
            "Epoch 14 Step 65 of 219, loss = 3.4145179938605e-05\n",
            "Epoch 14 Step 66 of 219, loss = 2.1256029725691405e-05\n",
            "Epoch 14 Step 67 of 219, loss = 8.875043130274207e-05\n",
            "Epoch 14 Step 68 of 219, loss = 0.0007268006181107012\n",
            "Epoch 14 Step 69 of 219, loss = 2.04441213611517e-05\n",
            "Epoch 14 Step 70 of 219, loss = 1.6897694081308146e-05\n",
            "Epoch 14 Step 71 of 219, loss = 0.00029509188345855364\n",
            "Epoch 14 Step 72 of 219, loss = 2.423609868174026e-05\n",
            "Epoch 14 Step 73 of 219, loss = 5.508554397692933e-05\n",
            "Epoch 14 Step 74 of 219, loss = 5.878220764543585e-05\n",
            "Epoch 14 Step 75 of 219, loss = 1.9363728853249995e-05\n",
            "Epoch 14 Step 76 of 219, loss = 7.606931114878535e-05\n",
            "Epoch 14 Step 77 of 219, loss = 6.450094349474966e-05\n",
            "Epoch 14 Step 78 of 219, loss = 3.494158909234102e-05\n",
            "Epoch 14 Step 79 of 219, loss = 2.2679122082536196e-05\n",
            "Epoch 14 Step 80 of 219, loss = 4.5899749181899097e-05\n",
            "Epoch 14 Step 81 of 219, loss = 2.5219782912699884e-05\n",
            "Epoch 14 Step 82 of 219, loss = 1.943820197425339e-05\n",
            "Epoch 14 Step 83 of 219, loss = 4.096889705351714e-05\n",
            "Epoch 14 Step 84 of 219, loss = 8.11522795061137e-05\n",
            "Epoch 14 Step 85 of 219, loss = 9.793709006089557e-05\n",
            "Epoch 14 Step 86 of 219, loss = 6.30298043233779e-05\n",
            "Epoch 14 Step 87 of 219, loss = 0.00023458285534161405\n",
            "Epoch 14 Step 88 of 219, loss = 3.993299912963266e-05\n",
            "Epoch 14 Step 89 of 219, loss = 0.7187498568706587\n",
            "Epoch 14 Step 90 of 219, loss = 3.1522601233291425e-05\n",
            "Epoch 14 Step 91 of 219, loss = 0.00010076409330395109\n",
            "Epoch 14 Step 92 of 219, loss = 4.3910981958106277e-05\n",
            "Epoch 14 Step 93 of 219, loss = 8.713852383834819e-05\n",
            "Epoch 14 Step 94 of 219, loss = 4.660033260961427e-05\n",
            "Epoch 14 Step 95 of 219, loss = 2.3260086152276926e-05\n",
            "Epoch 14 Step 96 of 219, loss = 0.0039002643173944307\n",
            "Epoch 14 Step 97 of 219, loss = 1.8849559381806102e-05\n",
            "Epoch 14 Step 98 of 219, loss = 0.00012572326136250922\n",
            "Epoch 14 Step 99 of 219, loss = 0.0007884854784379058\n",
            "Epoch 14 Step 100 of 219, loss = 7.912097282769537e-05\n",
            "Epoch 14 Step 101 of 219, loss = 0.00010477275816356268\n",
            "Epoch 14 Step 102 of 219, loss = 0.00014589028069167398\n",
            "Epoch 14 Step 103 of 219, loss = 0.0007035490731652772\n",
            "Epoch 14 Step 104 of 219, loss = 4.137847577112552e-05\n",
            "Epoch 14 Step 105 of 219, loss = 1.9393620220853336e-05\n",
            "Epoch 14 Step 106 of 219, loss = 4.519370907019038e-05\n",
            "Epoch 14 Step 107 of 219, loss = 4.369633899159453e-05\n",
            "Epoch 14 Step 108 of 219, loss = 0.0011469078916093167\n",
            "Epoch 14 Step 109 of 219, loss = 3.971042679040693e-05\n",
            "Epoch 14 Step 110 of 219, loss = 0.6272332260887197\n",
            "Epoch 14 Step 111 of 219, loss = 0.00023410106479104797\n",
            "Epoch 14 Step 112 of 219, loss = 3.5657007629197324e-05\n",
            "Epoch 14 Step 113 of 219, loss = 8.908855124900583e-05\n",
            "Epoch 14 Step 114 of 219, loss = 4.4098357079747075e-05\n",
            "Epoch 14 Step 115 of 219, loss = 5.569804508809284e-05\n",
            "Epoch 14 Step 116 of 219, loss = 7.73446379298548e-05\n",
            "Epoch 14 Step 117 of 219, loss = 4.347051259401269e-05\n",
            "Epoch 14 Step 118 of 219, loss = 2.7767294625391514e-05\n",
            "Epoch 14 Step 119 of 219, loss = 3.997795602117549e-05\n",
            "Epoch 14 Step 120 of 219, loss = 3.063618731857787e-05\n",
            "Epoch 14 Step 121 of 219, loss = 2.7261183163318492e-05\n",
            "Epoch 14 Step 122 of 219, loss = 5.712759764264774e-05\n",
            "Epoch 14 Step 123 of 219, loss = 1.504258742102138e-05\n",
            "Epoch 14 Step 124 of 219, loss = 2.5346143530668996e-05\n",
            "Epoch 14 Step 125 of 219, loss = 5.3432069620384937e-05\n",
            "Epoch 14 Step 126 of 219, loss = 0.0003217321836359588\n",
            "Epoch 14 Step 127 of 219, loss = 1.2494539419094508e-05\n",
            "Epoch 14 Step 128 of 219, loss = 2.7536873005828966e-05\n",
            "Epoch 14 Step 129 of 219, loss = 0.00011086317897479603\n",
            "Epoch 14 Step 130 of 219, loss = 2.933970711183065e-05\n",
            "Epoch 14 Step 131 of 219, loss = 4.014823542775048e-05\n",
            "Epoch 14 Step 132 of 219, loss = 2.5078245130316645e-05\n",
            "Epoch 14 Step 133 of 219, loss = 0.00013089379029906922\n",
            "Epoch 14 Step 134 of 219, loss = 8.422169793220746e-05\n",
            "Epoch 14 Step 135 of 219, loss = 0.0003003381648341019\n",
            "Epoch 14 Step 136 of 219, loss = 3.507625021370586e-05\n",
            "Epoch 14 Step 137 of 219, loss = 0.19860991413770535\n",
            "Epoch 14 Step 138 of 219, loss = 7.863619669024047e-05\n",
            "Epoch 14 Step 139 of 219, loss = 0.0001324577156083251\n",
            "Epoch 14 Step 140 of 219, loss = 0.8329179738589119\n",
            "Epoch 14 Step 141 of 219, loss = 7.551386499926593e-05\n",
            "Epoch 14 Step 142 of 219, loss = 0.0006019841071065457\n",
            "Epoch 14 Step 143 of 219, loss = 0.00015389347200311931\n",
            "Epoch 14 Step 144 of 219, loss = 7.201988231031464e-05\n",
            "Epoch 14 Step 145 of 219, loss = 9.698304472749442e-05\n",
            "Epoch 14 Step 146 of 219, loss = 0.00017532295424871336\n",
            "Epoch 14 Step 147 of 219, loss = 0.004810858990509814\n",
            "Epoch 14 Step 148 of 219, loss = 0.0012883715854172806\n",
            "Epoch 14 Step 149 of 219, loss = 0.06924062170563161\n",
            "Epoch 14 Step 150 of 219, loss = 0.0016344623734312336\n",
            "Epoch 14 Step 151 of 219, loss = 0.00015677455434115473\n",
            "Epoch 14 Step 152 of 219, loss = 1.0805398184355113\n",
            "Epoch 14 Step 153 of 219, loss = 0.00014362136428758276\n",
            "Epoch 14 Step 154 of 219, loss = 0.0001566076850281206\n",
            "Epoch 14 Step 155 of 219, loss = 0.0009104626454359277\n",
            "Epoch 14 Step 156 of 219, loss = 0.0002406796859872884\n",
            "Epoch 14 Step 157 of 219, loss = 0.0005598752420610253\n",
            "Epoch 14 Step 158 of 219, loss = 0.00016741200403203038\n",
            "Epoch 14 Step 159 of 219, loss = 0.7527465792570638\n",
            "Epoch 14 Step 160 of 219, loss = 0.0002572719359932307\n",
            "Epoch 14 Step 161 of 219, loss = 0.00016295048681058688\n",
            "Epoch 14 Step 162 of 219, loss = 0.0001678158080267167\n",
            "Epoch 14 Step 163 of 219, loss = 0.00014147540673548065\n",
            "Epoch 14 Step 164 of 219, loss = 8.779988309015607e-05\n",
            "Epoch 14 Step 165 of 219, loss = 0.0004896241963479042\n",
            "Epoch 14 Step 166 of 219, loss = 0.0001386808103234216\n",
            "Epoch 14 Step 167 of 219, loss = 0.0004012953297660715\n",
            "Epoch 14 Step 168 of 219, loss = 0.0001863019087977591\n",
            "Epoch 14 Step 169 of 219, loss = 0.00015573775749544438\n",
            "Epoch 14 Step 170 of 219, loss = 0.00024332585235242732\n",
            "Epoch 14 Step 171 of 219, loss = 0.0002417316079572629\n",
            "Epoch 14 Step 172 of 219, loss = 0.00022818964407633757\n",
            "Epoch 14 Step 173 of 219, loss = 0.00017018561698023404\n",
            "Epoch 14 Step 174 of 219, loss = 0.00015154753884871752\n",
            "Epoch 14 Step 175 of 219, loss = 0.00024368577692257531\n",
            "Epoch 14 Step 176 of 219, loss = 0.00043446349798159645\n",
            "Epoch 14 Step 177 of 219, loss = 0.0002882032135858026\n",
            "Epoch 14 Step 178 of 219, loss = 0.00017266263080273347\n",
            "Epoch 14 Step 179 of 219, loss = 0.00046030898840854206\n",
            "Epoch 14 Step 180 of 219, loss = 0.0015207407375896764\n",
            "Epoch 14 Step 181 of 219, loss = 0.00036408418282007915\n",
            "Epoch 14 Step 182 of 219, loss = 7.76088987777257e-05\n",
            "Epoch 14 Step 183 of 219, loss = 6.474964578728759e-05\n",
            "Epoch 14 Step 184 of 219, loss = 0.00016657431314115456\n",
            "Epoch 14 Step 185 of 219, loss = 0.0001944570221894537\n",
            "Epoch 14 Step 186 of 219, loss = 0.17326019083111532\n",
            "Epoch 14 Step 187 of 219, loss = 0.00013799991367591247\n",
            "Epoch 14 Step 188 of 219, loss = 0.0001334889241206838\n",
            "Epoch 14 Step 189 of 219, loss = 0.00025503204835786164\n",
            "Epoch 14 Step 190 of 219, loss = 0.00013960135584056843\n",
            "Epoch 14 Step 191 of 219, loss = 0.00016589613898077005\n",
            "Epoch 14 Step 192 of 219, loss = 0.0006302485144260572\n",
            "Epoch 14 Step 193 of 219, loss = 9.31325990904952e-05\n",
            "Epoch 14 Step 194 of 219, loss = 8.753184442866768e-05\n",
            "Epoch 14 Step 195 of 219, loss = 0.00010791487557071378\n",
            "Epoch 14 Step 196 of 219, loss = 8.138524958667404e-05\n",
            "Epoch 14 Step 197 of 219, loss = 0.0006018272979417816\n",
            "Epoch 14 Step 198 of 219, loss = 6.898086951423466e-05\n",
            "Epoch 14 Step 199 of 219, loss = 9.034721449552308e-05\n",
            "Epoch 14 Step 200 of 219, loss = 9.982947881326254e-05\n",
            "Epoch 14 Step 201 of 219, loss = 0.0001584373437708564\n",
            "Epoch 14 Step 202 of 219, loss = 0.00010487362709454828\n",
            "Epoch 14 Step 203 of 219, loss = 0.00010310659229162411\n",
            "Epoch 14 Step 204 of 219, loss = 0.00020531996688077925\n",
            "Epoch 14 Step 205 of 219, loss = 6.443008658152394e-05\n",
            "Epoch 14 Step 206 of 219, loss = 0.00011634002800064991\n",
            "Epoch 14 Step 207 of 219, loss = 0.0001177309515583147\n",
            "Epoch 14 Step 208 of 219, loss = 8.96097353688674e-05\n",
            "Epoch 14 Step 209 of 219, loss = 8.881301778274064e-05\n",
            "Epoch 14 Step 210 of 219, loss = 0.0002312554283321333\n",
            "Epoch 14 Step 211 of 219, loss = 9.964171511001041e-05\n",
            "Epoch 14 Step 212 of 219, loss = 0.002548478603557669\n",
            "Epoch 14 Step 213 of 219, loss = 0.0005399399271937\n",
            "Epoch 14 Step 214 of 219, loss = 0.00012701129526249133\n",
            "Epoch 14 Step 215 of 219, loss = 0.0001030701874924489\n",
            "Epoch 14 Step 216 of 219, loss = 0.00012370512592951854\n",
            "Epoch 14 Step 217 of 219, loss = 0.007312205503353653\n",
            "Epoch 14 Step 218 of 219, loss = 9.844872003365404e-05\n",
            "Epoch 14 average train_loss: 0.031355 test_loss: 2.839072 test_score 0.57%\n"
          ]
        }
      ],
      "source": [
        "for iEpoch in range(15):\n",
        "\n",
        "    # train\n",
        "    train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n",
        "        model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n",
        "    \n",
        "    bert_train_acc.append(train_score)\n",
        "    bert_avg_train_loss.append(avg_train_loss)\n",
        "    bert_train_predict.append(y_train_prediction)\n",
        "\n",
        "    ### evaluate\n",
        "    test_score, avg_test_loss, y_test_prediction = evaluateModel(\n",
        "        model, test_dataloader, device)\n",
        "\n",
        "    bert_test_acc.append(test_score)\n",
        "    bert_avg_test_loss.append(avg_test_loss)\n",
        "    bert_test_predict.append(y_test_prediction)\n",
        "\n",
        "    # log epoch\n",
        "    print(f'Epoch {iEpoch} average train_loss: {avg_train_loss:.6f} test_loss: {avg_test_loss:.6f} test_score {test_score:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Graphics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x24e3ee38460>]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(bert_train_acc)\n",
        "plt.plot(bert_test_acc)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(bert_avg_train_loss)\n",
        "plt.plot(bert_avg_test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model RuBert-Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://huggingface.co/sberbank-ai/ruRoberta-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model_path = os.path.join(base_path, 'ruRoberta-large/')\n",
        "#print(model_path)\n",
        "#\n",
        "#config_path = os.path.join(base_path, model_path, 'config.json')\n",
        "#conf = BertConfig.from_json_file(config_path)\n",
        "#conf.num_labels = len(l2i)\n",
        "#\n",
        "#output_model_file = os.path.join(base_path, model_path, 'pytorch_model.bin')\n",
        "#\n",
        "#model = BertModel(conf)\n",
        "#\n",
        "#model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "#model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/ruRoberta-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, attention_masks, token_type_ids = encode_text_pairs(tokenizer, all_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepeare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "train_dataloader = createDataLoader(part2indices['train_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)\n",
        "validate_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels, BATCH_SIZE_LOADER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-Trained BERTA-LARGE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"sberbank-ai/ruRoberta-large\")\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=LEARNING_RATE, \n",
        "    correct_bias=False)\n",
        "    \n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=LEARNING_RATE, \n",
        "    steps_per_epoch=len(train_dataloader), \n",
        "    epochs=EPOCHS_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "lberta_train_predict = []\n",
        "lberta_test_predict = []\n",
        "\n",
        "lberta_avg_train_loss = []\n",
        "lberta_avg_test_loss = []\n",
        "\n",
        "lberta_train_acc = []\n",
        "lberta_test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8284\\174737808.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n\u001b[0m\u001b[0;32m      5\u001b[0m         model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8284\\1998468137.py\u001b[0m in \u001b[0;36mtrainModelIteration\u001b[1;34m(model, optimizer, scheduler, in_dataloader, MAX_GRAD_NORM, EPOCH_INDEX)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         outputs = model(\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_token_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m         outputs = self.roberta(\n\u001b[0m\u001b[0;32m   1092\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    838\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[1;34m\"but got {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "for iEpoch in range(3):\n",
        "\n",
        "    # train\n",
        "    train_score, avg_train_loss, y_train_prediction = trainModelIteration(\n",
        "        model, optimizer, scheduler, train_dataloader, MAX_GRAD_NORM, iEpoch)\n",
        "    \n",
        "    lberta_train_acc.append(train_score)\n",
        "    lberta_avg_train_loss.append(avg_train_loss)\n",
        "    lberta_train_predict.append(y_train_prediction)\n",
        "\n",
        "    ### evaluate\n",
        "    test_score, avg_test_loss, y_test_prediction = evaluateModel(\n",
        "        model, test_dataloader, device)\n",
        "\n",
        "    lberta_test_acc.append(test_score)\n",
        "    lberta_avg_test_loss.append(avg_test_loss)\n",
        "    lberta_test_predict.append(y_test_prediction)\n",
        "\n",
        "    # log epoch\n",
        "    print(f'Epoch {iEpoch} average train_loss: {avg_train_loss:.6f} test_loss: {avg_test_loss:.6f} test_score {test_score:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x26220981b80>]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(lberta_train_acc)\n",
        "plt.plot(lberta_test_acc)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(lberta_avg_train_loss)\n",
        "plt.plot(lberta_avg_test_loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch_seq2seq_with_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "846dd53c5a100503afcb3f5301bb10f61481596a80ae839ecd432be859b5d4d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
