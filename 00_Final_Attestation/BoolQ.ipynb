{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Проект 3. Решить задачу DaNetQA / BoolQ\n",
        "\n",
        "Можно решить как задачу для русского, так и для английского.\n",
        "\n",
        "Либо провести эксперименты с многоязычной моделью\n",
        "\n",
        "https://russiansuperglue.com/ru/tasks/task_info/DaNetQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Описание\n",
        "Причинно-следственная связь, логический вывод, Natural Language Inference\n",
        "\n",
        "DaNetQA - это набор да/нет вопросов с ответами и фрагментом текста, содержащим ответ. Все вопросы были написаны авторами без каких-либо искусственных ограничений.\n",
        "\n",
        "Каждый пример представляет собой триплет (вопрос, фрагмент текста, ответ) с заголовком страницы в качестве необязательного дополнительного контекста.\n",
        "\n",
        "Настройка классификации текстовых пар аналогична существующим задачам логического вывода (NLI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тип задачи\n",
        "Логика, Commonsense, Знания о мире. Бинарная классификация: true/false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\leysh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadJSONL(path, name):\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    #df = df.set_index('idx')\n",
        "    print(name)\n",
        "    display(df.head())\n",
        "    if (df.columns.values == 'label').any():\n",
        "        s = np.unique(df['label'].to_numpy(), return_counts=True)[1]\n",
        "        print(f\"True answer: {s[1]}\")\n",
        "        print(f\"False answer: {s[0]}\")\n",
        "        print(\"\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>«Вы́ставочный центр» — станция Московского мон...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Вднх - это выставочный центр?</td>\n",
              "      <td>Вы́ставка достиже́ний наро́дного хозя́йства  ,...</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Был ли джиган в black star?</td>\n",
              "      <td>Вместе с этим треком они выступили на церемони...</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Xiaomi конкурент apple?</td>\n",
              "      <td>Xiaomi — китайская компания, основанная в 2010...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли автомат калашникова в вов?</td>\n",
              "      <td>Отметив некоторые недостатки и в целом удачную...</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            question  \\\n",
              "0      Вднх - это выставочный центр?   \n",
              "1      Вднх - это выставочный центр?   \n",
              "2        Был ли джиган в black star?   \n",
              "3            Xiaomi конкурент apple?   \n",
              "4  Был ли автомат калашникова в вов?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  «Вы́ставочный центр» — станция Московского мон...   True    0  \n",
              "1  Вы́ставка достиже́ний наро́дного хозя́йства  ,...   True    1  \n",
              "2  Вместе с этим треком они выступили на церемони...   True    2  \n",
              "3  Xiaomi — китайская компания, основанная в 2010...   True    3  \n",
              "4  Отметив некоторые недостатки и в целом удачную...  False    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 1061\n",
            "False answer: 688\n",
            "\n",
            "Validation set\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Есть ли вода на марсе?</td>\n",
              "      <td>Гидросфера Марса — это совокупность водных зап...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Состоит ли англия в евросоюзе?</td>\n",
              "      <td>В полночь с 31 января на 1 февраля 2020 года п...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Действительно ли в ссср не было адвокатов?</td>\n",
              "      <td>Семён Львович Ария  — советский и российский ю...</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Была ли чума в оране?</td>\n",
              "      <td>Чума — это и абсурд, что осмысливается как фор...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли кетчуп в читосе?</td>\n",
              "      <td>Текущий каталог продукции размещен на сайте пр...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     question  \\\n",
              "0                      Есть ли вода на марсе?   \n",
              "1              Состоит ли англия в евросоюзе?   \n",
              "2  Действительно ли в ссср не было адвокатов?   \n",
              "3                       Была ли чума в оране?   \n",
              "4                     Был ли кетчуп в читосе?   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  Гидросфера Марса — это совокупность водных зап...   True    0  \n",
              "1  В полночь с 31 января на 1 февраля 2020 года п...  False    1  \n",
              "2  Семён Львович Ария  — советский и российский ю...  False    2  \n",
              "3  Чума — это и абсурд, что осмысливается как фор...   True    3  \n",
              "4  Текущий каталог продукции размещен на сайте пр...   True    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True answer: 412\n",
            "False answer: 409\n",
            "\n",
            "Test set:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полезна ли ртуть с градусника?</td>\n",
              "      <td>Отравления ртутью  — расстройства здоровья, св...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Являются ли сапрофаги хищниками?</td>\n",
              "      <td>Фауна лесных почв — совокупность видов животны...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Водятся ли в индии крокодилы?</td>\n",
              "      <td>Болотный крокодил, или магер  — пресмыкающееся...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Есть ли в батате крахмал?</td>\n",
              "      <td>Клубневидно вздутые корни  весят до 15 кг, сод...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Был ли человек в железной маске?</td>\n",
              "      <td>Остров Сент-Маргерит  — крупнейший из Лерински...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           question  \\\n",
              "0    Полезна ли ртуть с градусника?   \n",
              "1  Являются ли сапрофаги хищниками?   \n",
              "2     Водятся ли в индии крокодилы?   \n",
              "3         Есть ли в батате крахмал?   \n",
              "4  Был ли человек в железной маске?   \n",
              "\n",
              "                                             passage  idx  \n",
              "0  Отравления ртутью  — расстройства здоровья, св...    0  \n",
              "1  Фауна лесных почв — совокупность видов животны...    1  \n",
              "2  Болотный крокодил, или магер  — пресмыкающееся...    2  \n",
              "3  Клубневидно вздутые корни  весят до 15 кг, сод...    3  \n",
              "4  Остров Сент-Маргерит  — крупнейший из Лерински...    4  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_train = loadJSONL(\"DaNetQA/raw_train.jsonl\", \"Train set\")\n",
        "df_validation = loadJSONL(\"DaNetQA/raw_val.jsonl\", \"Validation set\")\n",
        "df_test = loadJSONL(\"DaNetQA/raw_test.jsonl\", \"Test set:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Очистка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataCleaner:\n",
        "    def __init__(self) -> None:\n",
        "        self.flag_verbose = True\n",
        "\n",
        "        self.stop_words = stopwords.words('russian')\n",
        "        self.stemmer = SnowballStemmer('russian')\n",
        "\n",
        "        self.count_removed_symbols = dict()\n",
        "        self.count_removed_words = dict()\n",
        "\n",
        "        self.count_replaced_symbols = dict()\n",
        "        self.dict_replaced_symbols = dict()\n",
        "\n",
        "        self.count_replaced_words = dict()\n",
        "        self.dict_replaced_words = dict()\n",
        "\n",
        "        self.char_to_remove = ['«', '»', '—', ',', '.', '-', '/', ':', '!', \"?\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"=\", \"|\", \"\\\\\", \">\", \"<\"]\n",
        "        self.char_to_replace = [['ё', 'е']]\n",
        "\n",
        "    # функция подсчета количества измененных слов\n",
        "    def addReplacedWord(self, s_from, s_to = ' '):\n",
        "        if not self.count_replaced_words.keys().__contains__(s_from):\n",
        "            self.count_replaced_words[s_from] = 0\n",
        "        self.count_replaced_words[s_from] += 1\n",
        "        self.dict_replaced_words[s_from] = s_to\n",
        "\n",
        "    # функция подсчета количества удаленных слов\n",
        "    def addRemovedWord(self, w):\n",
        "        if w == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(w):\n",
        "                self.count_removed_symbols[w] = 0\n",
        "            self.count_removed_symbols[w] += 1\n",
        "\n",
        "    # функция подсчета количества удаленных символов\n",
        "    def addReplacedSymbol(self, s_from, s_to = ' '):\n",
        "        if s_to == ' ':\n",
        "            if not self.count_removed_symbols.keys().__contains__(s_from):\n",
        "                self.count_removed_symbols[s_from] = 0\n",
        "            self.count_removed_symbols[s_from] += 1\n",
        "        else:\n",
        "            if not self.count_replaced_symbols.keys().__contains__(s_from):\n",
        "                self.count_replaced_symbols[s_from] = 0\n",
        "            self.count_replaced_symbols[s_from] += 1\n",
        "            self.dict_replaced_symbols[s_from] = s_to\n",
        "\n",
        "    # удаление знаков ударения и прочих символов unicode\n",
        "    def unicodeToAscii(self, s):\n",
        "        tmp = []\n",
        "        for c in unicodedata.normalize('NFD', s):\n",
        "            if unicodedata.category(c) != 'Mn':\n",
        "                tmp.append(c)\n",
        "            else:\n",
        "                self.addReplacedSymbol(c)\n",
        "        return ''.join(tmp)\n",
        "\n",
        "    # если нужно удалить, то заменяем на пробел чтоб не потерят разделения слов\n",
        "    def replaceChar(self, s):\n",
        "        tmp = []\n",
        "        for i, c in enumerate(s):\n",
        "            if self.char_to_remove.__contains__(c):\n",
        "                self.addReplacedSymbol(c, s[i])\n",
        "                tmp.append(' ')\n",
        "            else:\n",
        "                tmp.append(c)\n",
        "        s = \"\".join(tmp)\n",
        "\n",
        "        for s_from, s_to in self.char_to_replace:\n",
        "            if c == s_from:\n",
        "                s[i] = s_to\n",
        "                self.addReplacedSymbol(s_from, s_to)\n",
        "        return s\n",
        "\n",
        "    # удаляем лишние пробелы\n",
        "    def trimSpaces(self, s):\n",
        "        while s.__contains__('  '):\n",
        "            s = s.replace('  ', ' ')\n",
        "        s = s.strip()\n",
        "        return s\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def removeStopWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            if word not in self.stop_words:\n",
        "                tmp.append(word)\n",
        "            else:\n",
        "                self.addRemovedWord(word)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    # удаляем слва из stopwords\n",
        "    def StemmWords(self, s):\n",
        "        tmp = []\n",
        "        for word in word_tokenize(s):\n",
        "            wordStemmed = self.stemmer.stem(word)\n",
        "            tmp.append(wordStemmed)\n",
        "            if word != wordStemmed:\n",
        "                self.addReplacedWord(word, wordStemmed)\n",
        "        return \" \".join(tmp)\n",
        "\n",
        "    def clean(self, df, column):\n",
        "        for i in range(len(df)):\n",
        "            df[column][i] = self.unicodeToAscii(df[column][i])\n",
        "            df[column][i] = df[column][i].lower()\n",
        "            df[column][i] = self.replaceChar(df[column][i])\n",
        "            df[column][i] = self.removeStopWords(df[column][i])\n",
        "            df[column][i] = self.StemmWords(df[column][i])\n",
        "            df[column][i] = self.trimSpaces(df[column][i])\n",
        "        return df\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def print(self, vals):\n",
        "        if self.flag_verbose == True:\n",
        "            print(vals)\n",
        "\n",
        "    # прокси для выключения вывода на экран summary\n",
        "    def display(self, vals):\n",
        "            if self.flag_verbose == True:\n",
        "                display(vals)\n",
        "\n",
        "    # сбор лога в dataframe, опциональный вывод на экран \n",
        "    def summary(self, verbose = True):\n",
        "        self.flag_verbose = verbose\n",
        "        dfs = []\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Chars        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_symbols:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Removed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word\", \"count_removed\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.count_removed_words:\n",
        "            current_df = pd.DataFrame([[c, self.count_removed_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Removed Words', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Replaced Chars       ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"symbol_from\", \"symbol_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_symbols:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_symbols[c], self.count_replaced_symbols[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Replaced Chars', dfRemoved])\n",
        "\n",
        "        self.print(\"===================================\")\n",
        "        self.print(\"===        Stemmed Words        ===\")\n",
        "        self.print(\"===================================\")\n",
        "        \n",
        "        cols = [\"word_from\", \"word_to\", \"count_replaced\"]\n",
        "        dfRemoved = pd.DataFrame(columns=cols)\n",
        "        for c in self.dict_replaced_words:\n",
        "            current_df = pd.DataFrame([[ c, self.dict_replaced_words[c], self.count_replaced_words[c]]], columns=cols) \n",
        "            dfRemoved = pd.concat([dfRemoved, current_df], ignore_index=True)\n",
        "        self.display(dfRemoved)\n",
        "        dfs.append(['Stemmed Words', dfRemoved])\n",
        "\n",
        "        return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_6512\\1488101549.py:102: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.unicodeToAscii(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_6512\\1488101549.py:103: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = df[column][i].lower()\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_6512\\1488101549.py:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.replaceChar(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_6512\\1488101549.py:105: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.removeStopWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_6512\\1488101549.py:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.StemmWords(df[column][i])\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_6512\\1488101549.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column][i] = self.trimSpaces(df[column][i])\n"
          ]
        }
      ],
      "source": [
        "t = DataCleaner()\n",
        "df_train = t.clean(df_train, 'passage')\n",
        "df_test = t.clean(df_test, 'passage')\n",
        "df_validation = t.clean(df_validation, 'passage')\n",
        "df_train = t.clean(df_train, 'question')\n",
        "df_test = t.clean(df_test, 'question')\n",
        "df_validation = t.clean(df_validation, 'question')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "========= Data Cleaner log ========\n",
            "===================================\n",
            "===        Removed Chars        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [symbol, count_removed]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Removed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [word, count_removed]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Replaced Chars       ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol_from</th>\n",
              "      <th>symbol_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [symbol_from, symbol_to, count_replaced]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "===        Stemmed Words        ===\n",
            "===================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_from</th>\n",
              "      <th>word_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>компримирова</td>\n",
              "      <td>компримиров</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>бута</td>\n",
              "      <td>бут</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>использов</td>\n",
              "      <td>использ</td>\n",
              "      <td>123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>переоборудов</td>\n",
              "      <td>переоборуд</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>оборудов</td>\n",
              "      <td>оборуд</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681</th>\n",
              "      <td>распостра</td>\n",
              "      <td>распостр</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>всказыв</td>\n",
              "      <td>всказ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>свидетельствов</td>\n",
              "      <td>свидетельств</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>мариху</td>\n",
              "      <td>марих</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>685</th>\n",
              "      <td>застыв</td>\n",
              "      <td>заст</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>686 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          word_from       word_to count_replaced\n",
              "0      компримирова   компримиров              3\n",
              "1              бута           бут              2\n",
              "2         использов       использ            123\n",
              "3      переоборудов    переоборуд              2\n",
              "4          оборудов        оборуд             16\n",
              "..              ...           ...            ...\n",
              "681       распостра      распостр              1\n",
              "682         всказыв         всказ              1\n",
              "683  свидетельствов  свидетельств              1\n",
              "684          мариху         марих              1\n",
              "685          застыв          заст              1\n",
              "\n",
              "[686 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dfs = t.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.to_json(\"DaNetQA/train_c.jsonl\", force_ascii=False, lines=True, orient='records')\n",
        "df_test.to_json(\"DaNetQA/test_c.jsonl\", force_ascii=False, lines=True, orient='records')\n",
        "df_validation.to_json(\"DaNetQA/val_c.jsonl\", force_ascii=False, lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=dfs[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_from</th>\n",
              "      <th>word_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>выставочныи</td>\n",
              "      <td>выставочны</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>станция</td>\n",
              "      <td>станц</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>московского</td>\n",
              "      <td>московск</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>монорельса</td>\n",
              "      <td>монорельс</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>расположена</td>\n",
              "      <td>располож</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50960</th>\n",
              "      <td>мусульманину</td>\n",
              "      <td>мусульманин</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50961</th>\n",
              "      <td>христианке</td>\n",
              "      <td>христианк</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50962</th>\n",
              "      <td>колоннады</td>\n",
              "      <td>колоннад</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50963</th>\n",
              "      <td>ростральных</td>\n",
              "      <td>ростральн</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50964</th>\n",
              "      <td>фотоэпиляция</td>\n",
              "      <td>фотоэпиляц</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50965 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          word_from      word_to count_replaced\n",
              "0       выставочныи   выставочны              7\n",
              "1           станция        станц              7\n",
              "2       московского     московск             26\n",
              "3        монорельса    монорельс              2\n",
              "4       расположена     располож             21\n",
              "...             ...          ...            ...\n",
              "50960  мусульманину  мусульманин              1\n",
              "50961    христианке    христианк              1\n",
              "50962     колоннады     колоннад              1\n",
              "50963   ростральных    ростральн              1\n",
              "50964  фотоэпиляция   фотоэпиляц              1\n",
              "\n",
              "[50965 rows x 3 columns]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_from</th>\n",
              "      <th>word_to</th>\n",
              "      <th>count_replaced</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>года</td>\n",
              "      <td>год</td>\n",
              "      <td>1802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>году</td>\n",
              "      <td>год</td>\n",
              "      <td>1101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>также</td>\n",
              "      <td>такж</td>\n",
              "      <td>984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>является</td>\n",
              "      <td>явля</td>\n",
              "      <td>709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>время</td>\n",
              "      <td>врем</td>\n",
              "      <td>689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32624</th>\n",
              "      <td>триплоидная</td>\n",
              "      <td>триплоидн</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32623</th>\n",
              "      <td>аутосом</td>\n",
              "      <td>аутос</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16557</th>\n",
              "      <td>туман</td>\n",
              "      <td>тума</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16559</th>\n",
              "      <td>подробностеи</td>\n",
              "      <td>подробн</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50964</th>\n",
              "      <td>фотоэпиляция</td>\n",
              "      <td>фотоэпиляц</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50965 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          word_from     word_to count_replaced\n",
              "32             года         год           1802\n",
              "102            году         год           1101\n",
              "140           также        такж            984\n",
              "159        является        явля            709\n",
              "152           время        врем            689\n",
              "...             ...         ...            ...\n",
              "32624   триплоидная   триплоидн              1\n",
              "32623       аутосом       аутос              1\n",
              "16557         туман        тума              1\n",
              "16559  подробностеи     подробн              1\n",
              "50964  фотоэпиляция  фотоэпиляц              1\n",
              "\n",
              "[50965 rows x 3 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sort_values(by=['count_replaced'],ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Number Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>passage</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>вод марс</td>\n",
              "      <td>гидросфер марс эт совокупн водн запас планет м...</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>состо англ евросоюз</td>\n",
              "      <td>полноч 31 январ 1 феврал 2020 год центральноев...</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>деиствительн ссср адвокат</td>\n",
              "      <td>сем львович ар советск россииск юрист крупнеиш...</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>чум оран</td>\n",
              "      <td>чум эт абсурд осмыслива форм существован зла э...</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>кетчуп читос</td>\n",
              "      <td>текущ каталог продукц размещ са производител к...</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    question  \\\n",
              "0                   вод марс   \n",
              "1        состо англ евросоюз   \n",
              "2  деиствительн ссср адвокат   \n",
              "3                   чум оран   \n",
              "4               кетчуп читос   \n",
              "\n",
              "                                             passage  label  idx  \n",
              "0  гидросфер марс эт совокупн водн запас планет м...   True    0  \n",
              "1  полноч 31 январ 1 феврал 2020 год центральноев...  False    1  \n",
              "2  сем львович ар советск россииск юрист крупнеиш...  False    2  \n",
              "3  чум эт абсурд осмыслива форм существован зла э...   True    3  \n",
              "4  текущ каталог продукц размещ са производител к...   True    4  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_vaidation = pd.read_json(\"DaNetQA/val_v1.jsonl\", lines=True)\n",
        "df_vaidation.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ True False False  True  True  True  True  True  True False]\n"
          ]
        }
      ],
      "source": [
        "validation_ft = df_vaidation['label'].to_numpy()\n",
        "print(validation_ft[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng_score = []\n",
        "for _ in range(5):\n",
        "    validation_pred = [(True if b == 1 else False) for b in np.random.randint(2, size=( len(validation_ft)))]\n",
        "    rng_score.append(accuracy_score(validation_ft, validation_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.48721071863581,\n",
              " 0.5115712545676004,\n",
              " 0.4920828258221681,\n",
              " 0.4713763702801462,\n",
              " 0.5164433617539586]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF + LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import codecs\n",
        "import json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_feature_DaNetQA(row):\n",
        "    res = str(row[\"question\"]).strip()\n",
        "    label = row.get(\"label\")\n",
        "    return res, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_features_DaNetQA(path, vect):\n",
        "    with codecs.open(path, encoding='utf-8-sig') as reader:\n",
        "        lines = reader.read().split(\"\\n\")\n",
        "        lines = list(map(json.loads, filter(None, lines)))\n",
        "    res = list(map(build_feature_DaNetQA, lines))\n",
        "    texts = list(map(lambda x: x[0], res))\n",
        "    labels = list(map(lambda x: x[1], res))\n",
        "    ids = [x[\"idx\"] for x in lines]\n",
        "    return (vect.transform(texts), labels), ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_DaNetQA(train, labels):\n",
        "    clf = LogisticRegression()\n",
        "    return clf.fit(train, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_DaNetQA(train_path, val_path, test_path, vect):\n",
        "    train, _ = build_features_DaNetQA(train_path, vect)\n",
        "    val, _ = build_features_DaNetQA(val_path, vect)\n",
        "    test, ids = build_features_DaNetQA(test_path, vect)\n",
        "    clf = fit_DaNetQA(*train)\n",
        "    try:\n",
        "        test_score = clf.score(*test)\n",
        "    except ValueError:\n",
        "        test_score = None\n",
        "    test_pred = clf.predict(test[0])\n",
        "    return clf, {\n",
        "        \"train\": clf.score(*train),\n",
        "        \"val\": clf.score(*val),\n",
        "        \"test\": test_score,\n",
        "        \"test_pred\": [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(ids, test_pred)]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pre-Trained TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://russiansuperglue.com/tasks/tf_idf\n",
        "!unzip tf_idf_baseline.zip\n",
        "!rm tf_idf_baseline.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vect = joblib.load(\"tfidf.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Score Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = \"DaNetQA/train.jsonl\"\n",
        "val_path = \"DaNetQA/val.jsonl\"\n",
        "test_path = \"DaNetQA/test.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.8010291595197255\n",
            "Accuracy on validation data = 0.5907429963459196\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_scores = eval_DaNetQA(train_path, val_path, test_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_scores[\"val\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Pre-Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = \"DaNetQA/train_c.jsonl\"\n",
        "val_path = \"DaNetQA/val_c.jsonl\"\n",
        "test_path = \"DaNetQA/test_c.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on train data = 0.7004002287021155\n",
            "Accuracy on validation data = 0.5371498172959805\n"
          ]
        }
      ],
      "source": [
        "_, DaNetQA_Cleared_scores = eval_DaNetQA(train_path, val_path, test_path, vect)\n",
        "print(f'Accuracy on train data = {DaNetQA_Cleared_scores[\"train\"]}')\n",
        "print(f'Accuracy on validation data = {DaNetQA_Cleared_scores[\"val\"]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Impot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 0:\n",
        "    !pip install tensorflow\n",
        "    !pip install pandas\n",
        "    !pip install scipy\n",
        "    !pip install transformers\n",
        "    !pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available: True\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import torch\n",
        "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm \n",
        "\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers.optimization import AdamW\n",
        "from transformers import BertForSequenceClassification as BertModel\n",
        "\n",
        "from scipy.special import expit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 128\n",
        "MAX_LEN = 256\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BATCH_SIZE_LOADER = 8\n",
        "EPOCHS_LIMIT = 25\n",
        "LEARNING_RATE = 3e-5\n",
        "MAX_GRAD_NORM = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectAttentionMask(seq):\n",
        "    return [float(i > 0) for i in seq]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collectTokenType(row, sepTokenIdx):\n",
        "    row = np.array(row)\n",
        "    mask = row == sepTokenIdx\n",
        "\n",
        "    whereMask = np.where(mask)[0]\n",
        "    idx = whereMask[0]\n",
        "    idx1 = whereMask[1]\n",
        "\n",
        "    token_type_row = np.zeros(row.shape[0], dtype=np.int32)\n",
        "    token_type_row[idx + 1:idx1 + 1] = 1\n",
        "    return token_type_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_text_pairs(tokenizer, sentences):\n",
        "    ENCODE_BATCH_SIZE = 20000\n",
        "    input_ids, attention_masks, token_type_ids = [], [], []\n",
        "    \n",
        "    clsTokenText = '[CLS]'\n",
        "    sepTokenText = '[SEP]'\n",
        "    sepTokenIdx = tokenizer.convert_tokens_to_ids(sepTokenText)\n",
        "\n",
        "    TEXT1_MAX = int(MAX_LEN*.75) # выделяет 75% размера слов для контекста\n",
        "    TEXT2_MAX = MAX_LEN - TEXT1_MAX # остальные слова это вопрос\n",
        "    for _, i in enumerate(range(0, len(sentences), ENCODE_BATCH_SIZE)):\n",
        "        # обрезаем предложение слов больше чем MAX_LEN\n",
        "        tokenized_texts = []\n",
        "        for sentence_context, sentence_question  in sentences[i:i + ENCODE_BATCH_SIZE]:\n",
        "            p1 = [clsTokenText] + tokenizer.tokenize(sentence_context)\n",
        "            p2 = [sepTokenText] + tokenizer.tokenize(sentence_question) + [sepTokenText]\n",
        "            final_tokens = p1[:TEXT1_MAX] + p2[:TEXT2_MAX]\n",
        "            tokenized_texts.append(final_tokens)\n",
        "\n",
        "        # токенизируем\n",
        "        b_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "        b_input_ids = pad_sequences(\n",
        "            b_input_ids, \n",
        "            maxlen=MAX_LEN, \n",
        "            dtype='long', \n",
        "            truncating='post', \n",
        "            padding='post')\n",
        "        input_ids.append(b_input_ids)\n",
        "\n",
        "        # маска внимания\n",
        "        b_attention_masks = [collectAttentionMask(seq) for seq in b_input_ids]\n",
        "        attention_masks.append(b_attention_masks)\n",
        "\n",
        "        # тип токена\n",
        "        b_token_type_ids = [collectTokenType(row, sepTokenIdx) for row in b_input_ids]\n",
        "        token_type_ids.append(b_token_type_ids)\n",
        "        \n",
        "    return np.vstack(input_ids), np.vstack(attention_masks), np.vstack(token_type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/leysh/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /home/leysh/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /home/leysh/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "wget: /home/leysh/miniconda3/envs/tf/lib/libuuid.so.1: no version information available (required by wget)\n",
            "--2022-10-27 23:55:14--  http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
            "Распознаётся files.deeppavlov.ai (files.deeppavlov.ai)… 178.63.27.41\n",
            "Подключение к files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:80... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 301 Moved Permanently\n",
            "Адрес: https://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz [переход]\n",
            "--2022-10-27 23:55:14--  https://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
            "Подключение к files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:443... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
            "Длина: 662024852 (631M) [application/octet-stream]\n",
            "Сохранение в: ‘rubert_cased_L-12_H-768_A-12_pt.tar.gz’\n",
            "\n",
            "rubert_cased_L-12_H 100%[===================>] 631,36M  11,2MB/s    за 76s     \n",
            "\n",
            "2022-10-27 23:56:30 (8,33 MB/s) - ‘rubert_cased_L-12_H-768_A-12_pt.tar.gz’ сохранён [662024852/662024852]\n",
            "\n",
            "/bin/bash: /home/leysh/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "rubert_cased_L-12_H-768_A-12_pt/\n",
            "rubert_cased_L-12_H-768_A-12_pt/bert_config.json\n",
            "rubert_cased_L-12_H-768_A-12_pt/vocab.txt\n",
            "rubert_cased_L-12_H-768_A-12_pt/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "!wget \"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
        "!tar -xvzf rubert_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!rm rubert_cased_L-12_H-768_A-12_pt.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\n",
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\out\n",
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\rubert_cased_L-12_H-768_A-12_pt/\n"
          ]
        }
      ],
      "source": [
        "print(base_path := os.path.abspath(''))\n",
        "print(out_dir := os.path.join(base_path, 'out'))\n",
        "print(model_path := os.path.join(base_path, 'rubert_cased_L-12_H-768_A-12_pt/'))\n",
        "#print(base_path := os.path.abspath( os.path.join('', '..') ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\New folder\\New folder\\DSnML_Innopolis2022\\00_Final_Attestation\\DaNetQA\n"
          ]
        }
      ],
      "source": [
        "parts = ['train_v1', 'val_v1']\n",
        "#parts = ['train', 'val', 'test']\n",
        "print(data_path := os.path.join(base_path, 'DaNetQA'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1_id, text2_id, label_id, index_id = 'passage', 'question', 'label', 'idx'\n",
        "l2i = {False: 0, True:1}\n",
        "part2indices = {p:set() for p in parts}\n",
        "\n",
        "all_ids, all_sentences, all_labels = [], [], []\n",
        "for p in parts:\n",
        "    fname = '{}.jsonl'.format(p)\n",
        "    df = pd.read_json(os.path.join(data_path, fname), lines=True)\n",
        "    ids = df[index_id].to_numpy()\n",
        "    all_ids.extend(ids)\n",
        "    part2indices[p] = ids\n",
        "    all_labels.extend(df[label_id].to_numpy())\n",
        "    all_sentences.extend(\n",
        "        np.array(\n",
        "            np.column_stack([df[text1_id].to_numpy(), \n",
        "            df[text2_id].to_numpy()])\n",
        "        ).tolist()\n",
        "    )\n",
        "\n",
        "all_ids = np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(total) 2570\n",
            "len(l2i) 2\n"
          ]
        }
      ],
      "source": [
        "print ('len(total)', len(all_sentences))\n",
        "i2l = {l2i[l]:l for l in l2i}\n",
        "print ( 'len(l2i)', len(l2i) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### One-Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path = os.path.join(base_path, model_path),\n",
        "    do_lower_case=True,\n",
        "    max_length=MAX_LEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, attention_masks, token_type_ids = encode_text_pairs(tokenizer, all_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_indices = np.array([l2i[l] for l in all_labels])\n",
        "labels = np.zeros((input_ids.shape[0], len(l2i)))\n",
        "for _, i in enumerate(label_indices):\n",
        "    labels[_, i] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepeare Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createDataLoader(set_ids, all_ids, input_ids, attention_masks, token_type_ids, all_labels):\n",
        "    mask = np.array([sid in set_ids for sid in all_ids])\n",
        "    set_ids = all_ids[mask]\n",
        "\n",
        "    inputs = input_ids[mask], \n",
        "    masks = attention_masks[mask], \n",
        "    type_ids_dev = token_type_ids[mask]\n",
        "    labels = all_labels[mask]\n",
        "\n",
        "    t_inputs = torch.tensor(inputs)\n",
        "    t_masks = torch.tensor(masks)\n",
        "    t_type_ids_dev = torch.tensor(type_ids_dev)\n",
        "    t_labels = torch.tensor(labels)\n",
        "\n",
        "    t_dataset = TensorDataset(\n",
        "        t_inputs, \n",
        "        t_masks, \n",
        "        t_type_ids_dev, \n",
        "        t_labels)\n",
        "    t_sampler = SequentialSampler(t_dataset)\n",
        "\n",
        "    return DataLoader(\n",
        "        t_dataset, \n",
        "        sampler=t_sampler, \n",
        "        batch_size=BATCH_SIZE_LOADER, \n",
        "        worker_init_fn=seed_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels)\n",
        "train_dataloader = createDataLoader(part2indices['train_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels)\n",
        "validate_dataloader = createDataLoader(part2indices['val_v1'], \n",
        "    all_ids, input_ids, attention_masks, token_type_ids, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (f'Training set shape: {input_ids_train.shape}')\n",
        "print (f'Validation set shape: {input_ids_dev.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Pre-Trained BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Load config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_path = os.path.join(base_path, model_path, 'bert_config.json')\n",
        "conf = BertConfig.from_json_file(config_path)\n",
        "conf.num_labels = len(l2i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Load weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_model_file = os.path.join( base_path, model_path, 'pytorch_model.bin' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Init CUDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BertModel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3444\\126306759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_model_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'BertModel' is not defined"
          ]
        }
      ],
      "source": [
        "model = BertModel(conf)\n",
        "\n",
        "model.load_state_dict(torch.load(output_model_file), strict=False)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### Limit learning for BERT layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizer & Scheduler\n",
        "Задаем гиперпараметры для цикла обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nStep = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\leysh\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=LEARNING_RATE, \n",
        "    correct_bias=False)\n",
        "    \n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=LEARNING_RATE, \n",
        "    steps_per_epoch=nStep, \n",
        "    epochs=EPOCHS_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_res = 0\n",
        "best_dev_score = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loss = []\n",
        "iEpoch = 2\n",
        "for iEpoch in range(EPOCHS_LIMIT):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decompose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0 of 161, loss = 0.5834347093477845\n",
            "Step 1 of 161, loss = 0.6012287070043385\n",
            "Step 2 of 161, loss = 0.4539189119823277\n",
            "Step 3 of 161, loss = 0.38429474097210914\n",
            "Step 4 of 161, loss = 0.41969503299333155\n",
            "Step 5 of 161, loss = 0.7280048234388232\n",
            "Step 6 of 161, loss = 0.296478355769068\n",
            "Step 7 of 161, loss = 0.5101183207007125\n",
            "Step 8 of 161, loss = 0.5604927400127053\n",
            "Step 9 of 161, loss = 0.44689390575513244\n",
            "Step 10 of 161, loss = 0.6486014351248741\n",
            "Step 11 of 161, loss = 0.4079419504851103\n",
            "Step 12 of 161, loss = 0.6168186126742512\n",
            "Step 13 of 161, loss = 0.5110041471198201\n",
            "Step 14 of 161, loss = 0.5091695527080446\n",
            "Step 15 of 161, loss = 0.48808223905507475\n",
            "Step 16 of 161, loss = 0.5731303720385768\n",
            "Step 17 of 161, loss = 0.5737245150376111\n",
            "Step 18 of 161, loss = 0.3986828844062984\n",
            "Step 19 of 161, loss = 0.4671868961304426\n",
            "Step 20 of 161, loss = 0.6433931519277394\n",
            "Step 21 of 161, loss = 0.6100153351435438\n",
            "Step 22 of 161, loss = 0.3719248389825225\n",
            "Step 23 of 161, loss = 0.318731798324734\n",
            "Step 24 of 161, loss = 0.5808804389089346\n",
            "Step 25 of 161, loss = 0.6195557145401835\n",
            "Step 26 of 161, loss = 0.5775796296074986\n",
            "Step 27 of 161, loss = 0.3170254402793944\n",
            "Step 28 of 161, loss = 0.516489143949002\n",
            "Step 29 of 161, loss = 0.5266237799078226\n",
            "Step 30 of 161, loss = 0.5290822733659297\n",
            "Step 31 of 161, loss = 0.6686653078068048\n",
            "Step 32 of 161, loss = 0.6537250340916216\n",
            "Step 33 of 161, loss = 0.6508400603197515\n",
            "Step 34 of 161, loss = 0.7870263466611505\n",
            "Step 35 of 161, loss = 0.49259423720650375\n",
            "Step 36 of 161, loss = 0.4733567025978118\n",
            "Step 37 of 161, loss = 0.4277383401058614\n",
            "Step 38 of 161, loss = 0.6229245194699615\n",
            "Step 39 of 161, loss = 0.5004779326263815\n",
            "Step 40 of 161, loss = 0.5752065796405077\n",
            "Step 41 of 161, loss = 0.5789131289056968\n",
            "Step 42 of 161, loss = 0.47589190540020354\n",
            "Step 43 of 161, loss = 0.6275993558811024\n",
            "Step 44 of 161, loss = 0.5779116966295987\n",
            "Step 45 of 161, loss = 0.6159124746918678\n",
            "Step 46 of 161, loss = 0.48244258132763207\n",
            "Step 47 of 161, loss = 0.5204559857957065\n",
            "Step 48 of 161, loss = 0.7919255329761654\n",
            "Step 49 of 161, loss = 0.46542113833129406\n",
            "Step 50 of 161, loss = 0.3414058885537088\n",
            "Step 51 of 161, loss = 0.45264694560319185\n",
            "Step 52 of 161, loss = 0.4529890454141423\n",
            "Step 53 of 161, loss = 0.47662230231799185\n",
            "Step 54 of 161, loss = 0.7001265774015337\n",
            "Step 55 of 161, loss = 0.7148143018130213\n",
            "Step 56 of 161, loss = 0.7709275365341455\n",
            "Step 57 of 161, loss = 0.3268912222702056\n",
            "Step 58 of 161, loss = 0.6877080062404275\n",
            "Step 59 of 161, loss = 0.581461574183777\n",
            "Step 60 of 161, loss = 0.6079521202482283\n",
            "Step 61 of 161, loss = 0.4365702283103019\n",
            "Step 62 of 161, loss = 0.49799198866821826\n",
            "Step 63 of 161, loss = 0.39422335784183815\n",
            "Step 64 of 161, loss = 0.5481765014119446\n",
            "Step 65 of 161, loss = 0.51736173289828\n",
            "Step 66 of 161, loss = 0.4280785461887717\n",
            "Step 67 of 161, loss = 0.5280125206336379\n",
            "Step 68 of 161, loss = 0.44436380988918245\n",
            "Step 69 of 161, loss = 0.5945794729050249\n",
            "Step 70 of 161, loss = 0.507156896404922\n",
            "Step 71 of 161, loss = 0.35905520245432854\n",
            "Step 72 of 161, loss = 0.5595839070156217\n",
            "Step 73 of 161, loss = 0.6304216426797211\n",
            "Step 74 of 161, loss = 0.4965377977350727\n",
            "Step 75 of 161, loss = 0.5724080537911505\n",
            "Step 76 of 161, loss = 0.4196541402488947\n",
            "Step 77 of 161, loss = 0.4528441787115298\n",
            "Step 78 of 161, loss = 0.4580554390558973\n",
            "Step 79 of 161, loss = 0.5532068768516183\n",
            "Step 80 of 161, loss = 0.5433842829224886\n",
            "Step 81 of 161, loss = 0.3476879814406857\n",
            "Step 82 of 161, loss = 0.40638388181105256\n",
            "Step 83 of 161, loss = 0.32568619633093476\n",
            "Step 84 of 161, loss = 0.5720681671518832\n",
            "Step 85 of 161, loss = 0.5730929707642645\n",
            "Step 86 of 161, loss = 0.35205217893235385\n",
            "Step 87 of 161, loss = 0.4441213857498951\n",
            "Step 88 of 161, loss = 0.5506101290229708\n",
            "Step 89 of 161, loss = 0.780889616406057\n",
            "Step 90 of 161, loss = 0.38059503794647753\n",
            "Step 91 of 161, loss = 0.44536618620622903\n",
            "Step 92 of 161, loss = 0.4055551728233695\n",
            "Step 93 of 161, loss = 0.5192417479120195\n",
            "Step 94 of 161, loss = 0.43728919024579227\n",
            "Step 95 of 161, loss = 0.5294890908990055\n",
            "Step 96 of 161, loss = 0.7247508834116161\n",
            "Step 97 of 161, loss = 0.3949478988070041\n",
            "Step 98 of 161, loss = 0.5144758529495448\n",
            "Step 99 of 161, loss = 0.6948019948322326\n",
            "Step 100 of 161, loss = 0.34801260265521705\n",
            "Step 101 of 161, loss = 0.7458417322486639\n",
            "Step 102 of 161, loss = 0.5260898002889007\n",
            "Step 103 of 161, loss = 0.5748847099021077\n",
            "Step 104 of 161, loss = 0.3308671014383435\n",
            "Step 105 of 161, loss = 0.6163538522087038\n",
            "Step 106 of 161, loss = 0.6311869199707871\n",
            "Step 107 of 161, loss = 0.4114727219566703\n",
            "Step 108 of 161, loss = 0.3170912710484117\n",
            "Step 109 of 161, loss = 0.704846904380247\n",
            "Step 110 of 161, loss = 0.44665352744050324\n",
            "Step 111 of 161, loss = 0.368850392755121\n",
            "Step 112 of 161, loss = 0.6154944181907922\n",
            "Step 113 of 161, loss = 0.4914802818093449\n",
            "Step 114 of 161, loss = 0.5670524265151471\n",
            "Step 115 of 161, loss = 0.5902488129213452\n",
            "Step 116 of 161, loss = 0.6929298366885632\n",
            "Step 117 of 161, loss = 0.7457732504699379\n",
            "Step 118 of 161, loss = 0.4301627748645842\n",
            "Step 119 of 161, loss = 0.4332249561557546\n",
            "Step 120 of 161, loss = 0.6490318418946117\n",
            "Step 121 of 161, loss = 0.541563778417185\n",
            "Step 122 of 161, loss = 0.6707843240583315\n",
            "Step 123 of 161, loss = 0.6497166896006092\n",
            "Step 124 of 161, loss = 0.5639557070098817\n",
            "Step 125 of 161, loss = 0.6474942804779857\n",
            "Step 126 of 161, loss = 0.49781843973323703\n",
            "Step 127 of 161, loss = 0.567121685249731\n",
            "Step 128 of 161, loss = 0.5469797309488058\n",
            "Step 129 of 161, loss = 0.5616941961925477\n",
            "Step 130 of 161, loss = 0.3587030537892133\n",
            "Step 131 of 161, loss = 0.3832861836999655\n",
            "Step 132 of 161, loss = 0.4818529586773366\n",
            "Step 133 of 161, loss = 0.7450890211039223\n",
            "Step 134 of 161, loss = 0.6687443514820188\n",
            "Step 135 of 161, loss = 0.46887859085109085\n",
            "Step 136 of 161, loss = 0.6399125955067575\n",
            "Step 137 of 161, loss = 0.5123145678080618\n",
            "Step 138 of 161, loss = 0.6593371196649969\n",
            "Step 139 of 161, loss = 0.38704752060584724\n",
            "Step 140 of 161, loss = 0.5314676116686314\n",
            "Step 141 of 161, loss = 0.6637703424785286\n",
            "Step 142 of 161, loss = 0.4599911404075101\n",
            "Step 143 of 161, loss = 0.48979451204650104\n",
            "Step 144 of 161, loss = 0.5355380214750767\n",
            "Step 145 of 161, loss = 0.5050031970749842\n",
            "Step 146 of 161, loss = 0.4590388904325664\n",
            "Step 147 of 161, loss = 0.42867347900755703\n",
            "Step 148 of 161, loss = 0.585113609675318\n",
            "Step 149 of 161, loss = 0.5302989308256656\n",
            "Step 150 of 161, loss = 0.650659530190751\n",
            "Step 151 of 161, loss = 0.446173450211063\n",
            "Step 152 of 161, loss = 0.5669045811519027\n",
            "Step 153 of 161, loss = 0.7866167244501412\n",
            "Step 154 of 161, loss = 0.48459747387096286\n",
            "Step 155 of 161, loss = 0.5623904005624354\n",
            "Step 156 of 161, loss = 0.43915495462715626\n",
            "Step 157 of 161, loss = 0.45568014378659427\n",
            "Step 158 of 161, loss = 0.48867845989298075\n",
            "Step 159 of 161, loss = 0.5242171890567988\n",
            "Step 160 of 161, loss = 0.5976895555853844\n"
          ]
        }
      ],
      "source": [
        "model.train() \n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "tr_loss = 0\n",
        "nb_tr_examples = 0\n",
        "nb_tr_steps = 0\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(\n",
        "        b_input_ids,\n",
        "        token_type_ids = b_token_type_ids, \n",
        "        attention_mask = b_input_mask, \n",
        "        labels = b_labels\n",
        "        )\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    train_loss.append(loss.item())\n",
        "    loss.backward()\n",
        "    clip_grad_norm(model.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    epochLoss = loss.item()\n",
        "    tr_loss += epochLoss\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "    \n",
        "    print(f\"Step {step} of {nStep}, loss = {epochLoss}\")\n",
        "avg_train_loss = tr_loss/nb_tr_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 average train_loss: 0.529285 dev_loss: 0.383752 dev_acc 87.45%\n"
          ]
        }
      ],
      "source": [
        "### val\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "tr_loss = 0\n",
        "nb_tr_steps = 0\n",
        "for step, batch in enumerate(prediction_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "        b_input_ids,\n",
        "        token_type_ids = b_token_type_ids, \n",
        "        attention_mask = b_input_mask, \n",
        "        labels = b_labels\n",
        "        )\n",
        "        loss, logits = outputs[:2]\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.append(logits)\n",
        "predictions = expit(np.vstack(predictions))\n",
        "edev_loss = tr_loss/nb_tr_steps\n",
        "\n",
        "y_indices, pred = np.argmax(labels_dev, axis=1), np.argmax(predictions, axis=1)\n",
        "dev_acc = accuracy_score(y_indices, pred)*100\n",
        "print(f'Epoch {iEpoch} average train_loss: {avg_train_loss:.6f} dev_loss: {edev_loss:.6f} dev_acc {dev_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dev_acc>best_dev_score: # compute result for test part and store to out file, if we found better model\n",
        "    best_dev_score = dev_acc\n",
        "    cv_res = best_dev_score\n",
        "\n",
        "    predictions, true_labels = [], []\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model( b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask, labels=b_labels )\n",
        "        \n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "    predictions = expit(np.vstack(predictions))\n",
        "    true_labels = np.concatenate(true_labels)\n",
        "    assert len(true_labels) == len(predictions)\n",
        "    recs = []\n",
        "    for idx, l, row in zip(test_ids, true_labels, predictions):\n",
        "        gt = i2l[np.argmax(l)]\n",
        "        pred = i2l[np.argmax(row)]\n",
        "        recs.append( (idx, gt, pred) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_acc = cv_res\n",
        "print (f'\\scores: {dev_acc:.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch_seq2seq_with_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "846dd53c5a100503afcb3f5301bb10f61481596a80ae839ecd432be859b5d4d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
