{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arnY8EH0RHWH"
      },
      "source": [
        "## Текстовая классификация для набора данных IMDB с использованием BERT.\n",
        "В этом занятии мы создаем бинарный классификатор для набора данных обзоров IMDB, используя [BERT] (https://arxiv.org/abs/1810.04805), предварительно обученную модель НЛП, открытую в Google , которую можно использовать для [ Transfer Learning](https://towardsdatascience.com/transfer-learning-in-nlp-fecc59f546e4) на текстовых данных. Ссылку на набор данных можно найти [здесь] (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).<br>\n",
        "\n",
        "\n",
        "**ВАЖНО :**\n",
        "Чтобы запустить этот блокнот, загрузите [Набор данных IMDB] (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) в папке **Данные**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9dE1jifozEz",
        "outputId": "9780c58e-8448-4fe1-f62d-d787055fad79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-17 08:54:53--  https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66212309 (63M) [text/plain]\n",
            "Saving to: ‘IMDB-Dataset.csv’\n",
            "\n",
            "IMDB-Dataset.csv    100%[===================>]  63.14M   178MB/s    in 0.4s    \n",
            "\n",
            "2022-09-17 08:54:54 (178 MB/s) - ‘IMDB-Dataset.csv’ saved [66212309/66212309]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MK-POIlJE0Eu",
        "outputId": "a3dcc2c6-c303-4836-fdd5-a65e3f614fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp39-cp39-win_amd64.whl (13.3 MB)\n",
            "     --------------------------------------- 13.3/13.3 MB 11.3 MB/s eta 0:00:00\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.2\n",
            "    Uninstalling numpy-1.23.2:\n",
            "      Successfully uninstalled numpy-1.23.2\n",
            "Successfully installed numpy-1.19.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.11.0rc0, 2.11.0rc1)\n",
            "ERROR: No matching distribution found for tensorflow==1.14.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp39-cp39-win_amd64.whl (222.0 MB)\n",
            "     -------------------------------------- 222.0/222.0 MB 7.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from torch==1.9.0) (4.4.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "Successfully installed torch-1.9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0 requires torch==1.13.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.13.0 requires torch==1.13.0, but you have torch 1.9.0 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.21.3\n",
            "  Downloading scikit-learn-0.21.3.tar.gz (12.2 MB)\n",
            "     --------------------------------------- 12.2/12.2 MB 11.5 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from scikit-learn==0.21.3) (1.19.5)\n",
            "Collecting scipy>=0.17.0\n",
            "  Using cached scipy-1.9.3-cp39-cp39-win_amd64.whl (40.2 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (setup.py): started\n",
            "  Building wheel for scikit-learn (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for scikit-learn\n",
            "Failed to build scikit-learn\n",
            "Installing collected packages: scipy, joblib, scikit-learn\n",
            "  Running setup.py install for scikit-learn: started\n",
            "  Running setup.py install for scikit-learn: finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [29 lines of output]\n",
            "      Partial import of sklearn during the build process.\n",
            "      Traceback (most recent call last):\n",
            "        File \"<string>\", line 2, in <module>\n",
            "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
            "        File \"C:\\Users\\leysh\\AppData\\Local\\Temp\\pip-install-k3jb1hp1\\scikit-learn_d3e2f1c7a1ac4018b97eb8c61d5b51aa\\setup.py\", line 290, in <module>\n",
            "          setup_package()\n",
            "        File \"C:\\Users\\leysh\\AppData\\Local\\Temp\\pip-install-k3jb1hp1\\scikit-learn_d3e2f1c7a1ac4018b97eb8c61d5b51aa\\setup.py\", line 286, in setup_package\n",
            "          setup(**metadata)\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\core.py\", line 135, in setup\n",
            "          config = configuration()\n",
            "        File \"C:\\Users\\leysh\\AppData\\Local\\Temp\\pip-install-k3jb1hp1\\scikit-learn_d3e2f1c7a1ac4018b97eb8c61d5b51aa\\setup.py\", line 174, in configuration\n",
            "          config.add_subpackage('sklearn')\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1018, in add_subpackage\n",
            "          config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 984, in get_subpackage\n",
            "          config = self._get_configuration_from_setup_py(\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 926, in _get_configuration_from_setup_py\n",
            "          config = setup_module.configuration(*args)\n",
            "        File \"sklearn\\setup.py\", line 62, in configuration\n",
            "          config.add_subpackage('utils')\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1018, in add_subpackage\n",
            "          config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 984, in get_subpackage\n",
            "          config = self._get_configuration_from_setup_py(\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 926, in _get_configuration_from_setup_py\n",
            "          config = setup_module.configuration(*args)\n",
            "        File \"sklearn\\utils\\setup.py\", line 8, in configuration\n",
            "          from Cython import Tempita\n",
            "      ModuleNotFoundError: No module named 'Cython'\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for scikit-learn\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Running setup.py install for scikit-learn did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [29 lines of output]\n",
            "      Partial import of sklearn during the build process.\n",
            "      Traceback (most recent call last):\n",
            "        File \"<string>\", line 2, in <module>\n",
            "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
            "        File \"C:\\Users\\leysh\\AppData\\Local\\Temp\\pip-install-k3jb1hp1\\scikit-learn_d3e2f1c7a1ac4018b97eb8c61d5b51aa\\setup.py\", line 290, in <module>\n",
            "          setup_package()\n",
            "        File \"C:\\Users\\leysh\\AppData\\Local\\Temp\\pip-install-k3jb1hp1\\scikit-learn_d3e2f1c7a1ac4018b97eb8c61d5b51aa\\setup.py\", line 286, in setup_package\n",
            "          setup(**metadata)\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\core.py\", line 135, in setup\n",
            "          config = configuration()\n",
            "        File \"C:\\Users\\leysh\\AppData\\Local\\Temp\\pip-install-k3jb1hp1\\scikit-learn_d3e2f1c7a1ac4018b97eb8c61d5b51aa\\setup.py\", line 174, in configuration\n",
            "          config.add_subpackage('sklearn')\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1018, in add_subpackage\n",
            "          config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 984, in get_subpackage\n",
            "          config = self._get_configuration_from_setup_py(\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 926, in _get_configuration_from_setup_py\n",
            "          config = setup_module.configuration(*args)\n",
            "        File \"sklearn\\setup.py\", line 62, in configuration\n",
            "          config.add_subpackage('utils')\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1018, in add_subpackage\n",
            "          config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 984, in get_subpackage\n",
            "          config = self._get_configuration_from_setup_py(\n",
            "        File \"C:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 926, in _get_configuration_from_setup_py\n",
            "          config = setup_module.configuration(*args)\n",
            "        File \"sklearn\\utils\\setup.py\", line 8, in configuration\n",
            "          from Cython import Tempita\n",
            "      ModuleNotFoundError: No module named 'Cython'\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: legacy-install-failure\n",
            "\n",
            "× Encountered error while trying to install package.\n",
            "╰─> scikit-learn\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for output from the failure.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert==0.6.2\n",
            "  Using cached pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "Collecting pytorch-nlp==0.5.0\n",
            "  Using cached pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: torch>=0.4.1 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from pytorch_pretrained_bert==0.6.2) (1.9.0)\n",
            "Collecting regex\n",
            "  Using cached regex-2022.9.13-cp39-cp39-win_amd64.whl (267 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from pytorch_pretrained_bert==0.6.2) (2.28.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from pytorch_pretrained_bert==0.6.2) (1.19.5)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.25.4-py3-none-any.whl (132 kB)\n",
            "     -------------------------------------- 132.5/132.5 kB 1.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (4.4.0)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "     ---------------------------------------- 79.6/79.6 kB ? eta 0:00:00\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.29.0,>=1.28.4\n",
            "  Downloading botocore-1.28.4-py3-none-any.whl (9.3 MB)\n",
            "     ---------------------------------------- 9.3/9.3 MB 11.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from requests->pytorch_pretrained_bert==0.6.2) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from requests->pytorch_pretrained_bert==0.6.2) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from requests->pytorch_pretrained_bert==0.6.2) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from requests->pytorch_pretrained_bert==0.6.2) (2.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from tqdm->pytorch_pretrained_bert==0.6.2) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from botocore<1.29.0,>=1.28.4->boto3->pytorch_pretrained_bert==0.6.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.4->boto3->pytorch_pretrained_bert==0.6.2) (1.16.0)\n",
            "Installing collected packages: tqdm, regex, jmespath, pytorch-nlp, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
            "Successfully installed boto3-1.25.4 botocore-1.28.4 jmespath-1.0.1 pytorch-nlp-0.5.0 pytorch_pretrained_bert-0.6.2 regex-2022.9.13 s3transfer-0.6.0 tqdm-4.64.1\n",
            "Collecting tqdm==4.41.1\n",
            "  Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\n",
            "     ---------------------------------------- 56.7/56.7 kB 1.5 MB/s eta 0:00:00\n",
            "Installing collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "Successfully installed tqdm-4.41.1\n",
            "Collecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp39-cp39-win_amd64.whl (8.9 MB)\n",
            "     ---------------------------------------- 8.9/8.9 MB 10.2 MB/s eta 0:00:00\n",
            "Collecting pytz>=2017.2\n",
            "  Using cached pytz-2022.5-py2.py3-none-any.whl (500 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.16.0)\n",
            "Installing collected packages: pytz, pandas\n",
            "Successfully installed pandas-1.1.5 pytz-2022.5\n",
            "Collecting matplotlib==3.2.2\n",
            "  Downloading matplotlib-3.2.2-cp39-cp39-win_amd64.whl (8.9 MB)\n",
            "     ---------------------------------------- 8.9/8.9 MB 10.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from matplotlib==3.2.2) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from matplotlib==3.2.2) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from matplotlib==3.2.2) (2.8.2)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp39-cp39-win_amd64.whl (55 kB)\n",
            "     ---------------------------------------- 55.4/55.4 kB ? eta 0:00:00\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.16.0)\n",
            "Installing collected packages: kiwisolver, cycler, matplotlib\n",
            "Successfully installed cycler-0.11.0 kiwisolver-1.4.4 matplotlib-3.2.2\n",
            "Collecting beautifulsoup4==4.6.3\n",
            "  Downloading beautifulsoup4-4.6.3-py3-none-any.whl (90 kB)\n",
            "     ---------------------------------------- 90.4/90.4 kB 1.3 MB/s eta 0:00:00\n",
            "Installing collected packages: beautifulsoup4\n",
            "Successfully installed beautifulsoup4-4.6.3\n"
          ]
        }
      ],
      "source": [
        "# To install only the requirements of this notebook, \n",
        "\n",
        "# ===========================\n",
        "\n",
        "!pip install numpy==1.19.5\n",
        "!pip install tensorflow==1.14.0\n",
        "!pip install torch==1.9.0\n",
        "!pip install scikit-learn==0.21.3\n",
        "!pip install pytorch_pretrained_bert==0.6.2 pytorch-nlp==0.5.0     \n",
        "!pip install tqdm==4.41.1\n",
        "!pip install pandas==1.1.5\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install beautifulsoup4==4.6.3\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.1.3-cp39-cp39-win_amd64.whl (7.6 MB)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from scikit-learn->sklearn) (1.9.3)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages (from scikit-learn->sklearn) (1.23.4)\n",
            "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
            "Successfully installed scikit-learn-1.1.3 sklearn-0.0 threadpoolctl-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TtokjlkCQbiw"
      },
      "outputs": [],
      "source": [
        "DATA_DIR=\".\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "BI8AvyFZRAha",
        "outputId": "b254d1da-f187-4c77-f1e0-748a5e6a8e90"
      },
      "outputs": [],
      "source": [
        "# загрузка и чтение набора данных\n",
        "# источник набора данных: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "try :\n",
        "    \n",
        "    from google.colab import files\n",
        "   # После загрузки набора данных загрузите его сюда.\n",
        "    uploaded = files.upload()\n",
        "    df = pd.read_csv(\"IMDB Dataset.csv\",engine='python', error_bad_lines=False)\n",
        "    df = pd.read_csv(\"/content/IMDB-Dataset.csv\",engine='python', error_bad_lines=False)\n",
        "\n",
        "except ModuleNotFoundError :\n",
        "    \n",
        "    # После загрузки набора данных поместите файл IMDB Dataset.csv в папку Data.\n",
        "    df = pd.read_csv(\"Data/IMDB Dataset.csv\",engine='python', error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MD5sedPwN7tX",
        "outputId": "a1646986-fbfb-4002-e410-d5f081c031ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AzOdtJlCRAfb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#сентимент положительный и отрицательный, нам нужно преобразовать его в 0,1\n",
        "le = LabelEncoder()\n",
        "df[\"sentiment\"] = le.fit_transform(df[\"sentiment\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dop1ppy6XQYn",
        "outputId": "4b538bb3-a604-402e-f219-baab3273c1df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    25000\n",
              "0    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "azXUgzdKXqX3"
      },
      "outputs": [],
      "source": [
        "#cleaning the text\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def strip(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = re.sub('\\[[^]]*\\]', '', soup.get_text())\n",
        "    pattern=r\"[^a-zA-z0-9\\s,']\"\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "df['review']=df['review'].apply(strip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "va7N2bOMjjKs",
        "outputId": "8e1fa0e0-9fae-4396-ff0d-7f08a881ec61"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production The filming tech...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's Love in the Time of Money is a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production The filming tech...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's Love in the Time of Money is a...          1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYHojJ9Kr_5w"
      },
      "source": [
        "BERT ожидает ввода данных в определенном формате\n",
        "1. Мы выполняем задачу классификации, поэтому используем специальный токен [CLS], чтобы сообщить об этом BERT.\n",
        "2. Ему нужно знать конец предложения, поэтому мы используем токен [SEP]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-yCoUW-FZtD7"
      },
      "outputs": [],
      "source": [
        "#BERT должен понимать две вещи:\n",
        "#1) Начало и конец каждого предложения\n",
        "# поэтому мы объявляем специальный токен CLS, который сообщает BERT, что это задача классификации\n",
        "sentences = df['review']\n",
        "sentence = [\"[CLS] \"+i+\" [SEP]\" for i in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "NUpOJpndZw5J",
        "outputId": "caefa2b7-dabb-4ef3-c6a5-795ca9eee0c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"[CLS] One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked They are right, as this is exactly what happened with meThe first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO Trust me, this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs, sex or violence Its is hardcore, in the classic use of the wordIt is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda Em City is home to manyAryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and moreso scuffles, death stares, dodgy dealings and shady agreements are never far awayI would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare Forget pretty pictures painted for mainstream audiences, forget charm, forget romanceOZ doesn't mess around The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence Not just violence, but injustice crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz, you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side [SEP]\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5A_V7BT2bKM"
      },
      "source": [
        "Теперь нам нужно разбить наш текст на токены, соответствующие словарю BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjfr85BSbY41",
        "outputId": "2e6311a0-2835-437f-f94e-bbe8b6fcf3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', \"'\", 'll', 'be', 'hooked', 'they', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'met', '##he', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'un', '##fl', '##in', '##ching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'tim', '##id', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', '##it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'pen', '##ite', '##nta', '##ry', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inward', '##s', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', '##ary', '##ans', ',', 'muslims', ',', 'gangs', '##tas', ',', 'latino', '##s', ',', 'christians', ',', 'italians', ',', 'irish', 'and', 'more', '##so', 'sc', '##uf', '##fles', ',', 'death', 'stares', ',', 'dod', '##gy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', '##i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', \"'\", 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '##oz', 'doesn', \"'\", 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'i', 'couldn', \"'\", 't', 'say', 'i', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'i', 'watched', 'more', ',', 'i', 'developed', 'a', 'taste', 'for', 'oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', ',', 'but', 'injustice', 'crooked', 'guards', 'who', \"'\", 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'\", 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'manner', '##ed', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitch', '##es', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '##tha', '##ts', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "#  BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "#Ограничение максимального размера токенов до 512 (BERT не принимает больше этого)\n",
        "tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:511], sentence))\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kVXxiuqQbkVp"
      },
      "outputs": [],
      "source": [
        "labels = list(df['sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1u24cpd2mBl"
      },
      "source": [
        "Теперь нам нужно задать входные идентификаторы BERT, т. е. последовательность целых чисел, которая однозначно идентифицирует каждый входной токен по его порядковому номеру."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vgssRTWCc_Xl"
      },
      "outputs": [],
      "source": [
        "# Установите максимальную длину последовательности.\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Дополнить наши входные токены, чтобы все они имели одинаковую длину\n",
        "input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ApEi7LbLdIBj"
      },
      "outputs": [],
      "source": [
        "# Используйте токенизатор BERT для преобразования токенов в их порядковые номера в словаре BERT\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbHLewX_24ef"
      },
      "source": [
        "BERT — это MLM (модель маскированного языка). Мы должны определить его маску."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uDp1d_FIFhKz"
      },
      "outputs": [],
      "source": [
        "# Создаем attention masks\n",
        "attention_masks = []\n",
        "# Создайте маску из 1 для каждого токена, а затем 0 для padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evbWDiQ73QZj"
      },
      "source": [
        "Теперь нам нужно разделить данные на train и test. Преобразуйте его в тензоры, а затем создайте итератор для наших данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wT7bDRYEFhPJ"
      },
      "outputs": [],
      "source": [
        "# Выберите размер батча для обучения.\n",
        "batch_size = 16\n",
        "\n",
        "# Используйте train_test_split, чтобы разделить наши данные на обучающие и проверочные наборы для обучения\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "                                             \n",
        "# Преобразование всех наших данных в тензоры pytorch, требуемый тип данных для нашей модели\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Создаем итератор наших данных с помощью torch DataLoader\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xRtmpSlFhSP",
        "outputId": "03c85ddf-00f5-40fd-ca58-63a877fdbde9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:36<00:00, 11070919.99B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): BertLayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Загружаем пред-обученную модель BERT\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) #binary classification\n",
        "if torch.cuda.is_available():\n",
        "    print(model.cuda())\n",
        "else:\n",
        "    print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzxxuH_6310Y"
      },
      "source": [
        " Fine-Tuning of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "bLZXK8OZquKl",
        "outputId": "82b08678-f04d-458a-ac8a-0e6185280ff1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n",
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [17], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m   train_loss_set\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())    \n\u001b[0;32m     48\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m   loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     50\u001b[0m \u001b[39m# Обновите параметры и сделайте шаг, используя вычисленный градиент\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[1;32mc:\\Users\\leysh\\anaconda3\\envs\\tensorflow11_6\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# BERT fine-tuning parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,# 2e-3, 2e-7\n",
        "                     warmup=.1)\n",
        "\n",
        "# Функция для расчета точности наших прогнозов по сравнению с метками\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "torch.cuda.empty_cache() \n",
        "# Сохраняем наши loss и точность для построения графика\n",
        "train_loss_set = []\n",
        "# Количество эпох обучения\n",
        "epochs = 4\n",
        "\n",
        "# BERT - цикл обучения модели\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "  \n",
        "  ## Обучение\n",
        "  \n",
        "  # Установите нашу модель в режим обучения\n",
        "    model.train()  \n",
        "  # Отслеживание переменных\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Обучение на данных для одной эпохи\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "    # Добавить пакет в GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "      # Распакуйте входные данные из нашего загрузчика данных\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Очистите градиенты (по умолчанию они накапливаются)\n",
        "        optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        train_loss_set.append(loss.item())    \n",
        "      # Backward pass\n",
        "        loss.backward()\n",
        "      # Обновите параметры и сделайте шаг, используя вычисленный градиент\n",
        "        optimizer.step()\n",
        "      # Обновить переменные\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "       \n",
        "  ## VALIDATION\n",
        "\n",
        "  # Поместите модель в режим валидации\n",
        "    model.eval()\n",
        "  # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "        # Forward pass, рассчитать логит-прогнозы\n",
        "          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
        "      # Переместить logits и labels на CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# Отрисовать ход обучения\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tensorflow11_6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "77f53a379ad290490246eaf4c3eb5f086a7dd3d16db5115c5b3ebf03d59f4bd5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
