{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание\n",
        "Возьмите данные отзывов о фильмах сайта Кинопоиск ру из соревнования https://www.kaggle.com/mikhailklemin/kinopoisks-movies-reviews \n",
        "\n",
        "Обучите реккурентную нейронную сеть с различными вариантами embeddig:\n",
        "* собственный embedding\n",
        "* word2vec\n",
        "* GLOVE\n",
        "\n",
        "после чего дообучите нейронную сеть Bert на\n",
        "* 10%\n",
        "* 20%\n",
        "* 50% \n",
        "обучающих примеров\n",
        "\n",
        "Попробуйте добиться точности выше, чем у рекуррентной нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6st-EzRWJEmE",
        "jukit_cell_id": "6gUAGkjX5g"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leysh/miniconda3/envs/catalyst1/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Mapping, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import yaml\n",
        "from catalyst.utils import set_global_seed, prepare_cudnn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "from catalyst.callbacks.metrics.accuracy import AccuracyCallback\n",
        "from catalyst.dl import (\n",
        "    CheckpointCallback,\n",
        "    OptimizerCallback,\n",
        "    SchedulerCallback,\n",
        "    SupervisedRunner,\n",
        ")\n",
        "\n",
        "def get_project_root() -> Path:\n",
        "    return Path(\"\").parent.parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "class_names_converter = {\n",
        "    'neg': 'Negative',\n",
        "    'pos': 'Positive',\n",
        "    'neu': 'Neutral',\n",
        "}\n",
        "\n",
        "def name_to_id(name):\n",
        "    return class_names.index(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'cointegrated/rubert-tiny2'\n",
        "BATCH_SIZE = 16 # 32 не влезает в 10GB VRAM\n",
        "SEED = 123\n",
        "NUM_CLASSES = 3\n",
        "LR = 3e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Класс для токенизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iyoDRjdKJEmG",
        "jukit_cell_id": "oOWb6j5g3N"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wrapper around Torch Dataset to perform text classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        labels: List[str] = None,\n",
        "        label_dict: Mapping[str, int] = None,\n",
        "        max_seq_length: int = None,\n",
        "        model_name: str = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (List[str]): a list with texts to classify or to train the\n",
        "                classifier on\n",
        "            labels List[str]: a list with classification labels (optional)\n",
        "            label_dict (dict): a dictionary mapping class names to class ids,\n",
        "                to be passed to the validation data (optional)\n",
        "            max_seq_length (int): maximal sequence length in tokens,\n",
        "                texts will be stripped to this length\n",
        "            model_name (str): transformer model name, needed to perform\n",
        "                appropriate tokenization\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.label_dict = label_dict\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        if self.label_dict is None and labels is not None:\n",
        "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
        "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
        "            # no easily handle unknown target values\n",
        "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        # suppresses tokenizer warnings\n",
        "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
        "\n",
        "        # special tokens for transformers\n",
        "        # in the simplest case a [CLS] token is added in the beginning\n",
        "        # and [SEP] token is added in the end of a piece of text\n",
        "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
        "        self.sep_label = self.tokenizer.special_tokens_map['sep_token']\n",
        "        self.sep_vid = self.tokenizer.vocab[self.sep_label]\n",
        "        self.cls_label = self.tokenizer.special_tokens_map['cls_token']\n",
        "        self.cls_vid = self.tokenizer.vocab[self.cls_label]\n",
        "        self.pad_label = self.tokenizer.special_tokens_map['pad_token']\n",
        "        self.pad_vid = self.tokenizer.vocab[self.pad_label]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: length of the dataset\n",
        "        \"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
        "        \"\"\"Gets element of the dataset\n",
        "\n",
        "        Args:\n",
        "            index (int): index of the element in the dataset\n",
        "        Returns:\n",
        "            Single element by index\n",
        "        \"\"\"\n",
        "        # encoding the text\n",
        "        x = self.texts[index]\n",
        "\n",
        "        # a dictionary with `input_ids` and `attention_mask` as keys\n",
        "        output_dict = self.tokenizer.encode_plus(\n",
        "            text=x,\n",
        "            ##text_pair=text_pair, \n",
        "            ##text_target=text_target, \n",
        "            ##text_pair_target=text_pair_target, \n",
        "            add_special_tokens=True,\n",
        "            #  Pad to a maximum length specified with the argument max_length\n",
        "            #  or to the maximum acceptable input length for the model if that argument is not provided.\n",
        "            padding=\"max_length\", \n",
        "            # Truncate to a maximum length specified with the argument max_length \n",
        "            # or to the maximum acceptable input length for the model if that argument is not provided. \n",
        "            # This will truncate token by token, \n",
        "            # removing a token from the longest sequence in the pair if a pair of sequences (or a batch of pairs) is provided.\n",
        "            truncation=True,\n",
        "            # Controls the maximum length to use by one of the truncation/padding parameters.\n",
        "            max_length=self.max_seq_length, \n",
        "            # return pytorch tensor\n",
        "            return_tensors=\"pt\", \n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        # for Catalyst, there needs to be a key called features\n",
        "        output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n",
        "        del output_dict[\"input_ids\"]\n",
        "\n",
        "        output_dict[\"token_type_ids\"] = output_dict[\"token_type_ids\"].squeeze(0)\n",
        "\n",
        "        # encoding target\n",
        "        if self.labels is not None:\n",
        "            y = self.labels[index]\n",
        "            y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n",
        "            output_dict[\"targets\"] = y_encoded\n",
        "            \n",
        "        return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_data(path_name) -> Tuple[dict, dict]:\n",
        "    \"\"\"\n",
        "    A custom function that reads data from CSV files, creates PyTorch datasets and\n",
        "    data loaders. The output is provided to be easily used with Catalyst\n",
        "\n",
        "    :param params: a dictionary read from the config.yml file\n",
        "    :return: a tuple with 2 dictionaries\n",
        "    \"\"\"\n",
        "    # reading CSV files to Pandas dataframes\n",
        "    train_df = pd.read_csv(f'Kinopoisk_train_{path_name}%.csv')\n",
        "    test_df = pd.read_csv(f'Kinopoisk_eval_{path_name}%.csv')\n",
        "\n",
        "    # creating PyTorch Datasets\n",
        "    \n",
        "    train_dataset = TextClassificationDataset(\n",
        "        texts=train_df['review'].values.tolist(),\n",
        "        labels=train_df['sentiment'].values.tolist(),\n",
        "        max_seq_length=256,\n",
        "        model_name=MODEL_NAME,\n",
        "    )\n",
        "\n",
        "    test_dataset = TextClassificationDataset(\n",
        "        texts=test_df['review'].values.tolist(),\n",
        "        labels=test_df['sentiment'].values.tolist(),\n",
        "        max_seq_length=256,\n",
        "        model_name=MODEL_NAME,\n",
        "    )\n",
        "\n",
        "    set_global_seed(SEED)\n",
        "\n",
        "    # creating PyTorch data loaders and placing them in dictionaries (for Catalyst)\n",
        "    train_val_loaders = {\n",
        "        \"train\": DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "        ),\n",
        "        \"valid\": DataLoader(\n",
        "            dataset=test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    del train_df\n",
        "    del test_df\n",
        "    del train_dataset\n",
        "    del test_dataset\n",
        "\n",
        "    return train_val_loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Класс для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified version of the same class by HuggingFace.\n",
        "    See transformers/modeling_distilbert.py in the transformers repository.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        pretrained_model_name: str, \n",
        "        num_classes: int = None, \n",
        "        dropout: float = 0.3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pretrained_model_name (str): HuggingFace model name.\n",
        "                See transformers/modeling_auto.py\n",
        "            num_classes (int): the number of class labels\n",
        "                in the classification task\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(pretrained_model_name, num_labels=num_classes)\n",
        "\n",
        "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        attention_mask = kwargs['attention_mask']\n",
        "        features = kwargs['features']\n",
        "        token_type_ids = kwargs['token_type_ids']\n",
        "        head_mask = None\n",
        "        \"\"\"Compute class probabilities for the input sequence.\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor): ids of each token,\n",
        "                size ([bs, seq_length]\n",
        "            attention_mask (torch.Tensor): binary tensor, used to select\n",
        "                tokens which are used to compute attention scores\n",
        "                in the self-attention heads, size [bs, seq_length]\n",
        "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
        "                we keep the head, size: [num_heads]\n",
        "                or [num_hidden_layers x num_heads]\n",
        "        Returns:\n",
        "            PyTorch Tensor with predicted class scores\n",
        "        \"\"\"\n",
        "        assert attention_mask is not None, \"attention mask is none\"\n",
        "\n",
        "        # taking BERTModel output\n",
        "        # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
        "        bert_output = self.model(\n",
        "            input_ids=features, \n",
        "            attention_mask=attention_mask, \n",
        "            token_type_ids=token_type_ids, \n",
        "            head_mask=head_mask)\n",
        "        # we only need the hidden state here and don't need\n",
        "        # transformer output, so index 0\n",
        "\n",
        "        seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
        "        del bert_output, features, attention_mask, token_type_ids, head_mask\n",
        "        # mean pooling, i.e. getting average representation of all tokens\n",
        "        pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
        "        del pooled_output\n",
        "\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-Convert Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = Path('../datasets/kinopoisk/')\n",
        "\n",
        "def standardize_text(df, content_field):\n",
        "    df[content_field] = df[content_field].str.replace(r\"http\\S+\", \"\")\n",
        "    df[content_field] = df[content_field].str.replace(r\"@\\S+\", \"\")\n",
        "    df[content_field] = df[content_field].str.replace(\n",
        "        r\"[^А-Яа-яA-Za-z0-9Ёё(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
        "    df[content_field] = df[content_field].str.replace(r\"[Ёё]\", \"е\")\n",
        "    df[content_field] = df[content_field].str.replace(r\"[\\t\\n]\", \"\")\n",
        "    df[content_field] = df[content_field].str.replace(r\"[^А-Яа-яa-zA-Z]\", \" \")\n",
        "    df[content_field] = df[content_field].str.lower()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "94Rb9iofJEmI",
        "jukit_cell_id": "miTk5oP3Qe",
        "outputId": "5c7ce975-66a4-4ab9-a9aa-66ba72ddd7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len = (1983,)\n",
            "Negative\n",
            "len = (2470,)\n",
            "Neutral\n",
            "len = (8714,)\n",
            "Positive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"http\\S+\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"@\\S+\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[Ёё]\", \"е\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[\\t\\n]\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[^А-Яа-яa-zA-Z]\", \" \")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len = (3965,)\n",
            "Negative\n",
            "len = (4941,)\n",
            "Neutral\n",
            "len = (17428,)\n",
            "Positive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"http\\S+\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"@\\S+\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[Ёё]\", \"е\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[\\t\\n]\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[^А-Яа-яa-zA-Z]\", \" \")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len = (9914,)\n",
            "Negative\n",
            "len = (12352,)\n",
            "Neutral\n",
            "len = (43569,)\n",
            "Positive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"http\\S+\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"@\\S+\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[Ёё]\", \"е\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[\\t\\n]\", \"\")\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_13496\\323947140.py:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[content_field] = df[content_field].str.replace(r\"[^А-Яа-яa-zA-Z]\", \" \")\n"
          ]
        }
      ],
      "source": [
        "for perc in [0.1, 0.2, 0.5]:\n",
        "    df = pd.DataFrame(columns=['review', 'sentiment'])\n",
        "    \n",
        "    for class_path in dataset_path.iterdir():\n",
        "        if class_path.is_dir():\n",
        "            dirs = np.array(list(class_path.iterdir()))\n",
        "            np.random.shuffle(dirs)\n",
        "            rews_fhs = np.random.choice(dirs, round(len(dirs)*perc))\n",
        "            print(f'len = {rews_fhs.shape}')\n",
        "            print(class_names_converter[class_path.name])\n",
        "            for rew_fh in rews_fhs:\n",
        "                with open(Path(rew_fh), encoding='utf-8') as f:\n",
        "                    review = f.read()\n",
        "                    current_df = pd.DataFrame(\n",
        "                        {'review': [review], 'sentiment': class_names_converter[class_path.name]})\n",
        "                    df = pd.concat([df, current_df], ignore_index=True)\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    df = standardize_text(df, \"review\")\n",
        "    df['sentiment'] = df['sentiment'].map(name_to_id)\n",
        "\n",
        "    train_dataset, eval_dataset = train_test_split(df, test_size = 0.2)\n",
        "\n",
        "    train_dataset.to_csv(f'Kinopoisk_train_{perc:.0%}.csv')\n",
        "    eval_dataset.to_csv(f'Kinopoisk_eval_{perc:.0%}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MGdcCGSYJEmO",
        "jukit_cell_id": "cpSBILUvCR",
        "outputId": "f419fdf1-b4d2-4407-c3af-b1fe368ea481"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "1/1 * Epoch (train): 100%|██████████| 659/659 [00:30<00:00, 21.86it/s, accuracy01=0.600, loss=0.637, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (1/1) accuracy01: 0.6559384790771108 | accuracy01/std: 0.12504121922786524 | loss: 0.8617919843714994 | loss/mean: 0.8617919843714994 | loss/std: 0.16782431436785505 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1/1 * Epoch (valid): 100%|██████████| 165/165 [00:05<00:00, 31.80it/s, accuracy01=0.200, loss=1.368, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (1/1) accuracy01: 0.6636294609072898 | accuracy01/std: 0.12052975597766279 | loss: 0.7986804979175172 | loss/mean: 0.7986804979175172 | loss/std: 0.13749004082896044 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (1/1) lr: 3e-05 | momentum: 0.9\n",
            "Top models:\n",
            "logdir/model.0001.pth\t0.7987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "1/2 * Epoch (train): 100%|██████████| 1317/1317 [00:57<00:00, 22.75it/s, accuracy01=0.455, loss=1.181, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (1/2) accuracy01: 0.6727583424383646 | accuracy01/std: 0.12344518444846372 | loss: 0.7989554702853663 | loss/mean: 0.7989554702853663 | loss/std: 0.18282527998282225 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1/2 * Epoch (valid): 100%|██████████| 330/330 [00:12<00:00, 27.19it/s, accuracy01=1.000, loss=0.219, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (1/2) accuracy01: 0.7102715018036834 | accuracy01/std: 0.11763777609222056 | loss: 0.6934037586334241 | loss/mean: 0.6934037586334241 | loss/std: 0.192717224524619 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (1/2) lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2/2 * Epoch (train): 100%|██████████| 1317/1317 [00:59<00:00, 22.22it/s, accuracy01=0.727, loss=0.575, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (2/2) accuracy01: 0.7273935539107812 | accuracy01/std: 0.11401886684541465 | loss: 0.6564741386987897 | loss/mean: 0.6564741386987897 | loss/std: 0.18795131919977795 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2/2 * Epoch (valid): 100%|██████████| 330/330 [00:12<00:00, 27.06it/s, accuracy01=1.000, loss=0.272, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (2/2) accuracy01: 0.728498196316689 | accuracy01/std: 0.11195397154947952 | loss: 0.6516227998293529 | loss/mean: 0.6516227998293529 | loss/std: 0.19285006019220513 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (2/2) lr: 3e-05 | momentum: 0.9\n",
            "Top models:\n",
            "logdir/model.0002.pth\t0.6516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "1/4 * Epoch (train): 100%|██████████| 3292/3292 [02:29<00:00, 22.03it/s, accuracy01=0.667, loss=0.691, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (1/4) accuracy01: 0.7011278195534 | accuracy01/std: 0.11991643035608987 | loss: 0.7255106063641287 | loss/mean: 0.7255106063641287 | loss/std: 0.19090411759039225 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1/4 * Epoch (valid): 100%|██████████| 823/823 [00:34<00:00, 23.91it/s, accuracy01=0.667, loss=0.731, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (1/4) accuracy01: 0.7310700995137868 | accuracy01/std: 0.11241682628514992 | loss: 0.6453891860159463 | loss/mean: 0.6453891860159463 | loss/std: 0.20229306745918227 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (1/4) lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2/4 * Epoch (train): 100%|██████████| 3292/3292 [02:38<00:00, 20.71it/s, accuracy01=0.583, loss=0.750, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (2/4) accuracy01: 0.7581453634175741 | accuracy01/std: 0.10454674585165724 | loss: 0.5854744199034438 | loss/mean: 0.5854744199034438 | loss/std: 0.179694417392816 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2/4 * Epoch (valid): 100%|██████████| 823/823 [00:34<00:00, 23.56it/s, accuracy01=0.867, loss=0.596, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (2/4) accuracy01: 0.7445127972213321 | accuracy01/std: 0.10689175895339627 | loss: 0.6159893971301804 | loss/mean: 0.6159893971301804 | loss/std: 0.18552633861610127 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (2/4) lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3/4 * Epoch (train): 100%|██████████| 3292/3292 [02:38<00:00, 20.73it/s, accuracy01=0.833, loss=0.364, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (3/4) accuracy01: 0.8116123642529952 | accuracy01/std: 0.0985156907378575 | loss: 0.4693079384532854 | loss/mean: 0.4693079384532854 | loss/std: 0.1781956542713888 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3/4 * Epoch (valid): 100%|██████████| 823/823 [00:34<00:00, 23.71it/s, accuracy01=0.667, loss=0.873, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (3/4) accuracy01: 0.7652464494796095 | accuracy01/std: 0.10977876813860052 | loss: 0.6358669336024836 | loss/mean: 0.6358669336024836 | loss/std: 0.2897278886732773 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (3/4) lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4/4 * Epoch (train): 100%|██████████| 3292/3292 [02:47<00:00, 19.70it/s, accuracy01=0.750, loss=0.431, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train (4/4) accuracy01: 0.8713450292397652 | accuracy01/std: 0.08454109581111163 | loss: 0.33952026477257163 | loss/mean: 0.33952026477257163 | loss/std: 0.16731665275732086 | lr: 3e-05 | momentum: 0.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4/4 * Epoch (valid): 100%|██████████| 823/823 [00:35<00:00, 23.13it/s, accuracy01=0.667, loss=0.879, lr=3.000e-05, momentum=0.900]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid (4/4) accuracy01: 0.788790157233844 | accuracy01/std: 0.10160383827029872 | loss: 0.6363295485410774 | loss/mean: 0.6363295485410774 | loss/std: 0.3250218898072485 | lr: 3e-05 | momentum: 0.9\n",
            "* Epoch (4/4) lr: 3e-05 | momentum: 0.9\n",
            "Top models:\n",
            "logdir/model.0002.pth\t0.6160\n"
          ]
        }
      ],
      "source": [
        "num_epochs = [1, 2, 4]\n",
        "model_names = ['10', '20', '50']\n",
        "runner = []\n",
        "for num_epochs, model_name in zip(num_epochs, model_names):\n",
        "    # загружаем датасет нужного размера\n",
        "    train_val_loaders = read_data(model_name)\n",
        "\n",
        "    # загружаем предтренированную модель из HF\n",
        "    model = BertForSequenceClassification(\n",
        "        pretrained_model_name=MODEL_NAME,\n",
        "        num_classes=NUM_CLASSES,\n",
        "    )\n",
        "\n",
        "    param_optimizer = list(model.model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "    # обнуляем сид\n",
        "    set_global_seed(SEED)\n",
        "    prepare_cudnn(deterministic=True)\n",
        "\n",
        "    runner = SupervisedRunner(input_key=(\"features\", \"attention_mask\", \"token_type_ids\"))\n",
        "    # тренировка\n",
        "    runner.train(\n",
        "        model=model,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        loaders=train_val_loaders,\n",
        "        callbacks=[\n",
        "            AccuracyCallback(num_classes=NUM_CLASSES, input_key=\"logits\", target_key=\"targets\"),\n",
        "            OptimizerCallback(accumulation_steps=4, metric_key=\"loss\"),\n",
        "            SchedulerCallback(loader_key=\"valid\", metric_key=\"loss\"),\n",
        "            CheckpointCallback(logdir=\"logdir\", loader_key=\"valid\", metric_key=\"loss\", minimize=True),\n",
        "        ],\n",
        "        logdir=\"logdir\",\n",
        "        num_epochs=num_epochs,\n",
        "        verbose=True,\n",
        "    )\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.save(model, f'model_{model_name}%.pt')\n",
        "    del model, runner, criterion, optimizer, scheduler, train_val_loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iI_EZq-9JEmO",
        "jukit_cell_id": "dAU32vMHuC",
        "outputId": "9d4efda2-63df-4d30-c8eb-033fd0d434a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Model: model_10%.pt\n",
            "valid (1/1) accuracy01: 0.6636294609072898 | accuracy01/std: 0.12052975597766279\n",
            "* Epoch (1/1) \n",
            "defaultdict(None, {'accuracy01': 0.6636294609072898, 'accuracy01/std': 0.12052975597766279})\n",
            "\n",
            "=== Model: model_20%.pt\n",
            "valid (1/1) accuracy01: 0.728498196316689 | accuracy01/std: 0.11195397154947952\n",
            "* Epoch (1/1) \n",
            "defaultdict(None, {'accuracy01': 0.728498196316689, 'accuracy01/std': 0.11195397154947952})\n",
            "\n",
            "=== Model: model_50%.pt\n",
            "valid (1/1) accuracy01: 0.788790157233844 | accuracy01/std: 0.10160383827029872\n",
            "* Epoch (1/1) \n",
            "defaultdict(None, {'accuracy01': 0.788790157233844, 'accuracy01/std': 0.10160383827029872})\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for model_name in model_names:\n",
        "    # and running inference\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # read and process data\n",
        "    train_val_loaders = read_data(model_name)   \n",
        "\n",
        "    runner = SupervisedRunner(input_key=(\"features\", \"attention_mask\", \"token_type_ids\"))\n",
        "    runner.model = torch.load(f\"model_{model_name}%.pt\")\n",
        "    print(f\"=== Model: {f'model_{model_name}%.pt'}\")\n",
        "    # getting validation metrics\n",
        "    metrics = runner.evaluate_loader(\n",
        "        loader=train_val_loaders[\"valid\"],\n",
        "        callbacks=[AccuracyCallback(input_key=\"logits\", target_key=\"targets\")],\n",
        "    )\n",
        "    print(metrics)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXLp-WF2JEmQ",
        "jukit_cell_id": "7VYXas56u0"
      },
      "source": [
        "Результат модели LSTM, созданной ранее в ДЗ (34 LSTM, GRU) показал точность в 73% на тестовой выборке.  \n",
        "\n",
        "Fine-tune BERT с предобучением и токенизатором от ruBERT   в 79% при обучении на 50% датасете. \n",
        "\n",
        "Обучение на 20% размере датасета показывает такую же точность модели как и LTSM."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('catalyst1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e6374150d64b7b463fc3f558c333ee2100da45f3ddc290a75b2f3063ea5e3af3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
