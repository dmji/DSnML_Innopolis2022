{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание\n",
        "Используя нейронную сеть Seq2Seq с механизмом Attention обучите переводчик с Русского на Английский язык<br>\n",
        "Для обучения используйте данные: https://www.quest.dcs.shef.ac.uk/wmt20_files_qe/ru-en.openkiwi-predictor.tar.gz \n",
        "\n",
        "Попробуйте поэкспериментировать с разными вариантами рекуррентных сетей\n",
        "* LSTM \n",
        "* GRU\n",
        "* RNN\n",
        "\n",
        "c различными параметрами\n",
        "* глубина нейронных сетей\n",
        "* длина последовательности\n",
        "* кол-во эпох.\n",
        "\n",
        "Составьте отчет в табличной форме (можно прямо в ноутбуке с результатами) с указанием\n",
        "*  всех протестированных архитектур\n",
        "* bleau метрик по каждой из них (для расчета создайте самостоятельно тестовые данные, которые не будут участвовать в обучении)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rQd3pAQXa0eA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from genericpath import exists\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to load SentencePiece model. Error: Internal: C:\\Users\\circleci\\project\\third_party\\sentencepiece\\src\\sentencepiece_processor.cc(76) [model_proto->ParseFromArray(serialized.data(), serialized.size())] ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18776\\2629525649.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_sp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/ru-en/openkiwi-predictor/best_model.torch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torchtext\\data\\functional.py\u001b[0m in \u001b[0;36mload_sp_model\u001b[1;34m(spm)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \"\"\"\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_sp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBufferedReader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_sp_model_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\leysh\\anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;31m# TODO: use this to make a __dir__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to load SentencePiece model. Error: Internal: C:\\Users\\circleci\\project\\third_party\\sentencepiece\\src\\sentencepiece_processor.cc(76) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
          ]
        }
      ],
      "source": [
        "torchtext.data.load_sp_model(\"data/ru-en/openkiwi-predictor/best_model.torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxKa0yTLa0eC"
      },
      "source": [
        "Загружаем данные Парные предлождения Русский - Английский\n",
        "==================\n",
        "\n",
        "::\n",
        "\n",
        "    Я есть холод    I am cold.\n",
        "\n",
        ".. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "read_binary() missing 1 required positional argument: 'resource'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21060\\1583879763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_clipboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/ru-en/openkiwi-predictor/best_model.torch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m: read_binary() missing 1 required positional argument: 'resource'"
          ]
        }
      ],
      "source": [
        "from os import read\n",
        "from pandas import read_clipboard\n",
        "\n",
        "t = )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Токенизатор или словарь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-JnOXhRZa0eF"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Удаление юникод символов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AGEb8a-1a0eH"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Нормализация строк"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Заполнение словарей "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "axtlS7AMa0eI"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V4OnrMcJa0eJ"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mtmN_zZDH9a",
        "outputId": "17b5fe48-9da0-4bb1-f142-e52a14212a7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "if not exists(\"data.zip\"):\n",
        "    !wget https://download.pytorch.org/tutorial/data.zip\n",
        "    !unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "G9Ze-5FNa0eK",
        "outputId": "9cc638e9-459a-43a1-9102-101ab4a509c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['je suis trop gros .', 'i m too fat .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wygv_jRZa0eK"
      },
      "source": [
        "Модель Seq2Seq \n",
        "=================\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86lOcbjVa0eL"
      },
      "source": [
        " Encoder\n",
        "-----------\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZyurCkXUa0eL"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEoB_71ga0eM"
      },
      "source": [
        "The Decoder\n",
        "-----------\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-VBwYUDa0eM"
      },
      "source": [
        "Simple Decoder\n",
        "^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E4i8T-Ija0eM"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S-zXRmfDa0eN"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) # q * k\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0)) # v * alpha\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0) # Z\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSKTjGsQa0eO"
      },
      "source": [
        "\n",
        "Обучение\n",
        "========\n",
        "\n",
        "Подготовка данных\n",
        "-----------------------\n",
        "\n",
        "Для обучения для каждой пары нам понадобится входной тензор (индексы\n",
        "слова во входном предложении) и целевой тензор (индексы слов в целевом предложении). При создании этих векторов мы добавим токен EOS к обеим последовательностям.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jAeBC4Ana0eP"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Jd082Oa0eP"
      },
      "source": [
        "Обучение модели\n",
        "------------------\n",
        "\n",
        "Для обучения мы пропускаем предложение ввода через encoder и отслеживаем каждый вывод и последнее скрытое состояние. Затем декодеру предоставляется токен `` <SOS> '' в качестве его первого входа и последнее скрытое состояние кодировщика в качестве его первого скрытого состояния.\n",
        "\n",
        "«Teaching forcing» - это концепция использования реальных целевых результатов в качестве следующего входа deocder вместо использования предположения декодера в качестве следующего входа.\n",
        "\n",
        "\n",
        "Вы можете наблюдать результаты созданных учителем сетей, которые хорошо работают с последовательной грамматикой, но далеки от правильного перевода - интуитивно они научинаучились представлять грамматику вывода и могут «улавливать» значение, когда учитель говорит ему первые несколько слов, но они не научились должным образом составлять предложение из перевода.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O_Kl6n2Ea0eP"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor\n",
        "        , target_tensor\n",
        "        , encoder\n",
        "        , decoder\n",
        "        , encoder_optimizer\n",
        "        , decoder_optimizer\n",
        "        , criterion\n",
        "        , max_length=MAX_LENGTH\n",
        "    ):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Отслеживание времени"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KtpH1tGQa0eQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GKsaIMBa0eQ"
      },
      "source": [
        "Весь тренировочный процесс выглядит так:\n",
        "\n",
        "- Запустить таймер\n",
        "- Инициализировать оптимизаторы и критерий\n",
        "- Создать набор обучающих пар\n",
        "- Запустить пустой массив потерь для построения\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sehopJIla0eR"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh42o7C3a0eR"
      },
      "source": [
        "Выводим результаты\n",
        "----------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ikCsRNdVa0eR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz62w52Ra0eR"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pWwkvIBwa0eR"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2s_lr-3Ia0eS"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxhI-k7pa0eS"
      },
      "source": [
        "Обучение\n",
        "=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-06DRJAha0eS",
        "outputId": "12b9a18b-012d-4ac1-df91-efb862f5fe99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 41s (- 23m 42s) (5000 6%) 2.8450\n",
            "3m 16s (- 21m 15s) (10000 13%) 2.2882\n",
            "4m 52s (- 19m 31s) (15000 20%) 1.9480\n",
            "6m 30s (- 17m 53s) (20000 26%) 1.6953\n",
            "8m 7s (- 16m 15s) (25000 33%) 1.5229\n",
            "9m 43s (- 14m 34s) (30000 40%) 1.3639\n",
            "11m 19s (- 12m 56s) (35000 46%) 1.2127\n",
            "12m 54s (- 11m 17s) (40000 53%) 1.0877\n",
            "14m 30s (- 9m 40s) (45000 60%) 0.9840\n",
            "16m 8s (- 8m 4s) (50000 66%) 0.8748\n",
            "17m 41s (- 6m 25s) (55000 73%) 0.8178\n",
            "19m 12s (- 4m 48s) (60000 80%) 0.7566\n",
            "20m 42s (- 3m 11s) (65000 86%) 0.6799\n",
            "22m 13s (- 1m 35s) (70000 93%) 0.6718\n",
            "23m 42s (- 0m 0s) (75000 100%) 0.6048\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rPxP7KQa0eT",
        "outputId": "82675b8f-3fc5-49de-a529-fe5c9c313046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> vous me faites rougir .\n",
            "= you re making me blush .\n",
            "< you re pulling my leg . <EOS>\n",
            "\n",
            "> je suis responsable de la securite de tom .\n",
            "= i m responsible for tom s safety .\n",
            "< i m responsible for tom s safety . <EOS>\n",
            "\n",
            "> nous n en avons pas encore termine .\n",
            "= we re not done yet .\n",
            "< we re not finished yet . <EOS>\n",
            "\n",
            "> je ne suis pas instituteur .\n",
            "= i m not a teacher .\n",
            "< i m not a teacher . <EOS>\n",
            "\n",
            "> je fais partie du club de tennis .\n",
            "= i m in the tennis club .\n",
            "< i m in the tennis of . <EOS>\n",
            "\n",
            "> nous sommes les gentils .\n",
            "= we re the good guys .\n",
            "< we re all good . <EOS>\n",
            "\n",
            "> c est un enfant pourri gate .\n",
            "= he is a spoiled child .\n",
            "< he is a child child . <EOS>\n",
            "\n",
            "> je suis content que ca vous rende heureux .\n",
            "= i m glad that makes you happy .\n",
            "< i m glad that makes you happy . <EOS>\n",
            "\n",
            "> je ne te le dis pas .\n",
            "= i m not telling you .\n",
            "< i m not telling you . <EOS>\n",
            "\n",
            "> nous sommes dans le meme bateau .\n",
            "= we re in the same boat .\n",
            "< we re in the same boat . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "icrdKH-Vq98F"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
        "\n",
        "def BLEU(sent1,sent2):\n",
        "  sent1_bleu = []\n",
        "  sent2_bleu = []\n",
        "\n",
        "  for w1 in sent2:\n",
        "    sent2_bleu.append(w1.split())\n",
        "\n",
        "  for i, w2 in enumerate(sent1):\n",
        "    sent1_bleu.append([w2.split()])\n",
        "\n",
        "  sf = SmoothingFunction()\n",
        "\n",
        "  score = corpus_bleu(sent1_bleu,sent2_bleu,weights = (0,1,0,0),smoothing_function= sf.method4)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94qRXfu7sGJC",
        "outputId": "0c9b626a-1dd2-4cae-ba5c-afd457c2577a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BLEU(['you are what you eat .'],['you are what you think .'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy8wZh5aa0eT"
      },
      "source": [
        "Визуализируем Attention\n",
        "---------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "CGz-E0tua0eT",
        "outputId": "c50c3e8b-c912-4680-b05f-777e261eab4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1c88d527a60>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"vous etes tres vif .\")\n",
        "plt.matshow(attentions.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "up0BBK5Sa0eT",
        "outputId": "469356df-41ba-4149-faad-050213fd93e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input = elle a cinq ans de moins que moi .\n",
            "output = she s five years younger than me . <EOS>\n",
            "input = elle est trop petit .\n",
            "output = she s too drunk . <EOS>\n",
            "input = je ne crains pas de mourir .\n",
            "output = i m not scared of dying . <EOS>\n",
            "input = c est un jeune directeur plein de talent .\n",
            "output = he s a talented young director . <EOS>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_25232\\744067545.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_25232\\744067545.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_25232\\744067545.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "46-hpxbOtnjJ",
        "outputId": "fadff61d-11c6-4775-8b44-925364458cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input = je ne ai pas peur de toi\n",
            "output = i m not scared of you . <EOS>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_25232\\744067545.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_25232\\744067545.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n",
            "C:\\Users\\leysh\\AppData\\Local\\Temp\\ipykernel_25232\\744067545.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "evaluateAndShowAttention(\"je ne ai pas peur de toi\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch_seq2seq_with_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "846dd53c5a100503afcb3f5301bb10f61481596a80ae839ecd432be859b5d4d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
